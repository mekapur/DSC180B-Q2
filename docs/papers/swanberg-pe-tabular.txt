
License: CC BY 4.0
arXiv:2502.06555v1 [cs.LG] 10 Feb 2025
Is API Access to LLMs Useful for Generating Private Synthetic Tabular Data?
Marika Swanberg
Boston University & Google Research marikaswanberg@google.com
&Ryan McKenna
Google Research mckennar@google.com
&Edo Roth Google Research edor@google.com
&Albert Cheu Google Research cheu@google.com
&Peter Kairouz Google Research kairouz@google.com
Work done while an intern at Google Research.
Abstract

Differentially private (DP) synthetic data is a versatile tool for enabling the analysis of private data. Recent advancements in large language models (LLMs) have inspired a number of algorithm techniques for improving DP synthetic data generation. One family of approaches uses DP finetuning on the foundation model weights; however, the model weights for state-of-the-art models may not be public. In this work we propose two DP synthetic tabular data algorithms that only require API access to the foundation model. We adapt the Private Evolution algorithm (Lin et al., 2023; Xie et al., 2024)â€”which was designed for image and text dataâ€”to the tabular data domain. In our extension of Private Evolution, we define a query workload-based distance measure, which may be of independent interest. We propose a family of algorithms that use one-shot API access to LLMs, rather than adaptive queries to the LLM. Our findings reveal that API-access to powerful LLMs does not always improve the quality of DP synthetic data compared to established baselines that operate without such access. We provide insights into the underlying reasons and propose improvements to LLMs that could make them more effective for this application.
1 Introduction

Synthetic data has long been a â€œholy grailâ€ for performing computations on sensitive data, with the allure of protecting privacy while supporting typical data queries and regular data workflows out-of-the-box. Unfortunately, without a rigorous treatment of privacy, the synthetic dataset may inadvertently reveal information about the sensitive data from which it is derived.

Differential privacy (DP) (Dwork et al., 2006; 2016) has emerged as the gold standard for quantifying privacy leakage by algorithms that process sensitive data records from users. At a high level, a (randomized) algorithm satisfies differential privacy if the algorithmâ€™s output distribution is not affected very much by a single personâ€™s data, regardless of what the other data records are. This ensures that the mechanismâ€™s output reveals little about any individual personâ€™s data as a result of their participation in the data analysis, even after arbitrary post-processing of the mechanism output.

Many algorithms have been developed for DP synthetic data, particularly for tabular data (McKenna et al., 2022; Tao et al., 2021; Liu et al., 2021b; Aydore et al., 2021; Liu et al., 2021a; Cai et al., 2021; Zhang et al., 2021). With the advancement of large language models (LLMs), a number of recent works propose improved DP synthetic data algorithms that use LLMs trained on public data1. Among these are two broad categories of methods: those which privately finetune a foundation model, and those which only use API access to the foundation model. Sablayrolles et al. (2023), Tran & Xiong (2024), and Afonja et al. (2024) use private finetuning on generative language models to generate private tabular synthetic data, and Kurakin et al. (2023) similarly do private LoRA finetuning on an LLM to generate synthetic text data. Similarly, Ghalebikesabi et al. (2023) employ DP finetuning of diffusion models for generating DP synthetic images.

Despite their power, these DP finetuning methods have significant hurdles. First, finetuning algorithms require white-box access to the model, as the weights need to be directly adjusted. This is a problem because many state-of-the-art models are proprietary, with weights that remain confidential. Only a limited set of researchers are able to even experiment with DP finetuning on such models. Secondly, the resources needed for DP finetuning scales with model dimensionality; time and energy costs quickly become prohibitive. These hurdles motivate alternative ways of using foundation models. In particular, even many proprietary models have a publicly accessible API.

A series of works in the synthetic image (Lin et al., 2023) and text (Xie et al., 2024) domains use only API access to foundation models. The algorithm, Private Evolution, combines adaptive queries to the foundation model with a genetic algorithm to privately generate synthetic image and text data. These methods were further extended to the federated setting by Hou et al. (2024). Yet a different approach (Amin et al., 2024) uses private prediction combined with other privacy budget saving tricks on the foundation model to generate DP synthetic text.

In light of these recent successes for image and text data, we ask: Can API access to an LLM improve algorithms for generating DP synthetic tabular data?

A priori it is not obvious why an LLM would be useful at all for generating synthetic tabular data that it was not trained on; however, in initial experiments we found that with descriptive column names, the LLM we used has a reasonable prior over realistic-looking data records. This prior is a powerful source of information we harness in our algorithms.

We design and evaluate two types of DP synthetic tabular generation algorithms that leverage LLM API access. In Section 3, we adapt Private Evolution (Lin et al., 2023; Xie et al., 2024) to the tabular domain. A key part of our solution uses a workload-aware family of distance functions, which may be of independent interest, to align the genetic algorithm with the final workload error. In Section 4 we introduce a new class of private synthetic data algorithms that use one-shot API access to the foundation model. Unlike prior methods, which require adaptive queries to the foundation model or finetuning the modelâ€™s weights, our method consumes only one (offline) round of queries to the foundation model. Along the way, we evaluate our two approaches against a number of accuracy baselines to determine whether they advance the state-of-the-art for DP synthetic tabular data.

We evaluated our algorithms with Gemini 1.0 Pro (Gemini Team Google, 2023), which allowed us to constrain the outputs to structured tabular records. In our evaluations, we find that the proposed methods fail to consistently beat our baselines. Despite this, we think our attempts are instructive to the research community and could inform the development of state-of-the-art methods, especially as foundation models improve. In light of our findings, we share our key take-aways:
The role of data domain.

The state-of-the-art for DP synthetic data generation is highly domain specific. In particular, DP tabular synthetic data has been very well-studied compared to image and text, so the state of the art for tabular data is much harder to improve on. Additionally, prior work on Private Evolution relies on public image and text embeddings to measure the fidelity of the synthetic data, but similar embeddings do not exist for tabular data. Our workload-aware distance function in Section 3 is one substitute, but surely other solutions exist as well.
The importance of appropriate baselines.

In the tabular data domain, there is no single algorithm that dominates on all datasets, query workloads, and privacy budgets. Any new algorithm in this area requires extensive comparison to the handful of algorithms that dominate the state-of-the-art, as well as naive baselines. In Section 4, we show that combining Gemini-generated data with JAM (Fuentes et al., 2024) outperforms all other methods; however, in testing other baselines we find that this holds regardless of the public data we give JAM. Without this naive baseline, we would have reached a false conclusion that the Gemini-generated data was improving the state-of-the-art.
2 Preliminaries

We begin by presenting the definition of differential privacy, which is a constraint on an algorithm ğ’œ that processes a dataset ğ±=(ğ±1,â€¦,ğ±n) of user records, one per user. Two datasets are called neighbors if they differ on one personâ€™s record. At a high level, differential privacy requires that for any pair of neighboring datasets, the algorithmâ€™s output distributions are similar when run on each dataset.
Definition 1 (Differential Privacy (Dwork et al., 2006; 2016))

A randomized algorithm ğ’œ:ğ’°nâ†’ğ’´ is Îµ-differentially private if for every pair of neighboring datasets ğ±,ğ±â€²âˆˆğ’°n, and for all outputs yâˆˆğ’´,
	
Prâ¡[ğ’œâ¢(ğ±)=y]â‰¤eÎµâ‹…Prâ¡[ğ’œâ¢(ğ±â€²)=y]+Î´,
	

where the probability is taken over the internal coins of ğ’œ.

The differential privacy guarantee is parameterized by Îµ>0, where algorithms with lower values have less privacy leakage and higher values of epsilon denote more privacy leakage from the algorithmâ€™s output. DP gives a worst-case guarantee (over the algorithmâ€™s inputs and outputs) on how much information an algorithm leaks about its input.
2.1 Prior Work
GAN-based methods for DP synthetic data

Many prior works have proposed synthetic data mechanisms based on generative adversarial networks. See Yang et al. (2024) for a nice survey of these and other approaches. These mechanisms generally work by fitting the parameters of the model via DP-SGD, and then using the model to generate synthetic data after training. These techniques are typically best suited for unstructured data like images or text.
Marginal-based methods for DP synthetic data

Many mechanisms for DP synthetic data generation work by adding noise to low-dimensional marginals of the data distribution McKenna et al. (2021; 2022); Cai et al. (2021); Aydore et al. (2021); Fuentes et al. (2024); Vietri et al. (2022); Liu et al. (2021b; a); Zhang et al. (2021). Some mechanisms in this space are also designed to leverage public data when itâ€™s available Fuentes et al. (2024); Liu et al. (2021b; a). Benchmarks have confirmed these approaches work very well in tabular data settings Tao et al. (2021).
3 Adapting Private Evolution to Tabular Data

We adapt Private Evolution (PE) (Lin et al., 2023; Xie et al., 2024) to the tabular data domain. Private Evolution works in rounds, by maintaining a set of candidates St generated by the foundation model and using a distance function together with a differentially private histogram to have each private record individually vote for candidates. The best performing synthetic candidates become part of an elite set for that round Stâ€²; at the end of each round, the foundation model is prompted to generate more examples similar to the elite set, which then become the new candidates St+1.

The first set of candidates are populated by a ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†â¢_â¢ğ– ğ–¯ğ–¨, which prompts the model to generate some prespecified number of initial candidates adhering to the column names and datatypes of the private dataset. Each subsequent set of candidates are generated via the ğ–µğ–ºğ—‹ğ—‚ğ–ºğ—ğ—‚ğ—ˆğ—‡â¢_â¢ğ– ğ–¯ğ–¨ which takes the current elite set of candidates and prompts the model to generate some number of additional candidates that are similar.
Algorithm 1 Private Evolution (Lin et al., 2023; Xie et al., 2024)

Input: Private samples Sğ—‰ğ—‹ğ—‚ğ—, Number of iterations T, Number of generated samples Nğ—Œğ—’ğ—‡ğ—ğ—, Distance function dâ¢iâ¢sâ¢tÎµâ¢(â‹…,â‹…), Noise multiplier Ïƒ
      Output: Synthetic data Sğ—Œğ—’ğ—‡ğ—ğ—
1:S1â†ğ–±ğ–ºğ—‡ğ–½ğ—ˆğ—†â¢_â¢ğ– ğ–¯ğ–¨â¢(2â‹…Nğ—Œğ—’ğ—‡ğ—ğ—)
2:for t=1 to T do
3:     H=[] â–· Initialize histogram over St
4:     for xpâ¢râ¢iâ¢vâˆˆSğ—‰ğ—‹ğ—‚ğ— do
5:         i=argâ¡minjâˆˆ[n]â¡dâ¢iâ¢sâ¢tÎµâ¢(xpâ¢râ¢iâ¢v,St) â–· Compute closest synthetic candidate
6:         Hâ¢[i]=Hâ¢[i]+1      
7:     Hâ†H+ğ’©â¢(0,Ïƒâ¢I2â‹…Nğ—Œğ—’ğ—‡ğ—ğ—) â–· Add noise to ensure DP
8:     Hâ†maxâ¡(0,H) â–· Post-process element-wise
9:     ğ’«tâ†H/sâ¢uâ¢mâ¢(H) â–· Compute empirical distribution on St
10:     Stâ€²â† draw Nğ—Œğ—’ğ—‡ğ—ğ— samples with replacement from ğ’«t
11:     St+1â†ğ–µğ–ºğ—‹ğ—‚ğ–ºğ—ğ—‚ğ—ˆğ—‡â¢_â¢ğ– ğ–¯ğ–¨â¢(Stâ€²)
12:return ST
3.1 Workload-Aware Distance Function

Prior methods that applied Private Evolution to image (Lin et al., 2023) and text (Xie et al., 2024) data used public text and image embeddings, respectively, to measure the distance between candidate synthetic examples and the private examples. Choosing a sensible distance function for tabular records is less straightforward: public tabular embeddings (if they exist) likely wouldnâ€™t capture the features of unseen data, simple â„“p distance fails to account for differences in scale among columns.

Instead, we derive a workload-aware distance function. A private synthetic dataset is typically optimized for and evaluated on a particular workload of (linear) queries W={q1,â€¦,qk}. The workload error is typically some â„“p variation on:
	
ğ–¶ğ–¤ğ—‹ğ—‹ğ—ˆğ—‹â¢(Sğ—‰ğ—‹ğ—‚ğ—,Sğ—Œğ—’ğ—‡ğ—ğ—)=âˆ‘iâˆˆ[k]|qiâ¢(Sğ—‰ğ—‹ğ—‚ğ—)âˆ’qiâ¢(Sğ—Œğ—’ğ—‡ğ—ğ—)|.
	

Note that workload error is a function of pairs of datasests; however, the distance function we require is a function of pairs of individual records. We unpack the workload error further: assuming the queries are linear, then they correspond to a sum over a predicate on data records qiâ¢(ğ±)=âˆ‘jâˆˆ[n]Ïˆiâ¢(xj). Thus, for the given predicates Ïˆ=(Ïˆ1,â€¦,Ïˆk) corresponding to the queries in W, we will define the workload-aware distance function between a private record and synthetic candidate:
	
ğ–¶ğ–½ğ—‚ğ—Œğ—Ïˆâ¢(x,c)=âˆ‘iâˆˆk|Ïˆiâ¢(x)âˆ’Ïˆiâ¢(c)|.
	

A dataset of synthetic candidates with low workload-aware distance will have low workload error compared to the private data.
3.2 Experimental results

We evaluate our adapted private evolution algorithm on a modified version of NYC Taxi and Limousine Commission data using Gemini 1.0 Pro. We use data from January 2024, to to avoid data contamination between the public and private evaluation data (Google Cloud, 2023).

For our workload, we use a scaled â„“1-distance on numerical variables for each combination of categorical variables. We rescale the numerical variables to account for different value rangesâ€”for example, trip distance (in miles) versus trip duration (in seconds). As an initial experiment, we ran the algorithm without any privacy constraints, and we found that the workload error converges and the 1-way marginals of the synthetic data converges to the 1-way marginals on the private data as well (see Figure 1). Itâ€™s worth noting that, depending on the expressiveness of the foundation model, itâ€™s not a given that the PE algorithm would converge even without privacy.
Refer to caption
Figure 1: Top 1-way marginals on private (outlined) and synthetic (yellow) data. Bottom workload error of synthetic data over time for Private Evolution with Îµ=âˆ.

We then ran the same experiments but with a DP histogram instead of a nonprivate one. We experimented with various hyperparameters: how many initial random examples to use, how many iterations to use, how to split the budget across iterations, etc. We found that using increasing budget across runs worked better than an even or decreasing budget; additionally, having more candidates relative to Nğ—Œğ—’ğ—‡ğ—ğ— worked best, and finally, using fewer iterations worked bestâ€”in fact, using only a single iteration was the optimal setting we could find.

With differential privacy, the private evolution algorithm failed to beat two simple baselines: independent which privately computes all 1-way marginals and samples data from the product over the private marginals, and DP workload which directly computes the workload queries with DP, without generating any synthetic data. These two baselines are not the only two which weâ€™d hope to beat, rather, theyâ€™re the bare minimum. Moreover, querying any large foundation model hundreds of times is relatively slow.
What we learned

The observation that the workload-aware private evolution algorithm performs best with one shot data generation implies that: whatever marginal gains we get from iterating multiple times, they are outweighed by the privacy cost of composing over iterations. Additionally, while PE was developed for image and text domains where finetuning a foundation model is the main alternative for DP synthetic data, there is a vast literature on algorithms for private synthetic tabular data that do not require access to generative models. These lessons paved the way for our second attempt, which proved to be more successful, though still did not beat the current state-of-the-art.
4 Using Gemini-Generated Records as Public Data

As overviewed in Section 2.1, there is a substantial body of work on DP synthetic tabular data. Some state-of-the-art algorithms within this space make use of public data to improve the accuracy or efficiency of the algorithm on private data; however, for many applications such public data may not be available in the format required2, as is discussed in-depth in Liu et al. (2021b)[Section 6.1]. Our second approach uses Gemini generated data in lieu of this public data.
4.1 Approach overview

Using Geminiâ€™s structured output functionality, we prompt Gemini to generate data records with a response schema matching the column names and datatypes of our private dataset. Importantly, none of the private records influence the prompts to Geminiâ€”only the column names and datatypes do. This data generation occurs â€œofflineâ€ and in one shot with no loss of privacy budget. We call this dataset ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ—. Later, we plug this synthetic public dataset into various DP synthetic tabular algorithms that use public data as well as the private data.

One major benefit of the one-shot nature of this method (rather than querying Gemini interactively in a loop) is we can generate many synthetic public data records and reuse the same generated records when trying different approaches. This is not possible when the records are generated adaptively as in Private Evolution. Thus, this method takes advantage of our observations about PE. We begin with a high-level overview of how public data is incorporated into two DP synthetic data algorithms. For both, we consider what happens when we use Gemini-generated tabular data as the public data source for these algorithms.
ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–» Liu et al. (2021a)

The ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–» algorithm is an improvement of MWEM (Hardt et al., 2012), which we will not discuss in detail. The basic idea is to use public data to initialize the generating distribution over synthetic records and iteratively refine this distribution to reduce the workload error. The public records reduce the number of iterations required, by providing a â€œwarm startâ€ for the synthetic data distribution, along with reducing the data domain over which the distribution is estimated.

A key sub-routine of ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–» is to estimate a distribution that approximately matches some noisy statistics. Specifically, let Q denote a collection of linear queries and let y~=Qâ¢(D)+Î¾=âˆ‘xâˆˆDQâ¢(x)+Î¾ be the noisy answers to those queries on the sensitive data. ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–» finds a distribution supported on the â€œpublicâ€ data ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ—, and finds the weights to assign to each public record to minimize the â„“2 squared error to the noisy observations.
	
wâˆ—=argâ¢minwâˆˆâ„+âˆ¥âˆ‘xâˆˆğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ—wxQ(x)âˆ’y~âˆ¥22
	

When the public records are sufficiently representative, this method can work quite well. However, with small or unrepresentative public datasets, this method may not find a good distribution even in the absence of noise.
Gemini inference

We use the Gemini-generated records as the public records for the subroutine of ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–», calling this â€œGemini inferenceâ€, setting Q to be the query workload.
MST modified to take public data

The standard MST algorithm (McKenna et al., 2021) has three phases: selecting marginal queries, measuring the marginals with DP, and lastly using Private-PGM to post-process the noisy marginals and generate a synthetic dataset. We modify the final step (generation), replacing Private-PGM with the subroutine from ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–» that utilizes ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ—. This method differs from the Gemini inference approach primarily in how the queries Q are selected.
JAM

The JAM-PGM mechanism (Fuentes et al., 2024) was developed for marginal queries, and utilizes public data in a different manner. It privately decides whether to measure each marginal query on the public data or the private data in order to minimize the overall workload error. This mechanism has the benefit that it can utilize public data that is accurate on some, but not necessarily all, marginals. We run this mechanism as-is, using ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ— as the public data.
4.2 Baselines for Comparison

Because there are a wealth of methods for generating private synthetic data with and without public data, we have a fair number of baselines that we need to compare any new methods to. A number of works that privately finetune foundation models for tabular data omit comparisons to state-of-the-art methods for generating DP tabular data, so it is unclear if they outperform existing approaches.

We study two baselines that require no privacy. First, we consider an in-distribution public dataset: publically data drawn from the same distribution as the private data. This is essentially the lowest error we could hope for, up to sampling error; however, in-distribution public data is usually not available. Second, we consider using the Gemini data with no DP to answer workload queries.

Next, we consider a number of baselines that do not require public data.

    â€¢

    DP workload: we compute the queries directly using DP.
    â€¢

    Independent baseline: privately compute the 1-way marginals and sample records from the corresponding product distribution over marginals.
    â€¢

    MST algorithm: a tabular synthetic data algorithm that does not use public data.
    â€¢

    ğ–¯ğ–¬ğ–¶ğ—‰ğ—ğ–» with uniform data and JAM with uniform data: using data that is drawn uniformly from the domain as public data for these algorithms.

4.3 Experimental setup

Our private dataset is UCI Adult (Becker & Kohavi, 1996). Using this structured output constraint, we sample Gemini with top-k=1 and temperature=1 to generate 131,000 records in ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ—. We use 2-way marginals as our query workload to evaluate the fidelity of the DP synthetic data.
4.4 Results

Figure 2 (Left) shows the workload error versus epsilon for the baseline methods discussed. Among these methods, MST achieves the lowest workload error (except the in-distribution public dataset which is our unachievable â€œbest-case baselineâ€). Figure 2 (Right) shows all of the results for the baseline methods in addition to the methods that use ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ—. Notice that JAM with ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ— performs best overall; however, JAM performs equally well with uniform data. This is because JAM is simply using the private data to compute answers to the queries rather than utilizing the public data. Thus, JAM with ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ— is not better than the state-of-the-art methods on this dataset and query workload. In general, ğ–¦ğ–¾ğ—†â¢_â¢ğ–²ğ—’ğ—‡ğ—ğ— may capture 1-way marginals on the data better than uniform, however it is generally inaccurate on k-way marginals.
Refer to caption
Refer to caption
Figure 2: (Left) Workload error for baseline methods for generating tabular synthetic data without use of Gemini. (Right) Workload error for baseline methods and our one-shot methods that use API access to Gemini.
5 Conclusion and Future Work

We evaluated two methods for incorporating API access to Gemini for generating DP synthetic tabular data. While our methods did not beat state-of-the-art methods, this work motivates a number of future directions. First, as foundation models continue to improve, combining our methods with better models (e.g. models trained on more tabular data) could potentially improve the final accuracy, especially if the models are trained specifically for the tabular setting. Additionally, because Gemini uses word embeddings, perhaps doing some finetuning on publicly available tabular data could improve the quality of the Gemini-generated tabular records fed into our one-shot method. Lastly, perhaps there are ways to achieve better accuracy by combining Private Evolution and our one-shot approach. Using foundation models for DP synthetic data generation is still a very new area of research, with many avenues for improvements and breakthroughs.
