Foundations and Trends R
⃝in
Theoretical Computer Science
Vol. 9, Nos. 3–4 (2014) 211–407
c⃝2014 C. Dwork and A. Roth
DOI: 10.1561/0400000042
The Algorithmic Foundations
of Diﬀerential Privacy
Cynthia Dwork
Microsoft Research, USA
dwork@microsoft.com
Aaron Roth
University of Pennsylvania, USA
aaroth@cis.upenn.edu

Contents
Preface
3
1
The Promise of Diﬀerential Privacy
5
1.1
Privacy-preserving data analysis . . . . . . . . . . . . . . .
6
1.2
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . .
10
2
Basic Terms
11
2.1
The model of computation
. . . . . . . . . . . . . . . . .
11
2.2
Towards deﬁning private data analysis
. . . . . . . . . . .
12
2.3
Formalizing diﬀerential privacy . . . . . . . . . . . . . . .
15
2.4
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . .
26
3
Basic Techniques and Composition Theorems
28
3.1
Useful probabilistic tools
. . . . . . . . . . . . . . . . . .
28
3.2
Randomized response . . . . . . . . . . . . . . . . . . . .
29
3.3
The laplace mechanism . . . . . . . . . . . . . . . . . . .
30
3.4
The exponential mechanism . . . . . . . . . . . . . . . . .
37
3.5
Composition theorems . . . . . . . . . . . . . . . . . . . .
41
3.6
The sparse vector technique . . . . . . . . . . . . . . . . .
55
3.7
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . .
64
ii

iii
4
Releasing Linear Queries with Correlated Error
66
4.1
An oﬄine algorithm: SmallDB . . . . . . . . . . . . . . .
70
4.2
An online mechanism: private multiplicative weights . . . .
76
4.3
Bibliographical notes
. . . . . . . . . . . . . . . . . . . .
86
5
Generalizations
88
5.1
Mechanisms via α-nets
. . . . . . . . . . . . . . . . . . .
89
5.2
The iterative construction mechanism
. . . . . . . . . . .
91
5.3
Connections . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.4
Bibliographical notes
. . . . . . . . . . . . . . . . . . . . 115
6
Boosting for Queries
117
6.1
The boosting for queries algorithm . . . . . . . . . . . . . 119
6.2
Base synopsis generators
. . . . . . . . . . . . . . . . . . 130
6.3
Bibliographical notes
. . . . . . . . . . . . . . . . . . . . 139
7
When Worst-Case Sensitivity is Atypical
140
7.1
Subsample and aggregate . . . . . . . . . . . . . . . . . . 140
7.2
Propose-test-Release
. . . . . . . . . . . . . . . . . . . . 143
7.3
Stability and privacy . . . . . . . . . . . . . . . . . . . . . 150
8
Lower Bounds and Separation Results
158
8.1
Reconstruction attacks
. . . . . . . . . . . . . . . . . . . 159
8.2
Lower bounds for diﬀerential privacy . . . . . . . . . . . . 164
8.3
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . 170
9
Diﬀerential Privacy and Computational Complexity
172
9.1
Polynomial time curators . . . . . . . . . . . . . . . . . . 174
9.2
Some hard-to-Syntheticize distributions
. . . . . . . . . . 177
9.3
Polynomial time adversaries . . . . . . . . . . . . . . . . . 185
9.4
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . 187
10 Diﬀerential Privacy and Mechanism Design
189
10.1 Diﬀerential privacy as a solution concept . . . . . . . . . . 191
10.2 Diﬀerential privacy as a tool in mechanism design . . . . . 193
10.3 Mechanism design for privacy aware agents
. . . . . . . . 204
10.4 Bibliographical notes
. . . . . . . . . . . . . . . . . . . . 213

iv
11 Diﬀerential Privacy and Machine Learning
216
11.1 The sample complexity of diﬀerentially private
machine learning . . . . . . . . . . . . . . . . . . . . . . . 219
11.2 Diﬀerentially private online learning . . . . . . . . . . . . . 222
11.3 Empirical risk minimization . . . . . . . . . . . . . . . . . 227
11.4 Bibliographical notes
. . . . . . . . . . . . . . . . . . . . 230
12 Additional Models
231
12.1 The local model . . . . . . . . . . . . . . . . . . . . . . . 232
12.2 Pan-private streaming model . . . . . . . . . . . . . . . . 237
12.3 Continual observation . . . . . . . . . . . . . . . . . . . . 240
12.4 Average case error for query release . . . . . . . . . . . . . 248
12.5 Bibliographical notes
. . . . . . . . . . . . . . . . . . . . 252
13 Reﬂections
254
13.1 Toward practicing privacy . . . . . . . . . . . . . . . . . . 254
13.2 The diﬀerential privacy lens . . . . . . . . . . . . . . . . . 258
Appendices
260
A The Gaussian Mechanism
261
A.1
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . 266
B Composition Theorems for (ε, δ)-DP
267
B.1
Extension of Theorem 3.16 . . . . . . . . . . . . . . . . . 267
Acknowledgments
269
References
270

Abstract
The problem of privacy-preserving data analysis has a long history
spanning multiple disciplines. As electronic data about individuals
becomes increasingly detailed, and as technology enables ever more
powerful collection and curation of these data, the need increases for a
robust, meaningful, and mathematically rigorous deﬁnition of privacy,
together with a computationally rich class of algorithms that satisfy
this deﬁnition. Diﬀerential Privacy is such a deﬁnition.
After motivating and discussing the meaning of diﬀerential privacy,
the preponderance of this monograph is devoted to fundamental tech-
niques for achieving diﬀerential privacy, and application of these tech-
niques in creative combinations, using the query-release problem as an
ongoing example. A key point is that, by rethinking the computational
goal, one can often obtain far better results than would be achieved by
methodically replacing each step of a non-private computation with a
diﬀerentially private implementation. Despite some astonishingly pow-
erful computational results, there are still fundamental limitations —
not just on what can be achieved with diﬀerential privacy but on what
can be achieved with any method that protects against a complete
breakdown in privacy. Virtually all the algorithms discussed herein
maintain diﬀerential privacy against adversaries of arbitrary compu-
tational power. Certain algorithms are computationally intensive, oth-
ers are eﬃcient. Computational complexity for the adversary and the
algorithm are both discussed.
We then turn from fundamentals to applications other than query-
release, discussing diﬀerentially private methods for mechanism design
and machine learning. The vast majority of the literature on diﬀeren-
tially private algorithms considers a single, static, database that is sub-
ject to many analyses. Diﬀerential privacy in other models, including
distributed databases and computations on data streams is discussed.

2
Finally, we note that this work is meant as a thorough introduc-
tion to the problems and techniques of diﬀerential privacy, but is not
intended to be an exhaustive survey — there is by now a vast amount of
work in diﬀerential privacy, and we can cover only a small portion of it.
C. Dwork and A. Roth. The Algorithmic Foundations of Diﬀerential Privacy. Foun-
dations and Trends
R
⃝in Theoretical Computer Science, vol. 9, nos. 3–4, pp. 211–407,
2014.
DOI: 10.1561/0400000042.

Preface
The problem of privacy-preserving data analysis has a long history
spanning multiple disciplines. As electronic data about individuals
becomes increasingly detailed, and as technology enables ever more
powerful collection and curation of these data, the need increases for a
robust, meaningful, and mathematically rigorous deﬁnition of privacy,
together with a computationally rich class of algorithms that satisfy
this deﬁnition. Diﬀerential Privacy is such a deﬁnition.
After motivating and discussing the meaning of diﬀerential privacy,
the preponderance of the book is devoted to fundamental techniques
for achieving diﬀerential privacy, and application of these techniques in
creative combinations (Sections 3–7), using the query-release problem
as an ongoing example. A key point is that, by rethinking the com-
putational goal, one can often obtain far better results than would be
achieved by methodically replacing each step of a non-private compu-
tation with a diﬀerentially private implementation.
Despite some astonishingly powerful computational results, there
are still fundamental limitations — not just on what can be achieved
with diﬀerential privacy but on what can be achieved with any method
that protects against a complete breakdown in privacy (Section 8).
Virtually all the algorithms discussed in this book maintain
diﬀerential privacy against adversaries of arbitrary computational
power. Certain algorithms are computationally intensive, others are
3

4
eﬃcient. Computational complexity for the adversary and the algo-
rithm are both discussed in Section 9.
In Sections 10 and 11 we turn from fundamentals to applications
other than query-release, discussing diﬀerentially private methods for
mechanism design and machine learning. The vast majority of the lit-
erature on diﬀerentially private algorithms considers a single, static,
database that is subject to many analyses. Diﬀerential privacy in other
models, including distributed databases and computations on data
streams is discussed in Section 12.
Finally, we note that this book is meant as a thorough introduc-
tion to the problems and techniques of diﬀerential privacy, but is not
intended to be an exhaustive survey — there is by now a vast amount of
work in diﬀerential privacy, and we can cover only a small portion of it.

1
The Promise of Diﬀerential Privacy
“Diﬀerential privacy” describes a promise, made by a data holder, or
curator, to a data subject: “You will not be aﬀected, adversely or oth-
erwise, by allowing your data to be used in any study or analysis,
no matter what other studies, data sets, or information sources, are
available.” At their best, diﬀerentially private database mechanisms
can make conﬁdential data widely available for accurate data analysis,
without resorting to data clean rooms, data usage agreements, data pro-
tection plans, or restricted views. Nonetheless, data utility will eventu-
ally be consumed: the Fundamental Law of Information Recovery states
that overly accurate answers to too many questions will destroy privacy
in a spectacular way.1 The goal of algorithmic research on diﬀerential
privacy is to postpone this inevitability as long as possible.
Diﬀerential privacy addresses the paradox of learning nothing about
an individual while learning useful information about a population. A
medical database may teach us that smoking causes cancer, aﬀecting
an insurance company’s view of a smoker’s long-term medical costs.
Has the smoker been harmed by the analysis? Perhaps — his insurance
1This result, proved in Section 8.1, applies to all techniques for privacy-preserving
data analysis, and not just to diﬀerential privacy.
5

6
The Promise of Diﬀerential Privacy
premiums may rise, if the insurer knows he smokes. He may also be
helped — learning of his health risks, he enters a smoking cessation
program. Has the smoker’s privacy been compromised? It is certainly
the case that more is known about him after the study than was known
before, but was his information “leaked”? Diﬀerential privacy will take
the view that it was not, with the rationale that the impact on the
smoker is the same independent of whether or not he was in the study.
It is the conclusions reached in the study that aﬀect the smoker, not
his presence or absence in the data set.
Diﬀerential privacy ensures that the same conclusions, for example,
smoking causes cancer, will be reached, independent of whether any
individual opts into or opts out of the data set. Speciﬁcally, it ensures
that any sequence of outputs (responses to queries) is “essentially”
equally likely to occur, independent of the presence or absence of any
individual. Here, the probabilities are taken over random choices made
by the privacy mechanism (something controlled by the data curator),
and the term “essentially” is captured by a parameter, ε. A smaller ε
will yield better privacy (and less accurate responses).
Diﬀerential privacy is a deﬁnition, not an algorithm. For a given
computational task T and a given value of ε there will be many diﬀer-
entially private algorithms for achieving T in an ε-diﬀerentially private
manner. Some will have better accuracy than others. When ε is small,
ﬁnding a highly accurate ε-diﬀerentially private algorithm for T can be
diﬃcult, much as ﬁnding a numerically stable algorithm for a speciﬁc
computational task can require eﬀort.
1.1
Privacy-preserving data analysis
Diﬀerential privacy is a deﬁnition of privacy tailored to the problem
of privacy-preserving data analysis. We brieﬂy address some concerns
with other approaches to this problem.
Data Cannot be Fully Anonymized and Remain Useful. Generally
speaking, the richer the data, the more interesting and useful it is.
This has led to notions of “anonymization” and “removal of person-
ally identiﬁable information,” where the hope is that portions of the

1.1. Privacy-preserving data analysis
7
data records can be suppressed and the remainder published and used
for analysis. However, the richness of the data enables “naming” an
individual by a sometimes surprising collection of ﬁelds, or attributes,
such as the combination of zip code, date of birth, and sex, or even the
names of three movies and the approximate dates on which an indi-
vidual watched these movies. This “naming” capability can be used in
a linkage attack to match “anonymized” records with non-anonymized
records in a diﬀerent dataset. Thus, the medical records of the gover-
nor of Massachussetts were identiﬁed by matching anonymized medical
encounter data with (publicly available) voter registration records, and
Netﬂix subscribers whose viewing histories were contained in a collec-
tion of anonymized movie records published by Netﬂix as training data
for a competition on recommendation were identiﬁed by linkage with
the Internet Movie Database (IMDb).
Diﬀerential privacy neutralizes linkage attacks: since being diﬀer-
entially private is a property of the data access mechanism, and is
unrelated to the presence or absence of auxiliary information available
to the adversary, access to the IMDb would no more permit a linkage
attack to someone whose history is in the Netﬂix training set than to
someone not in the training set.
Re-Identiﬁcation of “Anonymized” Records is Not the Only Risk. Re-
identiﬁcation of “anonymized” data records is clearly undesirable, not
only because of the re-identiﬁcation per se, which certainly reveals
membership in the data set, but also because the record may contain
compromising information that, were it tied to an individual, could
cause harm. A collection of medical encounter records from a speciﬁc
urgent care center on a given date may list only a small number of
distinct complaints or diagnoses. The additional information that a
neighbor visited the facility on the date in question gives a fairly nar-
row range of possible diagnoses for the neighbor’s condition. The fact
that it may not be possible to match a speciﬁc record to the neighbor
provides minimal privacy protection to the neighbor.
Queries Over Large Sets are Not Protective. Questions about speciﬁc
individuals cannot be safely answered with accuracy, and indeed one

8
The Promise of Diﬀerential Privacy
might wish to reject them out of hand (were it computationally fea-
sible to recognize them). Forcing queries to be over large sets is not
a panacea, as shown by the following diﬀerencing attack. Suppose it
is known that Mr. X is in a certain medical database. Taken together,
the answers to the two large queries “How many people in the database
have the sickle cell trait?” and “How many people, not named X, in the
database have the sickle cell trait?” yield the sickle cell status of Mr. X.
Query Auditing Is Problematic. One might be tempted to audit the
sequence of queries and responses, with the goal of interdicting any
response if, in light of the history, answering the current query would
compromise privacy. For example, the auditor may be on the lookout
for pairs of queries that would constitute a diﬀerencing attack. There
are two diﬃculties with this approach. First, it is possible that refusing
to answer a query is itself disclosive. Second, query auditing can be
computationally infeasible; indeed if the query language is suﬃciently
rich there may not even exist an algorithmic procedure for deciding if
a pair of queries constitutes a diﬀerencing attack.
Summary Statistics are Not “Safe.” In some sense, the failure of
summary statistics as a privacy solution concept is immediate from
the diﬀerencing attack just described. Other problems with summary
statistics include a variety of reconstruction attacks against a database
in which each individual has a “secret bit” to be protected. The utility
goal may be to permit, for example, questions of the form “How many
people satisfying property P have secret bit value 1?” The goal of the
adversary, on the other hand, is to signiﬁcantly increase his chance
of guessing the secret bits of individuals. The reconstruction attacks
described in Section 8.1 show the diﬃculty of protecting against even
a linear number of queries of this type: unless suﬃcient inaccuracy is
introduced almost all the secret bits can be reconstructed.
A striking illustration of the risks of releasing summary statistics
is in an application of a statistical technique, originally intended for
conﬁrming or refuting the presence of an individual’s DNA in a foren-
sic mix, to ruling an individual in or out of a genome-wide association
study. According to a Web site of the Human Genome Project, “Single
nucleotide polymorphisms, or SNPs (pronounced “snips”), are DNA

1.1. Privacy-preserving data analysis
9
sequence variations that occur when a single nucleotide (A,T,C, or G)
in the genome sequence is altered. For example a SNP might change
the DNA sequence AAGGCTAA to ATGGCTAA.” In this case we say
there are two alleles: A and T. For such a SNP we can ask, given a
particular reference population, what are the frequencies of each of the
two possible alleles? Given the allele frequencies for SNPs in the ref-
erence population, we can examine how these frequencies may diﬀer
for a subpopulation that has a particular disease (the “case” group),
looking for alleles that are associated with the disease. For this reason,
genome-wide association studies may contain the allele frequencies of
the case group for large numbers of SNPs. By deﬁnition, these allele
frequencies are only aggregated statistics, and the (erroneous) assump-
tion has been that, by virtue of this aggregation, they preserve privacy.
However, given the genomic data of an individual, it is theoretically
possible to determine if the individual is in the case group (and, there-
fore, has the disease). In response, the National Institutes of Health
and Wellcome Trust terminated public access to aggregate frequency
data from the studies they fund.
This is a challenging problem even for diﬀerential privacy, due to
the large number — hundreds of thousands or even one million — of
measurements involved and the relatively small number of individuals
in any case group.
“Ordinary” Facts are Not “OK.” Revealing “ordinary” facts, such as
purchasing bread, may be problematic if a data subject is followed over
time. For example, consider Mr. T, who regularly buys bread, year after
year, until suddenly switching to rarely buying bread. An analyst might
conclude Mr. T most likely has been diagnosed with Type 2 diabetes.
The analyst might be correct, or might be incorrect; either way Mr. T
is harmed.
“Just a Few.” In some cases a particular technique may in fact provide
privacy protection for “typical” members of a data set, or more gen-
erally, “most” members. In such cases one often hears the argument
that the technique is adequate, as it compromises the privacy of “just
a few” participants. Setting aside the concern that outliers may be pre-
cisely those people for whom privacy is most important, the “just a few”

10
The Promise of Diﬀerential Privacy
philosophy is not intrinsically without merit: there is a social judgment,
a weighing of costs and beneﬁts, to be made. A well-articulated deﬁni-
tion of privacy consistent with the “just a few” philosophy has yet to
be developed; however, for a single data set, “just a few” privacy can be
achieved by randomly selecting a subset of rows and releasing them in
their entirety (Lemma 4.3, Section 4). Sampling bounds describing the
quality of statistical analysis that can be carried out on random sub-
samples govern the number of rows to be released. Diﬀerential privacy
provides an alternative when the “just a few” philosophy is rejected.
1.2
Bibliographic notes
Sweeney [81] linked voter registration records to “anonymized” medical
encounter data; Narayanan and Shmatikov carried out a linkage attack
against anonymized ranking data published by Netﬂix [65]. The work
on presence in a forensic mix is due to Homer et al. [46]. The ﬁrst
reconstruction attacks were due to Dinur and Nissim [18].

2
Basic Terms
This section motivates and presents the formal deﬁnition of diﬀerential
privacy, and enumerates some of its key properties.
2.1
The model of computation
We assume the existence of a trusted and trustworthy curator who
holds the data of individuals in a database D, typically comprised of
some number n of rows. The intuition is that each row contains the
data of a single individual, and, still speaking intuitively, the privacy
goal is to simultaneously protect every individual row while permitting
statistical analysis of the database as a whole.
In the non-interactive, or oﬄine, model the curator produces some
kind of object, such as a “synthetic database,” collection of summary
statistics, or “sanitized database” once and for all. After this release the
curator plays no further role and the original data may be destroyed.
A query is a function to be applied to a database. The interactive,
or online, model permits the data analyst to ask queries adaptively,
deciding which query to pose next based on the observed responses to
previous queries.
11

12
Basic Terms
The trusted curator can be replaced by a protocol run by the set
of individuals, using the cryptographic techniques for secure multi-
party protocols, but for the most part we will not be appealing to
cryptographic assumptions. Section 12 describes this and other models
studied in the literature.
When all the queries are known in advance the non-interactive
model should give the best accuracy, as it is able to correlate noise
knowing the structure of the queries. In contrast, when no information
about the queries is known in advance, the non-interactive model poses
severe challenges, as it must provide answers to all possible queries.
As we will see, to ensure privacy, or even to prevent privacy catastro-
phes, accuracy will necessarily deteriorate with the number of questions
asked, and providing accurate answers to all possible questions will be
infeasible.
A privacy mechanism, or simply a mechanism, is an algorithm that
takes as input a database, a universe X of data types (the set of all
possible database rows), random bits, and, optionally, a set of queries,
and produces an output string. The hope is that the output string can
be decoded to produce relatively accurate answers to the queries, if
the latter are present. If no queries are presented then we are in the
non-interactive case, and the hope is that the output string can be
interpreted to provide answers to future queries.
In some cases we may require that the output string be a synthetic
database. This is a multiset drawn from the universe X of possible
database rows. The decoding method in this case is to carry out the
query on the synthetic database and then to apply some sort of simple
transformation, such as multiplying by a scaling factor, to obtain an
approximation to the the true answer to the query.
2.2
Towards deﬁning private data analysis
A natural approach to deﬁning privacy in the context of data analy-
sis is to require that the analyst knows no more about any individual
in the data set after the analysis is completed than she knew before
the analysis was begun. It is also natural to formalize this goal by

2.2. Towards deﬁning private data analysis
13
requiring that the adversary’s prior and posterior views about an indi-
vidual (i.e., before and after having access to the database) shouldn’t
be “too diﬀerent,” or that access to the database shouldn’t change the
adversary’s views about any individual “too much.” However, if the
database teaches anything at all, this notion of privacy is unachiev-
able. For example, suppose the adversary’s (incorrect) prior view is
that everyone has 2 left feet. Access to the statistical database teaches
that almost everyone has one left foot and one right foot. The adversary
now has a very diﬀerent view of whether or not any given respondent
has two left feet.
Part of the appeal of before/after, or “nothing is learned,” approach
to deﬁning privacy is the intuition that if nothing is learned about
an individual then the individual cannot be harmed by the analysis.
However, the “smoking causes cancer” example shows this intuition to
be ﬂawed; the culprit is auxiliary information (Mr. X smokes).
The “nothing is learned” approach to deﬁning privacy is reminiscent
of semantic security for a cryptosystem. Roughly speaking, semantic
security says that nothing is learned about the plaintext (the unen-
crypted message) from the ciphertext. That is, anything known about
the plaintext after seeing the ciphertext was known before seeing the
ciphertext. So if there is auxiliary information saying that the cipher-
text is an encryption of either “dog” or “cat,” then the ciphertext
leaks no further information about which of “dog” or “cat” has been
encrypted. Formally, this is modeled by comparing the ability of the
eavesdropper to guess which of “dog” and “cat” has been encrypted
to the ability of a so-called adversary simulator, who has the auxil-
iary information but does not have access to the ciphertext, to guess
the same thing. If for every eavesdropping adversary, and all auxiliary
information (to which both the adversary and the simulator are privy),
the adversary simulator has essentially the same odds of guessing as
does the eavesdropper, then the system enjoys semantic security. Of
course, for the system to be useful, the legitimate receiver must be able
to correctly decrypt the message; otherwise semantic security can be
achieved trivially.
We know that, under standard computational assumptions, seman-
tically secure cryptosystems exist, so why can we not build semantically

14
Basic Terms
secure private database mechanisms that yield answers to queries while
keeping individual rows secret?
First, the analogy is not perfect: in a semantically secure cryp-
tosystem there are three parties: the message sender (who encrypts
the plaintext message), the message receiver (who decrypts the cipher-
text), and the eavesdropper (who is frustrated by her inability to learn
anything about the plaintext that she did not already know before
it was sent). In contrast, in the setting of private data analysis there
are only two parties: the curator, who runs the privacy mechanism
(analogous to the sender) and the data analyst, who receives the infor-
mative responses to queries (like the message receiver) and also tries to
squeeze out privacy-compromising information about individuals (like
the eavesdropper). Because the legitimate receiver is the same party as
the snooping adversary, the analogy to encryption is ﬂawed: denying
all information to the adversary means denying all information to the
data analyst.
Second, as with an encryption scheme, we require the privacy mech-
anism to be useful, which means that it teaches the analyst something
she did not previously know. This teaching is unavailable to an adver-
sary simulator; that is, no simulator can “predict” what the analyst
has learned. We can therefore look at the database as a weak source
of random (unpredictable) bits, from which we can extract some very
high quality randomness to be used as a random pad. This can be used
in an encryption technique in which a secret message is added to a
random value (the “random pad”) in order to produce a string that
information-theoretically hides the secret. Only someone knowing the
random pad can learn the secret; any party that knows nothing about
the pad learns nothing at all about the secret, no matter his or her
computational power. Given access to the database, the analyst can
learn the random pad, but the adversary simulator, not given access
to the database, learns nothing at all about the pad. Thus, given as
auxiliary information the encryption of a secret using the random pad,
the analyst can decrypt the secret, but the adversary simulator learns
nothing at all about the secret. This yields a huge disparity between
the ability of the adversary/analyst to learn the secret and the ability

2.3. Formalizing diﬀerential privacy
15
of the adversary simulator to do the same thing, eliminating all hope
of anything remotely resembling semantic security.
The obstacle in both the smoking causes cancer example and the
hope for semantic security is auxiliary information. Clearly, to be
meaningful, a privacy guarantee must hold even in the context of
“reasonable” auxiliary knowledge, but separating reasonable from arbi-
trary auxiliary knowledge is problematic. For example, the analyst
using a government database might be an employee at a major search
engine company. What are “reasonable” assumptions about the auxil-
iary knowledge information available to such a person?
2.3
Formalizing diﬀerential privacy
We will begin with the technical deﬁnition of diﬀerential privacy, and
then go on to interpret it. Diﬀerential privacy will provide privacy by
process; in particular it will introduce randomness. An early exam-
ple of privacy by randomized process is randomized response, a tech-
nique developed in the social sciences to collect statistical information
about embarassing or illegal behavior, captured by having a property P.
Study participants are told to report whether or not they have prop-
erty P as follows:
1. Flip a coin.
2. If tails, then respond truthfully.
3. If heads, then ﬂip a second coin and respond “Yes” if heads and
“No” if tails.
“Privacy” comes from the plausible deniability of any outcome; in par-
ticular, if having property P corresponds to engaging in illegal behavior,
even a “Yes” answer is not incriminating, since this answer occurs with
probability at least 1/4 whether or not the respondent actually has
property P. Accuracy comes from an understanding of the noise gener-
ation procedure (the introduction of spurious “Yes” and “No” answers
from the randomization): The expected number of “Yes” answers is
1/4 times the number of participants who do not have property P plus
3/4 the number having property P. Thus, if p is the true fraction of

16
Basic Terms
participants having property P, the expected number of “Yes” answers
is (1/4)(1−p)+(3/4)p = (1/4)+p/2. Thus, we can estimate p as twice
the fraction answering “Yes” minus 1/2, that is, 2((1/4) + p/2) −1/2.
Randomization is essential; more precisely, any non-trivial privacy
guarantee that holds regardless of all present or even future sources
of auxiliary information, including other databases, studies, Web sites,
on-line communities, gossip, newspapers, government statistics, and so
on, requires randomization. This follows from a simple hybrid argu-
ment, which we now sketch. Suppose, for the sake of contradiction,
that we have a non-trivial deterministic algorithm. Non-triviality says
that there exists a query and two databases that yield diﬀerent out-
puts under this query. Changing one row at a time we see there exists
a pair of databases diﬀering only in the value of a single row, on which
the same query yields diﬀerent outputs. An adversary knowing that
the database is one of these two almost identical databases learns the
value of the data in the unknown row.
We will therefore need to discuss the input and output space of
randomized algorithms. Throughout this monograph we work with dis-
crete probability spaces. Sometimes we will describe our algorithms
as sampling from continuous distributions, but these should always
be discretized to ﬁnite precision in an appropriately careful way (see
Remark 2.1 below). In general, a randomized algorithm with domain
A and (discrete) range B will be associated with a mapping from A to
the probability simplex over B, denoted ∆(B):
Deﬁnition 2.1 (Probability Simplex). Given a discrete set B, the prob-
ability simplex over B, denoted ∆(B) is deﬁned to be:
∆(B) =


x ∈R|B| : xi ≥0 for all i and
|B|
X
i=1
xi = 1



Deﬁnition 2.2 (Randomized Algorithm). A randomized algorithm M
with domain A and discrete range B is associated with a mapping
M : A →∆(B). On input a ∈A, the algorithm M outputs M(a) = b
with probability (M(a))b for each b ∈B. The probability space is over
the coin ﬂips of the algorithm M.

2.3. Formalizing diﬀerential privacy
17
We will think of databases x as being collections of records from a
universe X. It will often be convenient to represent databases by their
histograms: x ∈N|X|, in which each entry xi represents the number of
elements in the database x of type i ∈X (we abuse notation slightly, let-
ting the symbol N denote the set of all non-negative integers, including
zero). In this representation, a natural measure of the distance between
two databases x and y will be their ℓ1 distance:
Deﬁnition 2.3 (Distance Between Databases). The ℓ1 norm of a
database x is denoted ∥x∥1 and is deﬁned to be:
∥x∥1 =
|X|
X
i=1
|xi| .
The ℓ1 distance between two databases x and y is ∥x −y∥1
Note that ∥x∥1 is a measure of the size of a database x (i.e., the
number of records it contains), and ∥x −y∥1 is a measure of how many
records diﬀer between x and y.
Databases may also be represented by multisets of rows (elements
of X) or even ordered lists of rows, which is a special case of a set,
where the row number becomes part of the name of the element. In this
case distance between databases is typically measured by the Hamming
distance, i.e., the number of rows on which they diﬀer.
However, unless otherwise noted, we will use the histogram
representation described above. (Note, however, that even when the
histogram notation is more mathematically convenient, in actual
implementations, the multiset representation will often be much more
concise).
We are now ready to formally deﬁne diﬀerential privacy, which intu-
itively will guarantee that a randomized algorithm behaves similarly on
similar input databases.
Deﬁnition 2.4 (Diﬀerential Privacy). A randomized algorithm M with
domain N|X| is (ε, δ)-diﬀerentially private if for all S ⊆Range(M) and
for all x, y ∈N|X| such that ∥x −y∥1 ≤1:
Pr[M(x) ∈S] ≤exp(ε) Pr[M(y) ∈S] + δ,

18
Basic Terms
where the probability space is over the coin ﬂips of the mechanism M.
If δ = 0, we say that M is ε-diﬀerentially private.
Typically we are interested in values of δ that are less than the
inverse of any polynomial in the size of the database. In particular,
values of δ on the order of 1/∥x∥1 are very dangerous: they permit “pre-
serving privacy” by publishing the complete records of a small number
of database participants — precisely the “just a few” philosophy dis-
cussed in Section 1.
Even when δ is negligible, however, there are theoretical distinc-
tions between (ε, 0)- and (ε, δ)-diﬀerential privacy. Chief among these
is what amounts to a switch of quantiﬁcation order. (ε, 0)-diﬀerential
privacy ensures that, for every run of the mechanism M(x), the out-
put observed is (almost) equally likely to be observed on every neigh-
boring database, simultaneously. In contrast (ε, δ)-diﬀerential privacy
says that for every pair of neighboring databases x, y, it is extremely
unlikely that, ex post facto the observed value M(x) will be much more
or much less likely to be generated when the database is x than when
the database is y. However, given an output ξ ∼M(x) it may be possi-
ble to ﬁnd a database y such that ξ is much more likely to be produced
on y than it is when the database is x. That is, the mass of ξ in the
distribution M(y) may be substantially larger than its mass in the
distribution M(x).
The quantity
L(ξ)
M(x)∥M(y) = ln
Pr[M(x) = ξ]
Pr[M(y) = ξ]

is important to us; we refer to it as the privacy loss incurred by observ-
ing ξ. This loss might be positive (when an event is more likely under x
than under y) or it might be negative (when an event is more likely
under y than under x). As we will see in Lemma 3.17, (ε, δ)-diﬀerential
privacy ensures that for all adjacent x, y, the absolute value of the pri-
vacy loss will be bounded by ε with probability at least 1−δ. As always,
the probability space is over the coins of the mechanism M.
Diﬀerential privacy is immune to post-processing: A data analyst,
without additional knowledge about the private database, cannot com-
pute a function of the output of a private algorithm M and make it

2.3. Formalizing diﬀerential privacy
19
less diﬀerentially private. That is, if an algorithm protects an individ-
ual’s privacy, then a data analyst cannot increase privacy loss — either
under the formal deﬁnition or even in any intuitive sense — simply by
sitting in a corner and thinking about the output of the algorithm. For-
mally, the composition of a data-independent mapping f with an (ε, δ)-
diﬀerentially private algorithm M is also (ε, δ) diﬀerentially private:
Proposition 2.1 (Post-Processing). Let M : N|X| →R be a randomized
algorithm that is (ε, δ)-diﬀerentially private. Let f : R →R′ be an
arbitrary randomized mapping. Then f ◦M : N|X| →R′ is (ε, δ)-
diﬀerentially private.
Proof. We
prove
the
proposition
for
a
deterministic
function
f : R →R′. The result then follows because any randomized mapping
can be decomposed into a convex combination of deterministic func-
tions, and a convex combination of diﬀerentially private mechanisms is
diﬀerentially private.
Fix any pair of neighboring databases x, y with ∥x −y∥1 ≤1, and
ﬁx any event S ⊆R′. Let T = {r ∈R : f(r) ∈S}. We then have:
Pr[f(M(x)) ∈S] = Pr[M(x) ∈T]
≤exp(ϵ) Pr[M(y) ∈T] + δ
= exp(ϵ) Pr[f(M(y)) ∈S] + δ
which was what we wanted.
It follows immediately from Deﬁnition 2.4 that (ε, 0)-diﬀerential pri-
vacy composes in a straightforward way: the composition of two (ε, 0)-
diﬀerentially private mechanisms is (2ε, 0)-diﬀerentially private. More
generally (Theorem 3.16), “the epsilons and the deltas add up”: the
composition of k diﬀerentially private mechanisms, where the ith mech-
anism is (εi, δi)-diﬀerentially private, for 1 ≤i ≤k, is (P
i εi, P
i δi)-
diﬀerentially private.
Group privacy for (ε, 0)-diﬀerentially private mechanisms also fol-
lows immediately from Deﬁnition 2.4, with the strength of the privacy
guarantee drops linearly with the size of the group.

20
Basic Terms
Theorem 2.2. Any (ε, 0)-diﬀerentially private mechanism M is (kε, 0)-
diﬀerentially private for groups of size k. That is, for all ∥x −y∥1 ≤k
and all S ⊆Range(M)
Pr[M(x) ∈S] ≤exp(kε) Pr[M(y) ∈S],
where the probability space is over the coin ﬂips of the mechanism M.
This addresses, for example, the question of privacy in surveys that
include multiple family members.1
More generally, composition and group privacy are not the same
thing and the improved composition bounds in Section 3.5.2 (Theo-
rem 3.20), which substantially improve upon the factor of k, do not —
and cannot — yield the same gains for group privacy, even when δ = 0.
2.3.1
What diﬀerential privacy promises
An Economic View. Diﬀerential privacy promises to protect individ-
uals from any additional harm that they might face due to their data
being in the private database x that they would not have faced had
their data not been part of x. Although individuals may indeed face
harm once the results M(x) of a diﬀerentially private mechanism M
have been released, diﬀerential privacy promises that the probability of
harm was not signiﬁcantly increased by their choice to participate. This
is a very utilitarian deﬁnition of privacy, because when an individual is
deciding whether or not to include her data in a database that will be
used in a diﬀerentially private manner, it is exactly this diﬀerence that
she is considering: the probability of harm given that she participates,
as compared to the probability of harm given that she does not partic-
ipate. She has no control over the remaining contents of the database.
Given the promise of diﬀerential privacy, she is assured that she should
1However, as the group gets larger, the privacy guarantee deteriorates, and this
is what we want: clearly, if we replace an entire surveyed population, say, of cancer
patients, with a completely diﬀerent group of respondents, say, healthy teenagers,
we should get diﬀerent answers to queries about the fraction of respondents who
regularly run three miles each day. Although something similar holds for (ε, δ)-
diﬀerential privacy, the approximation term δ takes a big hit, and we only obtain
(kε, ke(k−1)εδ)-diﬀerential privacy for groups of size k.

2.3. Formalizing diﬀerential privacy
21
be almost indiﬀerent between participating and not, from the point of
view of future harm. Given any incentive — from altruism to monetary
reward — diﬀerential privacy may convince her to allow her data to
be used. This intuition can be formalized in a utility-theoretic sense,
which we here brieﬂy sketch.
Consider an individual i who has arbitrary preferences over the
set of all possible future events, which we denote by A. These pref-
erences are expressed by a utility function ui : A →R≥0, and we
say that individual i experiences utility ui(a) in the event that a ∈A
comes to pass. Suppose that x ∈N|X| is a data-set containing indi-
vidual is private data, and that M is an ε-diﬀerentially private algo-
rithm. Let y be a data-set that is identical to x except that it does not
include the data of individual i (in particular, ∥x −y∥1 = 1), and let
f : Range(M) →∆(A) be the (arbitrary) function that determines the
distribution over future events A, conditioned on the output of mech-
anism M. By the guarantee of diﬀerential privacy, together with the
resilience to arbitrary post-processing guaranteed by Proposition 2.1,
we have:
Ea∼f(M(x))[ui(a)] =
X
a∈A
ui(a) ·
Pr
f(M(x))[a]
≤
X
a∈A
ui(a) · exp(ε)
Pr
f(M(y))[a]
= exp(ε)Ea∼f(M(y))[ui(a)]
Similarly,
Ea∼f(M(x))[ui(a)] ≥exp(−ε)Ea∼f(M(y))[ui(a)].
Hence, by promising a guarantee of ε-diﬀerential privacy, a data analyst
can promise an individual that his expected future utility will not be
harmed by more than an exp(ε) ≈(1+ε) factor. Note that this promise
holds independently of the individual is utility function ui, and holds
simultaneously for multiple individuals who may have completely dif-
ferent utility functions.

22
Basic Terms
2.3.2
What diﬀerential privacy does not promise
As we saw in the Smoking Causes Cancer example, while diﬀerential
privacy is an extremely strong guarantee, it does not promise uncon-
ditional freedom from harm. Nor does it create privacy where none
previously exists. More generally, diﬀerential privacy does not guaran-
tee that what one believes to be one’s secrets will remain secret. It
merely ensures that one’s participation in a survey will not in itself
be disclosed, nor will participation lead to disclosure of any speciﬁcs
that one has contributed to the survey. It is very possible that conclu-
sions drawn from the survey may reﬂect statistical information about
an individual. A health survey intended to discover early indicators of
a particular ailment may produce strong, even conclusive results; that
these conclusions hold for a given individual is not evidence of a diﬀer-
ential privacy violation; the individual may not even have participated
in the survey (again, diﬀerential privacy ensures that these conclusive
results would be obtained with very similar probability whether or not
the individual participated in the survey). In particular, if the survey
teaches us that speciﬁc private attributes correlate strongly with pub-
licly observable attributes, this is not a violation of diﬀerential privacy,
since this same correlation would be observed with almost the same
probability independent of the presence or absence of any respondent.
Qualitative Properties of Diﬀerential Privacy. Having introduced
and formally deﬁned diﬀerential privacy, we recaptiluate its key desir-
able qualities.
1. Protection against arbitrary risks, moving beyond protection
against re-identiﬁcation.
2. Automatic neutralization of linkage attacks, including all those
attempted with all past, present, and future datasets and other
forms and sources of auxiliary information.
3. Quantiﬁcation of privacy loss. Diﬀerential privacy is not a binary
concept, and has a measure of privacy loss. This permits compar-
isons among diﬀerent techniques: for a ﬁxed bound on privacy
loss, which technique provides better accuracy? For a ﬁxed accu-
racy, which technique provides better privacy?

2.3. Formalizing diﬀerential privacy
23
4. Composition. Perhaps most crucially, the quantiﬁcation of loss
also permits the analysis and control of cumulative privacy loss
over multiple computations. Understanding the behavior of diﬀer-
entially private mechanisms under composition enables the design
and analysis of complex diﬀerentially private algorithms from
simpler diﬀerentially private building blocks.
5. Group Privacy. Diﬀerential privacy permits the analysis and con-
trol of privacy loss incurred by groups, such as families.
6. Closure Under Post-Processing Diﬀerential privacy is immune to
post-processing: A data analyst, without additional knowledge
about the private database, cannot compute a function of the
output of a diﬀerentially private algorithm M and make it less
diﬀerentially private. That is, a data analyst cannot increase pri-
vacy loss, either under the formal deﬁnition or even in any intu-
itive sense, simply by sitting in a corner and thinking about the
output of the algorithm, no matter what auxiliary information is
available.
These are the signal attributes of diﬀerential privacy. Can we prove
a converse? That is, do these attributes, or some subset thereof, imply
diﬀerential privacy? Can diﬀerential privacy be weakened in these
respects and still be meaningful? These are open questions.
2.3.3
Final remarks on the deﬁnition
The Granularity of Privacy.
Claims of diﬀerential privacy should be
carefully scrutinized to ascertain the level of granularity at which pri-
vacy is being promised. Diﬀerential privacy promises that the behavior
of an algorithm will be roughly unchanged even if a single entry in
the database is modiﬁed. But what constitutes a single entry in the
database? Consider for example a database that takes the form of a
graph. Such a database might encode a social network: each individual
i ∈[n] is represented by a vertex in the graph, and friendships between
individuals are represented by edges.
We could consider diﬀerential privacy at a level of granularity cor-
responding to individuals: that is, we could require that diﬀerentially

24
Basic Terms
private algorithms be insensitive to the addition or removal of any ver-
tex from the graph. This gives a strong privacy guarantee, but might in
fact be stronger than we need. the addition or removal of a single vertex
could after all add or remove up to n edges in the graph. Depending
on what it is we hope to learn from the graph, insensitivity to n edge
removals might be an impossible constraint to meet.
We could on the other hand consider diﬀerential privacy at a level
of granularity corresponding to edges, and ask our algorithms to be
insensitive only to the addition or removal of single, or small numbers
of, edges from the graph. This is of course a weaker guarantee, but
might still be suﬃcient for some purposes. Informally speaking, if we
promise ε-diﬀerential privacy at the level of a single edge, then no data
analyst should be able to conclude anything about the existence of any
subset of 1/ε edges in the graph. In some circumstances, large groups
of social contacts might not be considered sensitive information: for
example, an individual might not feel the need to hide the fact that
the majority of his contacts are with individuals in his city or workplace,
because where he lives and where he works are public information. On
the other hand, there might be a small number of social contacts whose
existence is highly sensitive (for example a prospective new employer,
or an intimate friend). In this case, edge privacy should be suﬃcient
to protect sensitive information, while still allowing a fuller analysis of
the data than vertex privacy. Edge privacy will protect such an indi-
vidual’s sensitive information provided that he has fewer than 1/ε such
friends.
As another example, a diﬀerentially private movie recommendation
system can be designed to protect the data in the training set at the
“event” level of single movies, hiding the viewing/rating of any single
movie but not, say, hiding an individual’s enthusiasm for cowboy west-
erns or gore, or at the “user” level of an individual’s entire viewing and
rating history.
All Small Epsilons Are Alike.
When ε is small, (ε, 0)-diﬀerential
privacy asserts that for all pairs of adjacent databases x, y and all
outputs o, an adversary cannot distinguish which is the true database

2.3. Formalizing diﬀerential privacy
25
on the basis of observing o. When ε is small, failing to be (ε, 0)-
diﬀerentially private is not necessarily alarming — for example, the
mechanism may be (2ε, 0)-diﬀerentially private. The nature of the pri-
vacy guarantees with diﬀering but small epsilons are quite similar.
But what of large values for ϵ? Failure to be (15, 0)-diﬀerentially pri-
vate merely says there exist neighboring databases and an output o
for which the ratio of probabilities of observing o conditioned on the
database being, respectively, x or y, is large. An output of o might be
very unlikely (this is addressed by (ε, δ)-diﬀerential privacy); databases
x and y might be terribly contrived and ulikely to occur in the “real
world”; the adversary may not have the right auxiliary information to
recognize that a revealing output has occurred; or may not know enough
about the database(s) to determine the value of their symmetric diﬀer-
ence. Thus, much as a weak cryptosystem may leak anything from only
the least signiﬁcant bit of a message to the complete decryption key,
the failure to be (ε, 0)- or (ε, δ)-diﬀerentially private may range from
eﬀectively meaningless privacy breaches to complete revelation of the
entire database. A large epsilon is large after its own fashion.
A Few Additional Formalisms. Our privacy mechanism M will
often take some auxiliary parameters w as input, in addition to the
database x. For example, w may specify a query qw on the database x,
or a collection Qw of queries. The mechanism M(w, x) might (respec-
tively) respond with a diﬀerentially private approximation to qw(x) or
to some or all of the queries in Qw. For all δ ≥0, we say that a mech-
anism M(·, ·) satisﬁes (ε, δ)-diﬀerential privacy if for every w, M(w, ·)
satisﬁes (ε, δ)-diﬀerential privacy.
Another example of a parameter that may be included in w is a
security parameter κ to govern how small δ = δ(κ) should be. That
is, M(κ, ·) should be (ε, δ(κ)) diﬀerentially private for all κ. Typically,
and throughout this monograph, we require that δ be a negligible func-
tion in κ, i.e., δ = κ−ω(1). Thus, we think of δ as being cryptograph-
ically small, whereas ε is typically thought of as a moderately small
constant.
In the case where the auxiliary parameter w speciﬁes a collec-
tion Qw = {q : X n →R} of queries, we call the mechanism M a

26
Basic Terms
synopsis generator. A synopsis generator outputs a (diﬀerentially
private) synopsis A which can be used to compute answers to all the
queries in Qw. That is, we require that there exists a reconstruction
procedure R such that for each input v specifying a query qv ∈Qw,
the reconstruction procedure outputs R(A, v) ∈R. Typically, we
will require that with high probability M produces a synopsis A
such that the reconstruction procedure, using A, computes accurate
answers. That is, for all or most (weighted by some distribution) of the
queries qv ∈Qw, the error |R(A, v) −qv(x)| will be bounded. We will
occasionally abuse notation and refer to the reconstruction procedure
taking as input the actual query q (rather than some representation v
of it), and outputting R(A, q).
A special case of a synopsis is a synthetic database. As the name
suggests, the rows of a synthetic database are of the same type as
rows of the original database. An advantage to synthetic databases is
that they may be analyzed using the same software that the analyst
would use on the original database, obviating the need for a special
reconstruction procedure R.
Remark 2.1. Considerable care must be taken when programming real-
valued mechanisms, such as the Laplace mechanism, due to subtleties
in the implementation of ﬂoating point numbers. Otherwise diﬀerential
privacy can be destroyed, as outputs with non-zero probability on a
database x, may, because of rounding, have zero probability on adja-
cent databases y. This is just one way in which the implementation of
ﬂoating point requires scrutiny in the context of diﬀerential privacy,
and it is not unique.
2.4
Bibliographic notes
The deﬁnition of diﬀerential privacy is due to Dwork et al. [23]; the
precise formulation used here and in the literature ﬁrst appears in [20]
and is due to Dwork and McSherry. The term “diﬀerential privacy”
was coined by Michael Schroeder. The impossibility of semantic secu-
rity is due to Dwork and Naor [25]. Composition and group privacy
for (ε, 0)-diﬀerentially private mechanisms is ﬁrst addressed in [23].

2.4. Bibliographic notes
27
Composition for (ε, δ)-diﬀerential privacy was ﬁrst addressed in [21]
(but see the corrected proof in Appendix B, due to Dwork and Lei [22]).
The vulnerability of diﬀerential privacy to inappropriate implementa-
tions of ﬂoating point numbers was observed by Mironov, who proposed
a mitigation [63].

3
Basic Techniques and Composition Theorems
After reviewing a few probabilistic tools, we present the Laplace mech-
anism, which gives diﬀerential privacy for real (vector) valued queries.
An application of this leads naturally to the exponential mechanism,
which is a method for diﬀerentially private selection from a discrete
set of candidate outputs. We then analyze the cumulative privacy
loss incurred by composing multiple diﬀerentially private mechanisms.
Finally we give a method — the sparse vector technique — for pri-
vately reporting the outcomes of a potentially very large number of
computations, provided that only a few are “signiﬁcant.”
In this section, we describe some of the most basic techniques in
diﬀerential privacy that we will come back to use again and again. The
techniques described here form the basic building blocks for all of the
other algorithms that we will develop.
3.1
Useful probabilistic tools
The following concentration inequalities will frequently be useful. We
state them in easy to use forms rather than in their strongest forms.
28

3.2. Randomized response
29
Theorem 3.1 (Additive ChernoﬀBound). Let X1, . . . , Xm be indepen-
dent random variables bounded such that 0 ≤Xi ≤1 for all i. Let
S =
1
m
Pm
i=1 Xi denote their mean, and let µ = E[S] denote their
expected mean. Then:
Pr[S > µ + ε] ≤e−2mε2
Pr[S < µ −ε] ≤e−2mε2
Theorem 3.2 (Multiplicative ChernoﬀBound). Let X1, . . . , Xm be inde-
pendent random variables bounded such that 0 ≤Xi ≤1 for all i. Let
S =
1
m
Pm
i=1 Xi denote their mean, and let µ = E[S] denote their
expected mean. Then:
Pr[S > (1 + ε)µ] ≤e−mµε2/3
Pr[S < (1 −ε)µ] ≤e−mµε2/2
When we do not have independent random variables, all is not lost.
We may still apply Azuma’s inequality:
Theorem 3.3 (Azuma’s Inequality). Let f be a function of m random
variables X1, . . . , Xm, each Xi taking values from a set Ai such that
E[f] is bounded. Let ci denote the maximum eﬀect of Xi on f — i.e.,
for all ai, a′
i ∈Ai:
E[f|X1, . . . , Xi−1, Xi = ai] −E[f|X1, . . . , Xi−1, Xi = a′
i]
 ≤ci
Then:
Pr [f(X1, . . . , Xm) ≥E[f] + t] ≤exp
 
−
2t2
Pm
i=1 c2
i
!
Theorem 3.4 (Stirling’s Approximation). n! can be approximated by
√
2nπ(n/e)n:
√
2nπ(n/e)ne1/(12n+1) < n! <
√
2nπ(n/e)ne1/(12n).
3.2
Randomized response
Let us recall the simple randomized response mechanism, described
in Section 2, for evaluating the frequency of embarrassing or illegal

30
Basic Techniques and Composition Theorems
behaviors. Let XYZ be such an activity. Faced with the query, “Have
you engaged in XYZ in the past week?” the respondent is instructed
to perform the following steps:
1. Flip a coin.
2. If tails, then respond truthfully.
3. If heads, then ﬂip a second coin and respond “Yes” if heads and
“No” if tails.
The intuition behind randomized response is that it provides “plau-
sible deniability.” For example, a response of “Yes” may have been
oﬀered because the ﬁrst and second coin ﬂips were both Heads, which
occurs with probability 1/4. In other words, privacy is obtained by pro-
cess, there are no “good” or “bad” responses. The process by which
the responses are obtained aﬀects how they may legitimately be inter-
preted. As the next claim shows, randomized response is diﬀerentially
private.
Claim 3.5. The version of randomized response described above is
(ln 3, 0)-diﬀerentially private.
Proof. Fix a respondent. A case analysis shows that Pr[Response =
Yes|Truth = Yes] = 3/4. Speciﬁcally, when the truth is “Yes” the
outcome will be “Yes” if the ﬁrst coin comes up tails (probabil-
ity 1/2) or the ﬁrst and second come up heads (probability 1/4)),
while Pr[Response = Yes|Truth = No] = 1/4 (ﬁrst comes up heads and
second comes up tails; probability 1/4). Applying similar reasoning to
the case of a “No” answer, we obtain:
Pr[Response = Yes|Truth = Yes]
Pr[Response = Yes|Truth = No]
= 3/4
1/4 = Pr[Response = No|Truth = No]
Pr[Response = No|Truth = Yes] = 3.
3.3
The laplace mechanism
Numeric queries, functions f : N|X| →Rk, are one of the most fun-
damental types of database queries. These queries map databases to k

3.3. The laplace mechanism
31
real numbers. One of the important parameters that will determine just
how accurately we can answer such queries is their ℓ1 sensitivity:
Deﬁnition 3.1 (ℓ1-sensitivity). The ℓ1-sensitivity of a function f :
N|X| →Rk is:
∆f =
max
x,y∈N|X|
∥x−y∥1=1
∥f(x) −f(y)∥1.
The ℓ1 sensitivity of a function f captures the magnitude by which
a single individual’s data can change the function f in the worst case,
and therefore, intuitively, the uncertainty in the response that we must
introduce in order to hide the participation of a single individual.
Indeed, we will formalize this intuition: the sensitivity of a function
gives an upper bound on how much we must perturb its output to pre-
serve privacy. One noise distribution naturally lends itself to diﬀerential
privacy.
Deﬁnition 3.2 (The Laplace Distribution). The Laplace Distribution
(centered at 0) with scale b is the distribution with probability density
function:
Lap(x|b) = 1
2b exp

−|x|
b

.
The variance of this distribution is σ2 = 2b2. We will sometimes write
Lap(b) to denote the Laplace distribution with scale b, and will some-
times abuse notation and write Lap(b) simply to denote a random vari-
able X ∼Lap(b).
The Laplace distribution is a symmetric version of the exponential
distribution.
We will now deﬁne the Laplace Mechanism. As its name suggests,
the Laplace mechanism will simply compute f, and perturb each coor-
dinate with noise drawn from the Laplace distribution. The scale of the
noise will be calibrated to the sensitivity of f (divided by ε).1
1Alternately, using Gaussian noise with variance calibrated to ∆f ln(1/δ)/ε,
one can achieve (ε, δ)-diﬀerential privacy (see Appendix A). Use of the Laplace
mechanism is cleaner and the two mechanisms behave similarly under composition
(Theorem 3.20).

32
Basic Techniques and Composition Theorems
Deﬁnition 3.3 (The Laplace Mechanism). Given any function f
:
N|X| →Rk, the Laplace mechanism is deﬁned as:
ML(x, f(·), ε) = f(x) + (Y1, . . . , Yk)
where Yi are i.i.d. random variables drawn from Lap(∆f/ε).
Theorem 3.6. The Laplace mechanism preserves (ε, 0)-diﬀerential
privacy.
Proof. Let x ∈N|X| and y ∈N|X| be such that ∥x −y∥1 ≤1, and
let f(·) be some function f : N|X| →Rk. Let px denote the probabil-
ity density function of ML(x, f, ε) , and let py denote the probability
density function of ML(y, f, ε). We compare the two at some arbitrary
point z ∈Rk
px(z)
py(z) =
k
Y
i=1

exp(−ε|f(x)i−zi|
∆f
)
exp(−ε|f(y)i−zi|
∆f
)


=
k
Y
i=1
exp
ε(|f(y)i −zi| −|f(x)i −zi|)
∆f

≤
k
Y
i=1
exp
ε|f(x)i −f(y)i|
∆f

= exp
ε · ∥f(x) −f(y)∥1
∆f

≤exp(ε),
where the ﬁrst inequality follows from the triangle inequality, and
the last follows from the deﬁnition of sensitivity and the fact that
∥x −y∥1 ≤1. That px(z)
py(z) ≥exp(−ε) follows by symmetry.
Example 3.1 (Counting Queries). Counting queries are queries of the
form “How many elements in the database satisfy Property P?” We
will return to these queries again and again, sometimes in this pure
form, sometimes in fractional form (“What fraction of the elements
in the databases...?”), sometimes with weights (linear queries), and
sometimes in slightly more complex forms (e.g., apply h : N|X| →[0, 1]
to each element in the database and sum the results). Counting is an

3.3. The laplace mechanism
33
extremely powerful primitive. It captures everything learnable in the
statistical queries learning model, as well as many standard datamining
tasks and basic statistics. Since the sensitivity of a counting query is 1
(the addition or deletion of a single individual can change a count by
at most 1), it is an immediate consequence of Theorem 3.6 that (ε, 0)-
diﬀerential privacy can be achieved for counting queries by the addition
of noise scaled to 1/ε, that is, by adding noise drawn from Lap(1/ε).
The expected distortion, or error, is 1/ε, independent of the size of the
database.
A ﬁxed but arbitrary list of m counting queries can be viewed as
a vector-valued query. Absent any further information about the set
of queries a worst-case bound on the sensitivity of this vector-valued
query is m, as a single individual might change every count. In this
case (ε, 0)-diﬀerential privacy can be achieved by adding noise scaled
to m/ε to the true answer to each query.
We sometimes refer to the problem of responding to large numbers
of (possibly arbitrary) queries as the query release problem.
Example 3.2 (Histogram Queries). In the special (but common) case in
which the queries are structurally disjoint we can do much better —
we don’t necessarily have to let the noise scale with the number of
queries. An example is the histogram query. In this type of query the
universe N|X| is partitioned into cells, and the query asks how many
database elements lie in each of the cells. Because the cells are disjoint,
the addition or removal of a single database element can aﬀect the
count in exactly one cell, and the diﬀerence to that cell is bounded
by 1, so histogram queries have sensitivity 1 and can be answered
by adding independent draws from Lap(1/ε) to the true count in
each cell.
To understand the accuracy of the Laplace mechanism for general
queries we use the following useful fact:
Fact 3.7. If Y ∼Lap(b), then:
Pr[|Y | ≥t · b] = exp(−t).

34
Basic Techniques and Composition Theorems
This fact, together with a union bound, gives us a simple bound on
the accuracy of the Laplace mechanism:
Theorem 3.8. Let f : N|X| →Rk, and let y = ML(x, f(·), ε). Then
∀δ ∈(0, 1]:
Pr

∥f(x) −y∥∞≥ln
k
δ

·
∆f
ε

≤δ
Proof. We have:
Pr

∥f(x) −y∥∞≥ln
k
δ

·
∆f
ε

= Pr
"
max
i∈[k] |Yi| ≥ln
k
δ

·
∆f
ε
#
≤k · Pr

|Yi| ≥ln
k
δ

·
∆f
ε

= k ·
δ
k

= δ
where the second to last inequality follows from the fact that each
Yi ∼Lap(∆f/ε) and Fact 3.7.
Example 3.3 (First Names). Suppose we wish to calculate which ﬁrst
names, from a list of 10,000 potential names, were the most common
among participants of the 2010 census. This question can be repre-
sented as a query f : N|X| →R10000. This is a histogram query, and so
has sensitivity ∆f = 1, since every person can only have at most one
ﬁrst name. Using the above theorem, we see that we can simultaneously
calculate the frequency of all 10, 000 names with (1, 0)-diﬀerential pri-
vacy, and with probability 95%, no estimate will be oﬀby more than
an additive error of ln(10000/.05) ≈12.2. That’s pretty low error for a
nation of more than 300, 000, 000 people!
Diﬀerentially Private Selection.
The task in Example 3.3 is one of
diﬀerentially private selection: the space of outcomes is discrete and
the task is to produce a “best” answer, in this case the most populous
histogram cell.

3.3. The laplace mechanism
35
Example 3.4 (Most Common Medical Condition). Suppose we wish to
know which condition is (approximately) the most common in the med-
ical histories of a set of respondents, so the set of questions is, for
each condition under consideration, whether the individual has ever
received a diagnosis of this condition. Since individuals can experience
many conditions, the sensitivity of this set of questions can be high.
Nonetheless, as we next describe, this task can be addressed using addi-
tion of Lap(1/ε) noise to each of the counts (note the small scale of the
noise, which is independent of the total number of conditions). Cru-
cially, the m noisy counts themselves will not be released (although the
“winning” count can be released at no extra privacy cost).
Report Noisy Max.
Consider the following simple algorithm to deter-
mine which of m counting queries has the highest value: Add indepen-
dently generated Laplace noise Lap(1/ε) to each count and return the
index of the largest noisy count (we ignore the possibility of a tie). Call
this algorithm Report Noisy Max.
Note the “information minimization” principle at work in the
Report Noisy Max algorithm: rather than releasing all the noisy counts
and allowing the analyst to ﬁnd the max and its index, only the
index corresponding to the maximum is made public. Since the data
of an individual can aﬀect all counts, the vector of counts has high ℓ1-
sensitivity, speciﬁcally, ∆f = m, and much more noise would be needed
if we wanted to release all of the counts using the Laplace mechanism.
Claim 3.9. The Report Noisy Max algorithm is (ε, 0)-diﬀerentially
private.
Proof. Fix D = D′ ∪{a}. Let c, respectively c′, denote the vector of
counts when the database is D, respectively D′. We use two properties:
1. Monotonicity of Counts. For all j ∈[m], cj ≥c′
j; and
2. Lipschitz Property. For all j ∈[m], 1 + c′
j ≥cj.
Fix any i ∈[m]. We will bound from above and below the ratio of
the probabilities that i is selected with D and with D′.
Fix r−i, a draw from [Lap(1/ε)]m−1 used for all the noisy counts
except the ith count. We will argue for each r−i independently. We

36
Basic Techniques and Composition Theorems
use the notation Pr[i|ξ] to mean the probability that the output of the
Report Noisy Max algorithm is i, conditioned on ξ.
We ﬁrst argue that Pr[i|D, r−i] ≤eε Pr[i|D′, r−i]. Deﬁne
r∗= min
ri
: ci + ri > cj + rj ∀j ̸= i.
Note that, having ﬁxed r−i, i will be the output (the argmax noisy
count) when the database is D if and only if ri ≥r∗.
We have, for all 1 ≤j ̸= i ≤m:
ci + r∗> cj + rj
⇒(1 + c′
i) + r∗≥ci + r∗> cj + rj ≥c′
j + rj
⇒c′
i + (r∗+ 1) > c′
j + rj.
Thus, if ri ≥r∗+ 1, then the ith count will be the maximum when the
database is D′ and the noise vector is (ri, r−i). The probabilities below
are over the choice of ri ∼Lap(1/ε).
Pr[ri ≥1 + r∗] ≥e−ε Pr[ri ≥r∗] = e−ε Pr[i|D, r−i]
⇒Pr[i|D′, r−i] ≥Pr[ri ≥1 + r∗] ≥e−ε Pr[ri ≥r∗] = e−ε Pr[i|D, r−i],
which, after multiplying through by eε, yields what we wanted to show:
Pr[i|D, r−i] ≤eε Pr[i|D′, r−i].
We now argue that Pr[i|D′, r−i] ≤eε Pr[i|D, r−i]. Deﬁne
r∗= min
ri
: c′
i + ri > c′
j + rj ∀j ̸= i.
Note that, having ﬁxed r−i, i will be the output (argmax noisy count)
when the database is D′ if and only if ri ≥r∗.
We have, for all 1 ≤j ̸= i ≤m:
c′
i + r∗> c′
j + rj
⇒1 + c′
i + r∗> 1 + c′
j + rj
⇒c′
i + (r∗+ 1) > (1 + c′
j) + rj
⇒ci + (r∗+ 1) ≥c′
i + (r∗+ 1) > (1 + c′
j) + rj ≥cj + rj.
Thus, if ri ≥r∗+ 1, then i will be the output (the argmax noisy
count) on database D with randomness (ri, r−i). We therefore have,
with probabilities taken over choice of ri:
Pr[i|D, r−i] ≥Pr[ri ≥r∗+ 1] ≥e−ε Pr[ri ≥r∗] = e−ε Pr[i|D′, r−i],

3.4. The exponential mechanism
37
which, after multiplying through by eε, yields what we wanted to show:
Pr[i|D′, r−i] ≤eε Pr[i|D, r−i].
3.4
The exponential mechanism
In both the “most common name” and “most common condition” exam-
ples the “utility” of a response (name or medical condition, respec-
tively) we estimated counts using Laplace noise and reported the noisy
maximum. In both examples the utility of the response is directly
related to the noise values generated; that is, the popularity of the
name or condition is appropriately measured on the same scale and in
the same units as the magnitude of the noise.
The exponential mechanism was designed for situations in which
we wish to choose the “best” response but adding noise directly to the
computed quantity can completely destroy its value, such as setting a
price in an auction, where the goal is to maximize revenue, and adding a
small amount of positive noise to the optimal price (in order to protect
the privacy of a bid) could dramatically reduce the resulting revenue.
Example 3.5 (Pumpkins.). Suppose we have an abundant supply of
pumpkins and four bidders: A, F, I, K, where A, F, I each bid $1.00
and K bids $3.01. What is the optimal price? At $3.01 the revenue
is $3.01, at $3.00 and at $1.00 the revenue is $3.00, but at $3.02 the
revenue is zero!
The exponential mechanism is the natural building block for
answering queries with arbitrary utilities (and arbitrary non-numeric
range), while preserving diﬀerential privacy. Given some arbitrary
range R, the exponential mechanism is deﬁned with respect to some
utility function u : N|X| × R →R, which maps database/output pairs
to utility scores. Intuitively, for a ﬁxed database x, the user prefers that
the mechanism outputs some element of R with the maximum possible
utility score. Note that when we talk about the sensitivity of the utility
score u : N|X| × R →R, we care only about the sensitivity of u with
respect to its database argument; it can be arbitrarily sensitive in its

38
Basic Techniques and Composition Theorems
range argument:
∆u ≡max
r∈R
max
x,y:∥x−y∥1≤1 |u(x, r) −u(y, r)|.
The intuition behind the exponential mechanism is to output each pos-
sible r ∈R with probability proportional to exp(εu(x, r)/∆u) and so
the privacy loss is approximately:
ln
exp(εu(x, r)/∆u)
exp(εu(y, r)/∆u)

= ε[u(x, r) −u(y, r)]/∆u) ≤ε.
This intuitive view overlooks some eﬀects of a normalization term which
arises when an additional person in the database causes the utilities of
some elements r ∈R to decrease and others to increase. The actual
mechanism, deﬁned next, reserves half the privacy budget for changes
in the normalization term.
Deﬁnition 3.4 (The Exponential Mechanism). The exponential mech-
anism ME(x, u, R) selects and outputs an element r ∈R with
probability proportional to exp(εu(x,r)
2∆u ).
The exponential mechanism can deﬁne a complex distribution over
a large arbitrary domain, and so it may not be possible to implement
the exponential mechanism eﬃciently when the range of u is super-
polynomially large in the natural parameters of the problem.
Returning to the pumpkin example, utility for a price p on database
x is simply the proﬁt obtained when the price is p and the demand curve
is as described by x. It is important that the range of potential prices
is independent of the actual bids. Otherwise there would exist a price
with non-zero weight in one dataset and zero weight in a neighboring
set, violating diﬀerential privacy.
Theorem
3.10. The
exponential
mechanism
preserves
(ε, 0)-
diﬀerential privacy.
Proof. For clarity, we assume the range R of the exponential mecha-
nism is ﬁnite, but this is not necessary. As in all diﬀerential privacy
proofs, we consider the ratio of the probability that an instantiation

3.4. The exponential mechanism
39
of the exponential mechanism outputs some element r ∈R on two
neighboring databases x ∈N|X| and y ∈N|X| (i.e., ∥x −y∥1 ≤1).
Pr[ME(x, u, R) = r]
Pr[ME(y, u, R) = r] =
 
exp( εu(x,r)
2∆u )
P
r′∈R exp( εu(x,r′)
2∆u
)
!
 
exp( εu(y,r)
2∆u )
P
r′∈R exp( εu(y,r′)
2∆u
)
!
=
 
exp(εu(x,r)
2∆u )
exp(εu(y,r)
2∆u )
!
·


P
r′∈R exp(εu(y,r′)
2∆u )
P
r′∈R exp(εu(x,r′)
2∆u )


= exp
ε(u(x, r′) −u(y, r′))
2∆u

·


P
r′∈R exp(εu(y,r′)
2∆u )
P
r′∈R exp(εu(x,r′)
2∆u )


≤exp
ε
2

· exp
ε
2

·


P
r′∈R exp(εu(x,r′)
2∆u )
P
r′∈R exp(εu(x,r′)
2∆u )


= exp(ε).
Similarly, Pr[ME(y,u)=r]
Pr[ME(x,u)=r] ≥exp(−ε) by symmetry.
The exponential mechanism can often give strong utility guarantees,
because it discounts outcomes exponentially quickly as their quality
score falls oﬀ. For a given database x and a given utility measure u :
N|X| × R →R, let OPTu(x) = maxr∈R u(x, r) denote the maximum
utility score of any element r ∈R with respect to database x. We will
bound the probability that the exponential mechanism returns a “good”
element of R, where good will be measured in terms of OPTu(x). The
result is that it will be highly unlikely that the returned element r has
a utility score that is inferior to OPTu(x) by more than an additive
factor of O((∆u/ε) log |R|).
Theorem 3.11. Fixing a database x, let ROPT = {r ∈R : u(x, r) =
OPTu(x)} denote the set of elements in R which attain utility score

40
Basic Techniques and Composition Theorems
OPTu(x). Then:
Pr

u(ME(x, u, R)) ≤OPTu(x) −2∆u
ε

ln

|R|
|ROPT|

+ t

≤e−t
Proof.
Pr[u(ME(x, u, R)) ≤c] ≤
|R| exp(εc/2∆u)
|ROPT| exp(εOPTu(x)/2∆u)
=
|R|
|ROPT| exp
ε(c −OPTu(x))
2∆u

.
The inequality follows from the observation that each r
∈
R
with u(x, r)
≤
c has un-normalized probability mass at most
exp(εc/2∆u), and hence the entire set of such “bad” elements r has
total un-normalized probability mass at most |R| exp(εc/2∆u). In
contrast, we know that there exist at least |ROPT| ≥1 elements
with u(x, r) = OPTu(x), and hence un-normalized probability mass
exp(εOPTu(x)/2∆u), and so this is a lower bound on the normalization
term.
The theorem follows from plugging in the appropriate value
for c.
Since we always have |ROPT| ≥1, we can more commonly make
use of the following simple corollary:
Corollary 3.12. Fixing a database x, we have:
Pr

u(ME(x, u, R)) ≤OPTu(x) −2∆u
ε
(ln (|R|) + t)

≤e−t
As seen in the proofs of Theorem 3.11 and Corollary 3.12, the Expo-
nential Mechanism can be particularly easy to analyze.
Example 3.6 (Best of Two). Consider the simple question of determin-
ing which of exactly two medical conditions A and B is more common.
Let the two true counts be 0 for condition A and c > 0 for condition B.
Our notion of utility will be tied to the actual counts, so that conditions
with bigger counts have higher utility and ∆u = 1. Thus, the utility
of A is 0 and the utility of B is c. Using the Exponential Mechanism

3.5. Composition theorems
41
we can immediately apply Corollary 3.12 to see that the probability of
observing (wrong) outcome A is at most 2e−c(ε/(2∆u)) = 2e−cε/2.
Analyzing Report Noisy Max appears to be more complicated, as
it requires understanding what happens in the (probability 1/4) case
when the noise added to the count for A is positive and the noise added
to the count for B is negative.
A function is monotonic in the data set if the addition of an element
to the data set cannot cause the value of the function to decrease.
Counting queries are monotonic; so is the revenue obtained by oﬀering
a ﬁxed price to a collection of buyers.
Consider the Report One-Sided Noisy Arg-Max mechanism, which
adds noise to the utility of each potential output drawn from the one-
sided exponential distribution with parameter ε/∆u in the case of a
monotonic utility, or parameter ε/2∆u for the case of a non-monotonic
utility, and reports the resulting arg-max.
With this algorithm, whose privacy proof is almost identical to that
of Report Noisy Max (but loses a factor of two when the utility is
non-monotonic), we immediately obtain in Example 3.6 above that
outcome A is exponentially in c(ε/∆u) = cε less likely to be selected
than outcome B.
Theorem 3.13. Report One-Sided Noisy Arg-Max, when run with
parameter ε/2∆u is ϵ-diﬀerentially private.
Remark 3.1. Report noisy max when instantiated with Laplace noise
or exponential noise both have similar guarantees to the exponential
mechanism, but lead to distinct distributions. It turns out that instan-
tiating report noisy max with the Gumbel distribution leads to an
algorithm that samples exactly from the exponential mechanism dis-
tribution. This fact is folklore in machine learning, and known as the
“Gumbel Max Trick”.
3.5
Composition theorems
Now that we have several building blocks for designing diﬀerentially
private algorithms, it is important to understand how we can combine

42
Basic Techniques and Composition Theorems
them to design more sophisticated algorithms. In order to use these
tools, we would like that the combination of two diﬀerentially private
algorithms be diﬀerentially private itself. Indeed, as we will see, this is
the case. Of course the parameters ε and δ will necessarily degrade —
consider repeatedly computing the same statistic using the Laplace
mechanism, scaled to give ε-diﬀerential privacy each time. The average
of the answer given by each instance of the mechanism will eventually
converge to the true value of the statistic, and so we cannot avoid that
the strength of our privacy guarantee will degrade with repeated use.
In this section we give theorems showing how exactly the parameters
ε and δ compose when diﬀerentially private subroutines are combined.
Let us ﬁrst begin with an easy warm up: we will see that the
independent use of an (ε1, 0)-diﬀerentially private algorithm and an
(ε2, 0)-diﬀerentially private algorithm, when taken together, is (ε1 +
ε2, 0)-diﬀerentially private.
Theorem 3.14. Let M1 : N|X| →R1 be an ε1-diﬀerentially private
algorithm, and let M2 : N|X| →R2 be an ε2-diﬀerentially private
algorithm. Then their combination, deﬁned to be M1,2 : N|X| →R1 ×
R2 by the mapping: M1,2(x) = (M1(x), M2(x)) is ε1+ε2-diﬀerentially
private.
Proof. Let x, y ∈N|X| be such that ∥x −y∥1 ≤1. Fix any (r1, r2) ∈
R1 × R2. Then:
Pr[M1,2(x) = (r1, r2)]
Pr[M1,2(y) = (r1, r2)] = Pr[M1(x) = r1] Pr[M2(x) = r2]
Pr[M1(y) = r1] Pr[M2(y) = r2]
=
Pr[M1(x) = r1]
Pr[M1(y) = r1]
 Pr[M2(x) = r1]
Pr[M2(y) = r1]

≤exp(ε1) exp(ε2)
= exp(ε1 + ε2)
By symmetry, Pr[M1,2(x)=(r1,r2)]
Pr[M1,2(y)=(r1,r2)] ≥exp(−(ε1 + ε2)).
The composition theorem can be applied repeatedly to obtain the
following corollary:

3.5. Composition theorems
43
Corollary 3.15. Let Mi : N|X| →Ri be an (εi, 0)-diﬀerentially private
algorithm for i ∈[k]. Then if M[k] : N|X| →Qk
i=1 Ri is deﬁned to be
M[k](x) = (M1(x), . . . , Mk(x)), then M[k] is (Pk
i=1 εi, 0)-diﬀerentially
private.
A proof of the generalization of this theorem to (ε, δ)-diﬀerential
privacy appears in Appendix B:
Theorem 3.16. Let Mi : N|X| →Ri be an (εi, δi)-diﬀerentially private
algorithm for i ∈[k]. Then if M[k] : N|X| →Qk
i=1 Ri is deﬁned to
be M[k](x) = (M1(x), . . . , Mk(x)), then M[k] is (Pk
i=1 εi, Pk
i=1 δi)-
diﬀerentially private.
It is a strength of diﬀerential privacy that composition is
“automatic,” in that the bounds obtained hold without any special
eﬀort by the database curator.
3.5.1
Composition: some technicalities
In the remainder of this section, we will prove a more sophisticated
composition theorem. To this end, we will need some deﬁnitions and
lemmas, rephrasing diﬀerential privacy in terms of distance measures
between distributions. In the fractional quantities below, if the denom-
inator is zero, then we deﬁne the value of the fraction to be inﬁnite
(the numerators will always be positive).
Deﬁnition
3.5
(KL-Divergence). The KL-Divergence, or Relative
Entropy, between two random variables Y and Z taking values from
the same domain is deﬁned to be:
D(Y ∥Z) = Ey∼Y

ln Pr[Y = y]
Pr[Z = y]

.
It is known that D(Y ∥Z) ≥0, with equality if and only if Y and
Z are identically distributed. However, D is not symmetric, does not
satisfy the triangle inequality, and can even be inﬁnite, speciﬁcally when
Supp(Y ) is not contained in Supp(Z).
Deﬁnition 3.6 (Max Divergence). The Max Divergence between two
random variables Y and Z taking values from the same domain is

44
Basic Techniques and Composition Theorems
deﬁned to be:
D∞(Y ∥Z) =
max
S⊆Supp(Y )

ln Pr[Y ∈S]
Pr[Z ∈S]

.
The δ-Approximate Max Divergence between Y and Z is deﬁned to be:
Dδ
∞(Y ∥Z) =
max
S⊆Supp(Y ):Pr[Y ∈S]≥δ

ln Pr[Y ∈S] −δ
Pr[Z ∈S]

Remark 3.2. Note that a mechanism M is
1. ε-diﬀerentially private if and only if on every two neigh-
boring
databases
x
and
y,
D∞(M(x)∥M(y))
≤
ε
and
D∞(M(y)∥M(x)) ≤ε; and is
2. (ε, δ)-diﬀerentially private if and only if on every two neigh-
boring databases x, y: Dδ
∞(M(x)∥M(y)) ≤ε and Dδ
∞(M(y)∥
M(x)) ≤ε.
One other distance measure that will be useful is the statistical
distance between two random variables Y and Z, deﬁned as
∆(Y, Z) def
= max
S
| Pr[Y ∈S] −Pr[Z ∈S]|.
We say that Y and Z are δ-close if ∆(Y, Z) ≤δ.
We will use the following reformulations of approximate max-
divergence in terms of exact max-divergence and statistical distance:
Lemma 3.17.
1. Dδ
∞(Y ∥Z) ≤ε if and only if there exists a random variable Y ′
such that ∆(Y, Y ′) ≤δ and D∞(Y ′∥Z) ≤ε.
2. We have both Dδ
∞(Y ∥Z) ≤ε and Dδ
∞(Z∥Y ) ≤ε if and only if
there exist random variables Y ′, Z′ such that ∆(Y, Y ′) ≤δ/(eε +
1), ∆(Z, Z′) ≤δ/(eε + 1), and D∞(Y ′∥Z′) ≤ε.
Proof. For Part 1, suppose there exists Y ′ δ-close to Y such that
D∞(Y ∥Z) ≤ε. Then for every S,
Pr[Y ∈S] ≤Pr[Y ′ ∈S] + δ ≤eε · Pr[Z ∈S] + δ,
and thus Dδ
∞(Y ∥Z) ≤ε.

3.5. Composition theorems
45
Conversely, suppose that Dδ
∞(Y ∥Z) ≤ε. Let S = {y : Pr[Y = y] >
eε · Pr[Z = y]}. Then
X
y∈S
(Pr[Y = y] −eε · Pr[Z = y]) = Pr[Y ∈S] −eε · Pr[Z ∈S] ≤δ.
Moreover, if we let T = {y : Pr[Y = y] < Pr[Z = y]}, then we have
X
y∈T
(Pr[Z = y] −Pr[Y = y]) =
X
y /∈T
(Pr[Y = y] −Pr[Z = y])
≥
X
y∈S
(Pr[Y = y] −Pr[Z = y])
≥
X
y∈S
(Pr[Y = y] −eε · Pr[Z = y])/
Thus, we can obtain Y ′ from Y by lowering the probabilities on S and
raising the probabilities on T to satisfy:
1. For all y ∈S, Pr[Y ′ = y] = eε · Pr[Z = y] < Pr[Y = y].
2. For all y ∈T, Pr[Y = y] ≤Pr[Y ′ = y] ≤Pr[Z = y].
3. For all y /∈S ∪T, Pr[Y ′ = y] = Pr[Y = y] ≤eε · Pr[Z = y].
Then D∞(Y ′∥Z) ≤ε by inspection, and
∆(Y, Y ′) = Pr[Y ∈S] −Pr[Y ′ ∈S] = Pr[Y ∈S] −eε · Pr[Z ∈S] ≤δ.
We now prove Part 2. Suppose there exist random variables Y ′
and Z′ as stated. Then, for every set S,
Pr[Y ∈S] ≤Pr[Y ′ ∈S] +
δ
eε + 1
≤eε · Pr[Z′ ∈S] +
δ
eε + 1
≤eε ·

Pr[Z ∈S] +
δ
eε + 1

+
δ
eε + 1
= eε · Pr[Z ∈S] + δ.
Thus Dδ
∞(Y ∥Z) ≤ε, and by symmetry, Dδ
∞(Z∥Y ) ≤ε.
Conversely, given Y
and Z such that Dδ
∞(Y ∥Z)
≤
ε and
Dδ
∞(Z∥Y ) ≤ε, we proceed similarly to Part 1. However, instead of
simply decreasing the probability mass of Y on S to obtain Y ′ and

46
Basic Techniques and Composition Theorems
eliminate the gap with eε · Z, we also increase the probability mass of
Z on S. Speciﬁcally, for every y ∈S, we’ll take
Pr[Y ′ = y] = eε · Pr[Z′ = y]
=
eε
1 + eε · (Pr[Y = y] + Pr[Z = y])
∈[eε · Pr[Z = y], Pr[Y = y]].
This also implies that for y ∈S, we have:
Pr[Y = y] −Pr[Y ′ = y]
= Pr[Z′ = y] −Pr[Z = y]Pr[Y = y] −eε · Pr[Z = y]
eε + 1
,
and thus
α def
=
X
y∈S
 Pr[Y = y] −Pr[Y ′ = y]

=
X
y∈S
 Pr[Z′ = y] −Pr[Z = y]

= Pr[Y ∈S] −eε · Pr[Z ∈S]
eε + 1
≤
δ
eε + 1.
Similarly on the set S′ = {y : Pr[Z = y] > eε · Pr[Y = y]}, we can
decrease the probability mass of Z and increase the probability mass
of Y by a total of some α′ ≤δ/(eε + 1) so that for every y ∈S′, we
have Pr[Z′ = y] = eε · Pr[Y ′ = y].
If α = α′, then we can take Pr[Z′ = y] = Pr[Z = y] and
Pr[Y ′ = y] = Pr[Y = y] for all y /∈S ∪S′, giving D∞(Y ∥Z) ≤ε
and ∆(Y, Y ′) = ∆(Z, Z′) = α. If α ̸= α′, say α > α′, then we need to
still increase the probability mass of Y ′ and decrease the mass of Z′
by a total of β = α −α′ on points outside of S ∪S′ in order to ensure
that the probabilities sum to 1. That is, if we try to take the “mass
functions” Pr[Y ′ = y] and Pr[Z′ = y] as deﬁned above, then while we
do have the property that for every y, Pr[Y ′ = y] ≤eε · Pr[Z′ = y]
and Pr[Z′ = y] ≤eε · Pr[Y ′ = y] we also have P
y Pr[Y ′ = y] = 1 −β

3.5. Composition theorems
47
and P
y Pr[Z′ = y] = 1 + β. However, this means that if we let
R = {y : Pr[Y ′ = y] < Pr[Z′ = y]}, then
X
y∈R
 Pr[Z′ = y] −Pr[Y ′ = y]
 ≥
X
y
 Pr[Z′ = y] −Pr[Y ′ = y]
 = 2β.
So we can increase the probability mass of Y ′ on points in R by a total of
β and decrease the probability mass of Z′ on points in R by a total of β,
while retaining the property that for all y ∈R, Pr[Y ′ = y] ≤Pr[Z′ = y].
The resulting Y ′ and Z′ have the properties we want: D∞(Y ′, Z′) ≤ε
and ∆(Y, Y ′), ∆(Z, Z′) ≤α.
Lemma
3.18. Suppose that random variables Y
and Z
satisfy
D∞(Y ∥Z) ≤ε and D∞(Z∥Y ) ≤ε. Then D(Y ∥Z) ≤ε · (eε −1).
Proof. We know that for any Y and Z it is the case that D(Y ∥Z) ≥0
(via the “log-sum inequality”), and so it suﬃces to bound D(Y ∥Z) +
D(Z∥Y ). We get:
D(Y ∥Z) ≤D(Y ∥Z) + D(Z∥Y )
=
X
y
Pr[Y = y] ·

ln Pr[Y = y]
Pr[Z = y] + ln Pr[Z = y]
Pr[Y = y]

+ (Pr[Z = y] −Pr[Y = y]) ·

ln Pr[Z = y]
Pr[Y = y]

≤
X
y
[0 + |Pr[Z = y] −Pr[Y = y]| · ε]
= ε ·
X
y
[max{Pr[Y = y], Pr[Z = y]}
−min{Pr[Y = y], Pr[Z = y]}]
≤ε ·
X
y
[(eε −1) · min{Pr[Y = y], Pr[Z = y]}]
≤ε · (eε −1).
Lemma 3.19 (Azuma’s Inequality). Let C1, . . . , Ck be real-valued ran-
dom variables such that for every i ∈[k], Pr[|Ci| ≤α] = 1, and for

48
Basic Techniques and Composition Theorems
every (c1, . . . , ci−1) ∈Supp(C1, . . . , Ci−1), we have
E[Ci|C1 = c1, . . . , Ci−1 = ci−1] ≤β.
Then for every z > 0, we have
Pr
" k
X
i=1
Ci > kβ + z
√
k · α
#
≤e−z2/2.
3.5.2
Advanced composition
In addition to allowing the parameters to degrade more slowly, we
would like our theorem to be able to handle more complicated forms of
composition. However, before we begin, we must discuss what exactly
we mean by composition. We would like our deﬁnitions to cover the
following two interesting scenarios:
1. Repeated use of diﬀerentially private algorithms on the same
database. This allows both the repeated use of the same
mechanism multiple times, as well as the modular construction of
diﬀerentially private algorithms from arbitrary private building
blocks.
2. Repeated use of diﬀerentially private algorithms on diﬀerent
databases that may nevertheless contain information relating to
the same individual. This allows us to reason about the cumu-
lative privacy loss of a single individual whose data might be
spread across multiple data sets, each of which may be used inde-
pendently in a diﬀerentially private way. Since new databases are
created all the time, and the adversary may actually inﬂuence the
makeup of these new databases, this is a fundamentally diﬀerent
problem than repeatedly querying a single, ﬁxed, database.
We want to model composition where the adversary can adaptively
aﬀect the databases being input to future mechanisms, as well as the
queries to those mechanisms. Let F be a family of database access
mechanisms. (For example F could be the set of all ε-diﬀerentially
private mechanisms.) For a probabilistic adversary A, we consider two
experiments, Experiment 0 and Experiment 1, deﬁned as follows.

3.5. Composition theorems
49
Experiment b for family F and adversary A:
For i = 1, . . . , k:
1. A outputs two adjacent databases x0
i and x1
i , a mechanism
Mi ∈F, and parameters wi.
2. A receives yi ∈R Mi(wi, xi,b).
We allow the adversary A above to be stateful throughout the exper-
iment, and thus it may choose the databases, mechanisms, and the
parameters adaptively depending on the outputs of previous mecha-
nisms. We deﬁne A’s view of the experiment to be A’s coin tosses and
all of the mechanism outputs (y1, . . . , yk). (The xj
i’s, Mi’s, and wi’s
can all be reconstructed from these.)
For intuition, consider an adversary who always chooses x0
i to hold
Bob’s data and x1
i to diﬀer only in that Bob’s data are deleted. Then
experiment 0 can be thought of as the “real world,” where Bob allows
his data to be used in many data releases, and Experiment 1 as an
“ideal world,” where the outcomes of these data releases do not depend
on Bob’s data. Our deﬁnitions of privacy still require these two exper-
iments to be “close” to each other, in the same way as required by
the deﬁnitions of diﬀerential privacy. The intuitive guarantee to Bob is
that the adversary “can’t tell”, given the output of all k mechanisms,
whether Bob’s data was ever used.
Deﬁnition 3.7. We say that the family F of database access mecha-
nisms satisﬁes ε-diﬀerential privacy under k-fold adaptive composition
if for every adversary A, we have D∞(V 0∥V 1) ≤ε where V b denotes
the view of A in k-fold Composition Experiment b above.
(ε, δ)-diﬀerential privacy under k-fold adaptive composition instead
requires that Dδ
∞(V 0∥V 1) ≤ε.
Theorem 3.20 (Advanced Composition). For all ε, δ, δ′ ≥0, the class of
(ε, δ)-diﬀerentially private mechanisms satisﬁes (ε′, kδ + δ′)-diﬀerential
privacy under k-fold adaptive composition for:
ε′ =
q
2k ln(1/δ′)ε + kε(eε −1).

50
Basic Techniques and Composition Theorems
Proof. A view of the adversary A consists of a tuple of the form v =
(r, y1, . . . , yk), where r is the coin tosses of A and y1, . . . , yk are the
outputs of the mechanisms M1, . . . , Mk. Let
B = {v : Pr[V 0 = v] > eε′ · Pr[V 1 = v]}.
We will show that Pr[V 0 ∈B] ≤δ, and hence for every set S, we have
Pr[V 0 ∈S] ≤Pr[V 0 ∈B] + Pr[V 0 ∈(S \ B)] ≤δ + eε′ · Pr[V 1 ∈S].
This is equivalent to saying that Dδ
∞(V 0∥V 1) ≤ε′.
It remains to show Pr[V 0 ∈B] ≤δ. Let random variable V 0 =
(R0, Y 0
1 , . . . , Y 0
k ) denote the view of A in Experiment 0 and V 1 =
(R1, Y 1
1 , . . . , Y 1
k ) the view of A in Experiment 1. Then for a ﬁxed view
v = (r, y1, . . . , yk), we have
ln
 
Pr[V 0 = v]
Pr[V 1 = v]
!
= ln
 
Pr[R0 = r]
Pr[R1 = r] ·
k
Y
i=1
Pr[Y 0
i = yi|R0 = r, Y 0
1 = y1, . . . , Y 0
i−1 = yi−1]
Pr[Y 1
i = yi|R1 = r, Y 1
1 = y1, . . . , Y 1
i−1 = yi−1]
!
=
k
X
i=1
ln
 
Pr[Y 0
i = yi|R0 = r, Y 0
1 = y1, . . . , Y 0
i−1 = yi−1]
Pr[Y 1
i = yi|R1 = r, Y 1
1 = y1, . . . , Y 1
i−1 = yi−1]
!
def
=
k
X
i=1
ci(r, y1, . . . , yi).
Now for every preﬁx (r, y1, . . . , yi−1) we condition on R0 = r, Y 0
1 =
y1, . . . , Y 0
i−1
=
yi−1, and analyze the expectation and maximum
possible value of the random variable ci(R0, Y 0
1 , . . . , Y 0
i ) = ci(r, y1, . . .,
yi−1, Y 0
i ). Once the preﬁx is ﬁxed, the next pair of databases x0
i and
x1
i , the mechanism Mi, and parameter wi output by A are also deter-
mined (in both Experiment 0 and 1). Thus Y 0
i is distributed according
to Mi(wi, x0
i ). Moreover for any value yi, we have
ci(r, y1, . . . , yi−1, yi) = ln
 
Pr[Mi(wi, x0
i ) = yi]
Pr[Mi(wi, x1
i ) = yi]
!
.

3.5. Composition theorems
51
By ε-diﬀerential privacy this is bounded by ε. We can also reason as
follows:
|ci(r, y1, . . . , yi−1, yi)|
≤max{D∞(Mi(wi, x0
i )∥Mi(wi, x1
i )),
D∞(Mi(wi, x1
i )∥Mi(wi, x0
i ))}
= ε.
By Lemma 3.18, we have:
E[ci(R0, Y 0
1 , . . . , Y 0
i )|R0 = r, Y 0
1 = y1, . . . , Y 0
i−1 = yi−1]
= D(Mi(wi, x0
i )∥Mi(wi, x1
i ))
≤ε(eε −1).
Thus we can apply Azuma’s Inequality to the random variables Ci =
ci(R0, Y 0
1 , . . . , Y 0
i ) with α = ε, β = ε·ε0, and z =
p
2 ln(1/δ), to deduce
that
Pr[V 0 ∈B] = Pr
"X
i
Ci > ε′
#
< e−z2/2 = δ,
as desired.
To extend the proof to composition of (ε, δ)-diﬀerentially private
mechanisms, for δ > 0, we use the characterization of approximate max-
divergence from Lemma 3.17 (Part 2) to reduce the analysis to the same
situation as in the case of (ε, 0)-indistinguishable sequences. Speciﬁ-
cally, using Lemma 3.17, Part 2 for each of the diﬀerentially private
mechanisms selected by the adversary A and the triangle inequality
for statistical distance, it follows that that V 0 is kδ-close to a random
variable W = (R, Z1, . . . , Zk) such that for every preﬁx r, y1, . . . , yi−1,
if we condition on R = R1 = r, Z1 = Y 1
1 = y1, . . . , Zi−1 = Y 1
i−1 = yi−1,
then it holds that D∞(Zi∥Y 1
i ) ≤ε and D∞(Y 1
i ∥Zi) ≤ε.
This suﬃces to show that Dδ′
∞(W∥V 1) ≤ε′. Since V 0 is kδ-close to
W, Lemma 3.17, Part 1 gives Dδ′+kδ(V 0∥W) ≤ε′.
An immediate and useful corollary tells us a safe choice of ε for each
of k mechanisms if we wish to ensure (ε′, kδ + δ′)-diﬀerential privacy
for a given ε′, δ′.

52
Basic Techniques and Composition Theorems
Corollary 3.21. Given target privacy parameters 0 < ε′ < 1 and δ′ > 0,
to ensure (ε′, kδ + δ′) cumulative privacy loss over k mechanisms, it
suﬃces that each mechanism is (ε, δ)-diﬀerentially private, where
ε =
ε′
2
p
2k ln(1/δ′).
Proof. Theorem 3.20 tells us the composition will be (ε∗, kδ + δ′) for
all δ′, where ε∗=
p
2k ln(1/δ′) · ε + kε2. When ε′ < 1, we have that
ε∗≤ε′ as desired.
Note that the above corollary gives a rough guide for how to set ε
to get desired privacy parameters under composition. When one cares
about optimizing constants (which one does when dealing with actual
implementations), ε can be set more tightly by appealing directly to
the composition theorem.
Example 3.7. Suppose, over the course of his lifetime, Bob is a mem-
ber of k = 10, 000 (ε0, 0)-diﬀerentially private databases. Assuming
no coordination among these databases — the administrator of any
given database may not even be aware of the existence of the other
databases — what should be the value of ε0 so that, over the course
of his lifetime, Bob’s cumulative privacy loss is bounded by ε = 1 with
probability at least 1 −e−32? Theorem 3.20 says that, taking δ′ = e−32
it suﬃces to have ε0 ≤1/801. This turns out to be essentially opti-
mal against an arbitrary adversary, assuming no coordination among
distinct diﬀerentially private databases.
So how many queries can we answer with non-trivial accuracy? On
a database of size n let us say the accuracy is non-trivial if the error
is of order o(n). Theorem 3.20 says that for ﬁxed values of ε and δ,
it is possible to answer close to n2 counting queries with non-trivial
accuracy. Similarly, one can answer close to n queries while still having
noise o(√n) — that is, noise less than the sampling error. We will see
that it is possible to dramatically improve on these results, handling,
in some cases, even an exponential number of queries with noise only
slightly larger than √n, by coordinating the noise added to the individ-
ual responses. It turns out that such coordination is essential: without

3.5. Composition theorems
53
coordination the bound in the advanced composition theorem is almost
tight.
3.5.3
Laplace versus Gauss
An alternative to adding Laplacian noise is to add Gaussian noise. In
this case, rather than scaling the noise to the ℓ1 sensitivity ∆f, we
instead scale to the ℓ2 sensitivity:
Deﬁnition 3.8 (ℓ2-sensitivity). The ℓ2-sensitivity of a function f :
N|X| →Rk is:
∆2(f) =
max
x,y∈N|X|
∥x−y∥1=1
∥f(x) −f(y)∥2.
The Gaussian Mechanism with parameter b adds zero-mean Gaus-
sian noise with variance b in each of the k coordinates. The following
theorem is proved in Appendix A.
Theorem 3.22. Let ε ∈(0, 1) be arbitrary. For c2 > 2 ln(1.25/δ),
the Gaussian Mechanism with parameter σ ≥c∆2(f)/ε is (ε, δ)-
diﬀerentially private.
Among the advantages to Gaussian noise is that the noise added for
privacy is of the same type as other sources of noise; moreover, the sum
of two Gaussians is a Gaussian, so the eﬀects of the privacy mechanism
on the statistical analysis may be easier to understand and correct for.
The two mechanisms yield the same cumulative loss under
composition, so even though the privacy guarantee is weaker for each
individual computation, the cumulative eﬀects over many computations
are comparable. Also, if δ is suﬃciently (e.g., subpolynomially) small,
in practice we will never experience the weakness of the guarantee.
That said, there is a theoretical disadvantage to Gaussian noise,
relative to what we experience with Laplace noise. Consider Report
Noisy Max (with Laplace noise) in a case in which every candidate
output has the same quality score on database x as on its neighbor y.
Independent of the number of candidate outputs, the mechanism yields
(ε, 0)-diﬀerential privacy. If instead we use Gaussian noise and report
the max, and if the number of candidates is large compared to 1/δ,

54
Basic Techniques and Composition Theorems
then we will exactly select for the events with large Gaussian noise —
noise that occurs with probability less than δ. When we are this far
out on the tail of the Gaussian we no longer have a guarantee that the
observation is within an e±ε factor as likely to occur on x as on y.
3.5.4
Remarks on composition
The ability to analyze cumulative privacy loss under composition gives
us a handle on what a world of diﬀerentially private databases can oﬀer.
A few observations are in order.
Weak Quantiﬁcation.
Assume that the adversary always chooses x0
i
to hold Bob’s data, and x1
i to be the same database but with Bob’s data
deleted. Theorem 3.20, with appropriate choise of parameters, tells us
that an adversary — including one that knows or even selects(!) the
database pairs — has little advantage in determining the value of b ∈
{0, 1}. This is an inherently weak quantiﬁcation. We can ensure that the
adversary is unlikely to distinguish reality from any given alternative,
but we cannot ensure this simultaneously for all alternatives. If there
are one zillion databases but Bob is a member of only 10,000 of these,
then we are not simultaneously protecting Bob’s absence from all zillion
minus ten thousand. This is analogous to the quantiﬁcation in the
deﬁnition of (ε, δ)-diﬀerential privacy, where we ﬁx in advance a pair
of adjacent databases and argue that with high probability the output
will be almost equally likely with these two databases.
Humans
and
Ghosts.
Intuitively, an (ϵ, 0)-diﬀerentially private
database with a small number of bits per record is less protective than
a diﬀerentially private database with the same choice of ϵ that con-
tains our entire medical histories. So in what sense is our principle
privacy measure, ϵ, telling us the same thing about databases that dif-
fer radically in the complexity and sensitivity of the data they store?
The answer lies in the composition theorems. Imagine a world inhab-
ited by two types of beings: ghosts and humans. Both types of beings
behave the same, interact with others in the same way, write, study,
work, laugh, love, cry, reproduce, become ill, recover, and age in the
same fashion. The only diﬀerence is that ghosts have no records in

3.6. The sparse vector technique
55
databases, while humans do. The goal of the privacy adversary is to
determine whether a given 50-year old, the “target,” is a ghost or a
human. Indeed, the adversary is given all 50 years to do so. The adver-
sary does not need to remain passive, for example, she can organize
clinical trials and enroll patients of her choice, she can create humans
to populate databases, eﬀectively creating the worst-case (for privacy)
databases, she can expose the target to chemicals at age 25 and again
at 35, and so on. She can know everything about the target that could
possibly be entered into any database. She can know which databases
the target would be in, were the target human. The composition theo-
rems tell us that the privacy guarantees of each database — regardless
of the data type, complexity, and sensitivity — give comparable pro-
tection for the human/ghost bit.
3.6
The sparse vector technique
The Laplace mechanism can be used to answer adaptively chosen low
sensitivity queries, and we know from our composition theorems that
the privacy parameter degrades proportionally to the number of queries
answered (or its square root). Unfortunately, it will often happen that
we have a very large number of questions to answer — too many to yield
a reasonable privacy guarantee using independent perturbation tech-
niques, even with the advanced composition theorems of Section 3.5.
In some situations however, we will only care to know the identity of
the queries that lie above a certain threshold. In this case, we can hope
to gain over the naïve analysis by discarding the numeric answer to
queries that lie signiﬁcantly below the threshold, and merely report-
ing that they do indeed lie below the threshold. (We will be able to
get the numeric values of the above-threshold queries as well, at lit-
tle additional cost, if we so choose). This is similar to what we did in
the Report Noisy Max mechanism in section 3.3, and indeed iterating
either that algorithm or the exponential mechanism would be an option
for the non-interactive, or oﬄine, case.
In this section, we show how to analyze a method for this in the
online setting. The technique is simple — add noise and report only

56
Basic Techniques and Composition Theorems
whether the noisy value exceeds the threshold — and our emphasis is
on the analysis, showing that privacy degrades only with the number
of queries which actually lie above the threshold, rather than with the
total number of queries. This can be a huge savings if we know that
the set of queries that lie above the threshold is much smaller than the
total number of queries — that is, if the answer vector is sparse.
In a little more detail, we will consider a sequence of events — one
for each query — which occur if a query evaluated on the database
exceeds a given (known, public) threshold. Our goal will be to release a
bit vector indicating, for each event, whether or not it has occurred. As
each query is presented, the mechanism will compute a noisy response,
compare it to the (publicly known) threshold, and, if the threshold is
exceeded, reveal this fact. For technical reasons in the proof of privacy
(Theorem 3.24), the algorithm works with a noisy version ˆT of the
threshold T. While T is public the noisy version ˆT is not.
Rather than incurring a privacy loss for each possible query, the
analysis below will result in a privacy cost only for the query values
that are near or above the threshold.
The Setting.
Let m denote the total number of sensitivity 1 queries,
which may be chosen adaptively. Without loss of generality, there is
a single threshold T ﬁxed in advance (alternatively, each query can
have its own threshold, but the results are unchanged). We will be
adding noise to query values and comparing the results to T. A positive
outcome means that a noisy query value exceeds the threshold. We
expect a small number c of noisy values to exceed the threshold, and we
are releasing only the noisy values above the threshold. The algorithm
will use c in its stopping condition.
We will ﬁrst analyze the case in which the algorithm halts after c =
1 above-threshold query, and show that this algorithm is ϵ-diﬀerentially
private no matter how long the total sequence of queries is. We will then
analyze the case of c > 1 by using our composition theorems, and derive
bounds both for (ϵ, 0) and (ϵ, δ)-diﬀerential privacy.
We ﬁrst argue that AboveThreshold, the algorithm specialized to
the case of only one above-threshold query, is private and accurate.

3.6. The sparse vector technique
57
Algorithm 1 Input is a private database D, an adaptively chosen
stream of sensitivity 1 queries f1, . . ., and a threshold T. Output is a
stream of responses a1, . . .
AboveThreshold(D, {fi}, T, ϵ)
Let ˆT = T + Lap

2
ϵ

.
for Each query i do
Let νi = Lap(4
ϵ)
if fi(D) + νi ≥ˆT then
Output ai = ⊤.
Halt.
else
Output ai = ⊥.
end if
end for
Theorem 3.23. AboveThreshold is (ϵ, 0)-diﬀerentially private.
Proof. Fix any two neighboring databases D and D′. Let A denote
the random variable representing the output of AboveThresh-
old(D, {fi}, T, ϵ) and let A′ denote the random variable representing
the output of AboveThreshold(D′, {fi}, T, ϵ). The output of the algo-
rithm is some realization of these random variables, a ∈{⊤, ⊥}k and
has the form that for all i < k, ai = ⊥and ak = ⊤. There are two
types of random variables internal to the algorithm: the noisy thresh-
old ˆT and the perturbations to each of the k queries, {νi}k
i=1. For the
following analysis, we will ﬁx the (arbitrary) values of ν1, . . . , νk−1 and
take probabilities over the randomness of νk and ˆT. Deﬁne the fol-
lowing quantity representing the maximum noisy value of any query
f1, . . . , fk−1 evaluated on D:
g(D) = max
i<k (fi(D) + νi)
In the following, we will abuse notation and write Pr[ ˆT = t] as short-
hand for the pdf of ˆT evaluated at t (similarly for νk), and write 1[x]
to denote the indicator function of event x. Note that ﬁxing the values

58
Basic Techniques and Composition Theorems
of ν1, . . . , νk−1 (which makes g(D) a deterministic quantity), we have:
Pr
ˆT,νk
[A = a] = Pr
ˆT,νk
[ ˆT > g(D) and fk(D) + νk ≥ˆT]
= Pr
ˆT,νk
[ ˆT ∈(g(D), fk(D) + νk]]
=
Z ∞
−∞
Z ∞
−∞
Pr[νk = v]
· Pr[ ˆT = t]1[t ∈(g(D), fk(D) + v]]dvdt
.= ∗
We now make a change of variables. Deﬁne:
ˆv = v + g(D) −g(D′) + fk(D′) −fk(D)
ˆt = t + g(D) −g(D′)
and note that for any D, D′, |ˆv −v| ≤2 and |ˆt −t| ≤1. This follows
because each query fi(D) is 1-sensitive, and hence the quantity g(D)
is 1-sensitive as well. Applying this change of variables, we have:
∗=
Z ∞
−∞
Z ∞
−∞
Pr[νk = ˆv] · Pr[ ˆT = ˆt]1[(t + g(D) −g(D′))
∈(g(D), fk(D′) + v + g(D) −g(D′)]]dvdt
=
Z ∞
−∞
Z ∞
−∞
Pr[νk = ˆv] · Pr[ ˆT = ˆt]1[(t ∈(g(D′), fk(D′) + v]]dvdt
≤
Z ∞
−∞
Z ∞
−∞
exp(ϵ/2) Pr[νk = v]
· exp(ϵ/2) Pr[ ˆT = t]1[(t ∈(g(D′), fk(D′) + v]]dvdt
= exp(ϵ) Pr
ˆT,νk
[ ˆT > g(D′) and fk(D′) + νk ≥ˆT]
= exp(ϵ) Pr
ˆT,νk
[A′ = a]
where the inequality comes from our bounds on |ˆv −v| and |ˆt −t| and
the form of the pdf of the Laplace distribution.
Deﬁnition 3.9 (Accuracy). We will say that an algorithm which outputs
a stream of answers a1, . . . , ∈{⊤, ⊥}∗in response to a stream of k

3.6. The sparse vector technique
59
queries f1, . . . , fk is (α, β)-accurate with respect to a threshold T if
except with probability at most β, the algorithm does not halt before
fk, and for all ai = ⊤:
fi(D) ≥T −α
and for all ai = ⊥:
fi(D) ≤T + α.
What can go wrong in Algorithm 1? The noisy threshold ˆT can be
very far from T, say, | ˆT −T| > α. In addition a small count fi(D) <
T −α can have so much noise added to it that it is reported as above
threshold (even when the threshold is close to correct), and a large
count fi(D) > T + α can be reported as below threshold. All of these
happen with probability exponentially small in α. In summary, we can
have a problem with the choice of the noisy threshold or we can have a
problem with one or more of the individual noise values νi. Of course,
we could have both kinds of errors, so in the analysis below we allocate
α/2 to each type.
Theorem 3.24. For any sequence of k queries f1, . . . , fk such that
|{i < k : fi(D) ≥T −α}| = 0 (i.e. the only query close to being
above threshold is possibly the last one), AboveThreshold(D, {fi}, T, ϵ)
is (α, β) accurate for:
α = 8(log k + log(2/β))
ϵ
.
Proof. Observe that the theorem will be proved if we can show that
except with probability at most β:
max
i∈[k] |νi| + |T −ˆT| ≤α
If this is the case, then for any ai = ⊤, we have:
fi(D) + νi ≥ˆT ≥T −|T −ˆT|
or in other words:
fi(D) ≥T −|T −ˆT| −|νi| ≥T −α

60
Basic Techniques and Composition Theorems
Similarly, for any ai = ⊥we have:
fi(D) < ˆT ≤T + |T −ˆT| + |νi| ≤T + α
We will also have that for any i < k: fi(D) < T −α < T −|νi|−|T −ˆT|,
and so: fi(D) + νi ≤ˆT, meaning ai = ⊥. Therefore the algorithm does
not halt before k queries are answered.
We now complete the proof.
Recall that if Y ∼Lap(b), then: Pr[|Y | ≥t·b] = exp(−t). Therefore
we have:
Pr[|T −ˆT| ≥α
2 ] = exp

−ϵα
4

Setting this quantity to be at most β/2, we ﬁnd that we require α ≥
4 log(2/β)
ϵ
Similarly, by a union bound, we have:
Pr[max
i∈[k] |νi| ≥α/2] ≤k · exp

−ϵα
8

Setting this quantity to be at most β/2, we ﬁnd that we require α ≥
8(log(2/β)+log k)
ϵ
These two claims combine to prove the theorem.
We now show how to handle multiple “above threshold” queries
using composition.
The Sparse algorithm can be thought of as follows: As queries come
in, it makes repeated calls to AboveThreshold. Each time an above
threshold query is reported, the algorithm simply restarts the remain-
ing stream of queries on a new instantiation of AboveThreshold. It
halts after it has restarted AboveThreshold c times (i.e. after c above
threshold queries have appeared). Each instantiation of AboveThresh-
old is (ϵ, 0)-private, and so the composition theorems apply.
Theorem 3.25. Sparse is (ϵ, δ)-diﬀerentially private.
Proof. We observe that Sparse is exactly equivalent to the following
procedure: We run AboveThreshold(D, {fi}, T, ϵ′) on our stream of
queries {fi} setting
ϵ′ =



ϵ
c,
If δ = 0;
ϵ
p
8c ln 1
δ
,
Otherwise.

3.6. The sparse vector technique
61
Algorithm 2 Input is a private database D, an adaptively chosen
stream of sensitivity 1 queries f1, . . ., a threshold T, and a cutoﬀpoint
c. Output is a stream of answers a1, . . .
Sparse(D, {fi}, T, c, ϵ, δ)
If δ = 0 Let σ = 2c
ϵ . Else Let σ =
p
32c ln 1
δ
ϵ
Let ˆT0 = T + Lap(σ)
Let count = 0
for Each query i do
Let νi = Lap(2σ)
if fi(D) + νi ≥ˆTcount then
Output ai = ⊤.
Let count = count +1.
Let ˆTcount = T + Lap(σ)
else
Output ai = ⊥.
end if
if count ≥c then
Halt.
end if
end for
using the answers supplied by AboveThreshold. When AboveThresh-
old
halts
(after
1
above
threshold
query),
we
simply
restart
Sparse(D, {fi}, T, ϵ′) on the remaining stream, and continue in this
manner until we have restarted AboveThreshold c times. After the
c’th restart of AboveThreshold halts, we halt as well. We have already
proven that AboveThreshold(D, {fi}, T, ϵ′) is (ϵ′, 0) diﬀerentially pri-
vate. Finally, by the advanced composition theorem (Theorem 3.20), c
applications of an ϵ′ =
ϵ
p
8c ln 1
δ
-diﬀerentially private algorithm is (ϵ, δ)-
diﬀerentially private, and c applications of an ϵ′ = ϵ/c diﬀerentially
private algorithm is (ϵ, 0)-private as desired.
It remains to prove accuracy for Sparse, by again observing that
Sparse consists only of c calls to AboveThreshold. We note that if each

62
Basic Techniques and Composition Theorems
of these calls to AboveThreshold is (α, β/c)-accurate, then Sparse will
be (α, β)-accurate.
Theorem 3.26. For any sequence of k queries f1, . . . , fk such that
L(T) ≡|{i : fi(D) ≥T −α}| ≤c, if δ > 0, Sparse is (α, β) accu-
rate for:
α =
(ln k + ln 2c
β )
q
512c ln 1
δ
ϵ
.
If δ = 0, Sparse is (α, β) accurate for:
α = 8c(ln k + ln(2c/β))
ϵ
Proof. We simply apply Theorem 3.24 setting β to be β/c, and ϵ to be
ϵ
p
8c ln 1
δ
and ϵ/c, depending on whether δ > 0 or δ = 0, respectively.
Finally, we give a version of Sparse that actually outputs the
numeric values of the above threshold queries, which we can do with
only a constant factor loss in accuracy. We call this algorithm Numer-
icSparse, and it is simply a composition of Sparse with the Laplace
mechanism. Rather than outputting a vector a ∈{⊤, ⊥}∗, it outputs a
vector a ∈(R ∪{⊥})∗.
We observe that NumericSparse is private:
Theorem 3.27. NumericSparse is (ϵ, δ)-diﬀerentially private.
Proof. Observe that if δ = 0, NumericSparse(D, {fi}, T, c, ϵ, 0) is sim-
ply the adaptive composition of Sparse(D, {fi}, T, c, 8
9ϵ, 0), together
with the Laplace mechanism with privacy parameters (ϵ′, δ) = (1
9ϵ, 0).
If δ > 0, then NumericSparse(D, {fi}, T, c, ϵ, 0) is the composition
of Sparse(D, {fi}, T, c,
√
512
√
512+1ϵ, δ/2) together with the Laplace mecha-
nism with privacy parameters (ϵ′, δ) = (
1
√
512+1ϵ, δ/2). Hence the pri-
vacy of NumericSparse follows from simple composition.
To discuss accuracy, we must deﬁne what we mean by the accuracy
of a mechanism that outputs a stream a ∈(R ∪{⊥})∗in response to a
sequence of numeric valued queries:

3.6. The sparse vector technique
63
Algorithm 3 Input is a private database D, an adaptively chosen
stream of sensitivity 1 queries f1, . . ., a threshold T, and a cutoﬀpoint
c. Output is a stream of answers a1, . . .
NumericSparse(D, {fi}, T, c, ϵ, δ)
If δ = 0 Let ϵ1 ←8
9ϵ, ϵ2 ←2
9ϵ. Else Let ϵ1 =
√
512
√
512+1ϵ, ϵ2 =
2
√
512+1
If δ = 0 Let σ(ϵ) = 2c
ϵ . Else Let σ(ϵ) =
p
32c ln 2
δ
ϵ
Let ˆT0 = T + Lap(σ(ϵ1))
Let count = 0
for Each query i do
Let νi = Lap(2σ(ϵ1))
if fi(D) + νi ≥ˆTcount then
Let υi ←Lap(σ(ϵ2))
Output ai = fi(D) + υi.
Let count = count +1.
Let ˆTcount = T + Lap(σ(ϵ1))
else
Output ai = ⊥.
end if
if count ≥c then
Halt.
end if
end for
Deﬁnition 3.10 (Numeric Accuracy). We will say that an algorithm
which outputs a stream of answers a1, . . . , ∈(R ∪{⊥})∗in response
to a stream of k queries f1, . . . , fk is (α, β)-accurate with respect to a
threshold T if except with probability at most β, the algorithm does
not halt before fk, and for all ai ∈R:
|fi(D) −ai| ≤α
and for all ai = ⊥:
fi(D) ≤T + α.
Theorem 3.28. For any sequence of k queries f1, . . . , fk such that
L(T) ≡|{i : fi(D) ≥T −α}| ≤c, if δ > 0, NumericSparse is (α, β)

64
Basic Techniques and Composition Theorems
accurate for:
α =
(ln k + ln 4c
β )
q
c ln 2
δ(
√
512 + 1)
ϵ
.
If δ = 0, Sparse is (α, β) accurate for:
α = 9c(ln k + ln(4c/β))
ϵ
Proof. Accuracy requires two conditions: ﬁrst, that for all ai = ⊥:
fi(D) ≤T + α. This holds with probability 1 −β/2 by the accuracy
theorem for Sparse. Next, for all ai ∈R, it requires |fi(D) −ai| ≤α.
This holds for with probability 1 −β/2 by the accuracy of the Laplace
mechanism.
What did we show in the end? If we are given a sequence of queries
together with a guarantee that only at most c of them have answers
above T −α, we can answer those queries that are above a given thresh-
old T, up to error α. This accuracy is equal, up to constants and a factor
of log k, to the accuracy we would get, given the same privacy guar-
antee, if we knew the identities of these large above-threshold queries
ahead of time, and answered them with the Laplace mechanism. That
is, the sparse vector technique allowed us to ﬁsh out the identities of
these large queries almost “for free”, paying only logarithmically for the
irrelevant queries. This is the same guarantee that we could have got-
ten by trying to ﬁnd the large queries with the exponential mechanism
and then answering them with the Laplace mechanism. This algorithm,
however, is trivial to run, and crucially, allows us to choose our queries
adaptively.
3.7
Bibliographic notes
Randomized Response is due to Warner [84] (predating diﬀerential
privacy by four decades!). The Laplace mechanism is due to Dwork
et al. [23]. The exponential mechanism was invented by McSherry and
Talwar [60]. Theorem 3.16 (simple composition) was claimed in [21];
the proof appearing in Appendix B is due to Dwork and Lei [22];

3.7. Bibliographic notes
65
McSherry and Mironov obtained a similar proof. The material in Sec-
tions 3.5.1 and 3.5.2 is taken almost verbatim from Dwork et al. [32].
Prior to [32] composition was modeled informally, much as we did for
the simple composition bounds. For speciﬁc mechanisms applied on a
single database, there are “evolution of conﬁdence” arguments due to
Dinur, Dwork, and Nissim [18, 31], (which pre-date the deﬁnition of
diﬀerential privacy) showing that the privacy parameter in k-fold com-
position need only deteriorate like
√
k if we are willing to tolerate a
(negligible) loss in δ (for k < 1/ε2). Theorem 3.20 generalizes those
arguments to arbitrary diﬀerentially private mechanisms,
The claim that without coordination in the noise the bounds in
the composition theorems are almost tight is due to Dwork, Naor, and
Vadhan [29]. The sparse vector technique is an abstraction of a tech-
nique that was introduced, by Dwork, Naor, Reingold, Rothblum, and
Vadhan [28] (indicator vectors in the proof of Lemma 4.4). It has sub-
sequently found wide use (e.g. by Roth and Roughgarden [74], Dwork,
Naor, Pitassi, and Rothblum [26], and Hardt and Rothblum [44]). In
our presentation of the technique, the proof of Theorem 3.23 is due to
Salil Vadhan.

4
Releasing Linear Queries with Correlated Error
One of the most fundamental primitives in private data analysis is
the ability to answer numeric valued queries on a dataset. In the last
section, we began to see tools that would allow us to do this by adding
independently drawn noise to the query answers. In this section, we
continue this study, and see that by instead adding carefully correlated
noise, we can gain the ability to privately answer vastly more queries
to high accuracy. Here, we see two speciﬁc mechanisms for solving this
problem, which we will generalize in the next section.
In this section, we consider algorithms for solving the query release
problem with better accuracy than we would get by simply using com-
positions of the Laplace mechanism. The improvements are possible
because the set of queries is handled as a whole — even in the online
setting! — permitting the noise on individual queries to be correlated.
To immediately see that something along these lines might be possi-
ble, consider the pair of queries in the diﬀerencing attack described
in Section 1: “How many people in the database have the sickle cell
trait?” and “How many people, not named X, in the database have the
sickle cell trait?” Suppose a mechanism answers the ﬁrst question using
the Laplace mechanism and then, when the second question is posed,
66

67
responds “You already know the approximate answer, because you just
asked me almost the exact same question.” This coordinated response
to the pair of questions incurs no more privacy loss than either ques-
tion would do taken in isolation, so a (small) privacy savings has been
achieved.
The query release problem is quite natural: given a class of queries
Q over the database, we wish to release some answer ai for each query
fi ∈Q such that the error maxi |ai −fi(x)| is as low as possible, while
still preserving diﬀerential privacy.1 Recall that for any family of low
sensitivity queries, we can apply the Laplace mechanism, which adds
fresh, independent, noise to the answer to each query. Unfortunately,
at a ﬁxed privacy level, for (ϵ, 0)-privacy guarantees, the magnitude of
the noise that we must add with the Laplace mechanism scales with |Q|
because this is the rate at which the sensitivity of the combined queries
may grow. Similarly, for (ϵ, δ)-privacy guarantees, the noise scales with
p
|Q| ln(1/δ). For example, suppose that our class of queries Q consists
only of many copies of the same query: fi = f∗for all i. If we use
the Laplace mechanism to release the answers, it will add independent
noise, and so each ai will be an independent random variable with mean
f∗(x). Clearly, in this regime, the noise rate must grow with |Q| since
otherwise the average of the ai will converge to the true value f∗(x),
which would be a privacy violation. However, in this case, because
fi = f∗for all i, it would make more sense to approximate f∗only once
with a∗≈f∗(x) and release ai = a∗for all i. In this case, the noise rate
would not have to scale with |Q| at all. In this section, we aim to design
algorithms that are much more accurate than the Laplace mechanism
(with error that scales with log |Q|) by adding non-independent noise
as a function of the set of queries.
Recall that our universe is X
=
{χ1, χ2, . . . , χ|X|} and that
databases are represented by histograms in N|X|. A linear query is sim-
ply a counting query, but generalized to take values in the interval [0, 1]
rather than only boolean values. Speciﬁcally, a linear query f takes the
1It is the privacy constraint that makes the problem interesting. Without this
constraint, the query release problem is trivially and optimally solved by just out-
putting exact answers for every query.

68
Releasing Linear Queries with Correlated Error
form f : X →[0, 1], and applied to a database x returns either the
sum or average value of the query on the database (we will think of
both, depending on which is more convenient for the analysis). When
we think of linear queries as returning average values, we will refer to
them as normalized linear queries, and say that they take value:
f(x) =
1
∥x∥1
|X|
X
i=1
xi · f(χi).
When we think of linear queries as returning sum values we will refer
to them as un-normalized linear queries, and say that they take value:
f(x) =
|X|
X
i=1
xi · f(χi).
Whenever we state a bound, it should be clear from context whether we
are speaking of normalized or un-normalized queries, because they take
values in very diﬀerent ranges. Note that normalized linear queries take
values in [0, 1], whereas un-normalized queries take values in [0, ∥x∥1].
Note that with this deﬁnition linear queries have sensitivity ∆f ≤1.
Later sections will discuss arbitrary low-sensitivity queries.
We will present two techniques, one each for the oﬄine and online
cases. Surprisingly, and wonderfully, the oﬄine technique is an imme-
diate application of the exponential mechanism using well-known sam-
pling bounds from learning theory! The algorithm will simply be to
apply the exponential mechanism with range equal to the set of all
small databases y and quality function u(x, y) equal to minus the max-
imum approximation error incurred by querying y to obtain an approx-
imation for f(x):
u(x, y) = −max
f∈Q |f(x) −f(y)|.
(4.1)
Sampling bounds (see Lemma 4.3 below) tell us that a random subset of
ln |Q|/α2 elements of x will very likely give us a good approximation for
all f(x) (speciﬁcally, with additive error bounded by α), so we know it
is suﬃcient to restrict the set of possible outputs to small databases. We
don’t actually care that the potential output databases are small, only
that they are not too numerous: their number plays a role in the proof of

69
utility, which is an immediate application of the utility theorem for the
exponential mechanism (Theorem 3.11). More speciﬁcally, if the total
number of potential outputs is not too numerous then, in particular, the
total number of low-utility outputs is not too numerous, and therefore
the ratio of bad outputs to good outputs (there is at least one) is not
too large.
The online mechanism, which, despite not knowing the entire set of
queries in advance, will achieve the same accuracy as the oﬄine mecha-
nism, and will be a direct application of the sparse vector technique. As
a result, privacy will be immediate, but utility will require a proof. The
key will be to argue that, even for a very large set of counting queries,
few queries are “signiﬁcant”; that is, signiﬁcant queries will be sparse.
As with the sparse vector algorithms, we can scale noise according to
the number of signiﬁcant queries, with little dependence on the total
number of queries.
Before we go on and present the mechanisms, we will give just one
example of a useful class of linear queries.
Example 4.1. Suppose that elements of the database are represented
by d boolean features. For example, the ﬁrst feature may represent
whether or not the individual is male or female, the second feature may
represent whether or not they are a college graduate, the third feature
may represent whether or not they are US citizens, etc. That is, our
data universe is X = {0, 1}d. Given a subset of these attributes S ⊆
{1, . . . , d}, we might like to know how many people in the dataset have
these attributes. (e.g., “What fraction of the dataset consists of male
college graduates with a family history of lung cancer?”). This naturally
deﬁnes a query called a monotone conjunction query, parameterized by
a subset of attributes S and deﬁned as fS(z) = Q
i∈S zi, for z ∈X.
The class of all such queries is simply Q = {fS : S ⊆{1, . . . , d}}, and
has size |Q| = 2d. A collection of answers to conjunctions is sometimes
called a contingency or marginal table, and is a common method of
releasing statistical information about a dataset. Often times, we may
not be interested in the answers to all conjunctions, but rather just
those that ask about subsets of features S of size |S| = k for some ﬁxed
k. This class of queries Qk = {fS : S ⊆{1, . . . , d}, |S| = k} has size
 d
k
.

70
Releasing Linear Queries with Correlated Error
This large and useful class of queries is just one example of the sorts
of queries that can be accurately answered by the algorithms given
in this section. (Note that if we wish to also allow (non-monotone)
conjunctions which ask about negated attributes, we can do that as
well — simply double the feature space from d to 2d, and set zd+i =
1 −zi for all i ∈{1, . . . , d}.)
4.1
An oﬄine algorithm: SmallDB
In this section, we give an algorithm based on the idea of sampling a
small database using the exponential mechanism. What we will show
is that, for counting queries, it suﬃces to consider databases that are
small: their size will only be a function of the query class, and our
desired approximation accuracy α, and crucially not on ∥x∥1, the size
of the private database. This is important because it will allow us to
simultaneously guarantee, for all suﬃciently large databases, that there
is at least one database in the range of the exponential mechanism that
well approximates x on queries in Q, and that there are not too many
databases in the range to dissipate the probability mass placed on this
“good” database.
Algorithm 4 The Small Database Mechanism
SmallDB(x, Q, ε, α)
Let R ←{y ∈N|X| : ∥y∥1 = log |Q|
α2
}
Let u : N|X| × R →R be deﬁned to be:
u(x, y) = −max
f∈Q |f(x) −f(y)|
Sample And Output y ∈R with the exponential mechanism
ME(x, u, R)
We ﬁrst observe that the Small Database mechanism preserves
ε-diﬀerential privacy.
Proposition 4.1. The Small Database mechanism is (ε, 0) diﬀerentially
private.

4.1. An oﬄine algorithm: SmallDB
71
Proof. The Small Database mechanism is simply an instantiation of the
exponential mechanism. Therefore, privacy follows from Theorem 3.10.
We may similarly call on our analysis of the exponential mechanism
to understand the utility guarantees of the Small Database mechanism.
But ﬁrst, we must justify our choice of range R = {y ∈N|X| : ∥y∥1 =
log |Q|
α2
}, the set of all databases of size log |Q|/α2.
Theorem 4.2. For any ﬁnite class of linear queries Q, if R = {y ∈
N|X| : ∥y∥1 = log |Q|
α2
} then for all x ∈N|X|, there exists a y ∈R such
that:
max
f∈Q |f(x) −f(y)| ≤α
In other words, we will show that for any collection of linear queries
Q and for any database x, there is a “small” database y of size ∥y∥1 =
log |Q|
α2
that approximately encodes the answers to every query in Q, up
to error α.
Lemma 4.3 (Sampling Bounds). For any x ∈N|X| and for any collection
of linear queries Q, there exists a database y of size
∥y∥1 = log |Q|
α2
such that:
max
f∈Q |f(x) −f(y)| ≤α
Proof. Let m = log |Q|
α2
. We will construct a database y by taking m
uniformly random samples from the elements of x. Speciﬁcally, for i ∈
{1, . . . , m}, let Xi be a random variable taking value χj ∈X with
probability xj/∥x∥1, and let y be the database containing elements
X1, . . . , Xm. Now ﬁx any f ∈Q and consider the quantity f(y). We
have:
f(y) =
1
∥y∥1
|X|
X
i=1
yi · f(χi) = 1
m
m
X
i=1
f(Xi).

72
Releasing Linear Queries with Correlated Error
We note that each term f(Xi) of the sum is a bounded random variable
taking values 0 ≤f(Xi) ≤1 with expectation
E[f(Xi)] =
|X|
X
j=1
xj
∥x∥1
f(χj) = f(x) ,
and that the expectation of f(y) is:
E[f(y)] = 1
m
m
X
i=1
E[f(Xi)] = f(x) .
Therefore, we can apply the Chernoﬀbound stated in Theorem 3.1
which gives:
Pr [|f(y) −f(x)| > α] ≤2e−2mα2 .
Taking a union bound over all of the linear queries f ∈Q, we get:
Pr

max
f∈Q |f(y) −f(x)| > α

≤2|Q|e−2mα2 .
Plugging in m = log |Q|
α2
makes the right hand side smaller than 1 (so
long as |Q| > 2), proving that there exists a database of size m satis-
fying the stated bound, which completes the proof of the lemma.
The proof of Theorem 4.2 simply follows from the observation that
R contains all databases of size log |Q|
α2
.
Proposition 4.4. Let Q be any class of linear queries. Let y be the
database output by SmallDB(x, Q, ε, α). Then with probability 1 −β:
max
f∈Q |f(x) −f(y)| ≤α +
2
 log |X| log |Q|
α2
+ log

1
β

ε∥x∥1
.
Proof. Applying the utility bounds for the exponential mechanism
(Theorem 3.11) with ∆u =
1
∥x∥1 and OPTq(D) ≤α (which follows
from Theorem 4.2), we ﬁnd:
Pr

max
f∈Q |f(x) −f(y)| ≥α +
2
ε∥x∥1
(log (|R|) + t)

≤e−t .
We complete the proof by (1) noting that R, which is the set of all
databases of size at most log |Q|/α2, satisﬁes |R| ≤|X|log |Q|/α2 and
(2) by setting t = log

1
β

.

4.1. An oﬄine algorithm: SmallDB
73
Finally, we may now state the utility theorem for SmallDB.
Theorem 4.5. By the appropriate choice of α, letting y be the database
output by SmallDB(x, Q, ε, α
2 ), we can ensure that with probability
1 −β:
max
f∈Q |f(x) −f(y)| ≤

16 log |X| log |Q| + 4 log

1
β

ε∥x∥1


1/3
.
(4.2)
Equivalently, for any database x with
∥x∥1 ≥
16 log |X| log |Q| + 4 log

1
β

εα3
(4.3)
with probability 1 −β: maxf∈Q |f(x) −f(y)| ≤α.
Proof. By Theorem 4.2, we get:
max
f∈Q |f(x) −f(y)| ≤α
2 +
2
 4 log |X| log |Q|
α2
+ log

1
β

ε∥x∥1
.
Setting this quantity to be at most α and solving for ∥x∥1 yields (4.3).
Solving for α yields (4.4).
Note that this theorem states that for ﬁxed α and ε, even with
δ = 0, it is possible to answer almost exponentially many queries in the
size of the database.2 This is in contrast to the Laplace mechanism,
when we use it directly to answer linear queries, which can only answer
linearly many.
Note also that in this discussion, it has been most convenient to
think about normalized queries. However, we can get the corresponding
bounds for unnormalized queries simply by multiplying by ∥x∥1:
Theorem 4.6 (Accuracy theorem for un-normalized queries). By the
appropriate choice of α, letting y be the database output by
2Speciﬁcally, solving for k we ﬁnd that the mechanism can answer k queries for:
k ≤exp

O

α3ϵ∥x∥1
log |X|

.

74
Releasing Linear Queries with Correlated Error
SmallDB(x, Q, ε, α
2 ), we can ensure that with probability 1 −β:
max
f∈Q |f(x) −f(y)| ≤∥x∥2/3
1

16 log |X| log |Q| + 4 log

1
β

ε


1/3
. (4.4)
More Reﬁned Bounds.
We proved that every set of linear queries
Q has a collection of databases of size at most |X|log |Q|/α2 that well-
approximates every database x with respect to Q with error at most α.
This is often an over-estimate however, since it completely ignores the
structure of the queries. For example, if Q simply contains the same
query repeated over and over again, each time in a diﬀerent guise,
then there is no reason that the size of the range of the exponen-
tial mechanism should grow with |Q|. Similarly, there may even be
classes of queries Q that have inﬁnite cardinality, but nevertheless are
well approximated by small databases. For example, queries that corre-
spond to asking whether a point lies within a given interval on the real
line form an inﬁnitely large class Q, since there are uncountably many
intervals on the real line. Nevertheless, this class of queries exhibits
very simple structure that causes it to be well approximated by small
databases. By considering more reﬁned structure of our query classes,
we will be able to give bounds for diﬀerentially private mechanisms
which improve over the simple sampling bounds (Lemma 4.3) and can
be non-trivial even for doubly exponentially large classes of queries.3
We will not fully develop these bounds here, but will instead state
several results for the simpler class of counting queries. Recall that a
counting query f : X →{0, 1} maps database points to boolean values,
rather than any value in the interval [0, 1] as linear queries do.
Deﬁnition 4.1 (Shattering). A class of counting queries Q shatters a
collection of points S ⊆X if for every T ⊆S, there exists an f ∈Q such
that {x ∈S : f(x) = 1} = T. That is, Q shatters S if for every one of
the 2|S| subsets T of S, there is some function in Q that labels exactly
3In fact, our complexity measure for a class of queries can be ﬁnite even for
inﬁnite classes of queries, but here we are dealing with queries over a ﬁnite universe,
so there do not exist inﬁnitely many distinct queries.

4.1. An oﬄine algorithm: SmallDB
75
those elements as positive, and does not label any of the elements in
S \ T as positive.
Note that for Q to shatter S it must be the case that |Q| ≥2|S|
since Q must contain a function f for each subset T ⊆S. We can now
deﬁne our complexity measure for counting queries.
Deﬁnition 4.2 (Vapnik–Chervonenkis (VC) Dimension). A collection of
counting queries Q has VC-dimension d if there exists some set S ⊆X
of cardinality |S| = d such that Q shatters S, and Q does not shatter
any set of cardinality d+1. We can denote this quantity by VC-DIM(Q).
Consider again the class of 1-dimensional intervals on the range
[0, ∞] deﬁned over the domain X = R. The function fa,b corresponding
to the interval [a, b] is deﬁned such that fa,b(x) = 1 if and only if
x ∈[a, b]. This is an inﬁnite class of queries, but its VC-dimension
is 2. For any pair of distinct points x < y, there is an interval that
contains neither point (a, b < x), an interval that contains both points
(a < x < y < b), and an interval that contains each of the points but
not the other (a < x < b < y and x < a < y < b). However, for
any 3 distinct points x < y < z, there is no interval [a, b] such that
fa,b[x] = fa,b[z] = 1 but fa,b[y] = 0.
We observe that the VC-dimension of a ﬁnite concept class can
never be too large.
Lemma 4.7. For any ﬁnite class Q, VC-DIM(Q) ≤log |Q|.
Proof. If VC-DIM(Q) = d then Q shatters some set of items S ⊆X of
cardinality |S| = d. But by the deﬁnition of shattering, since S has 2d
distinct subsets, Q must have at least 2d distinct functions in it.
It will turn out that we can essentially replace the term log |Q|
with the term VC-DIM(Q) in our bounds for the SmallDB mechanism.
By the previous lemma, this is can only be an improvement for ﬁnite
classes Q.
Theorem 4.8. For any ﬁnite class of linear queries Q, if R = {y ∈
N|X| : ∥y∥∈O
 VC-DIM(Q)
α2

} then for all x ∈N|X|, there exists a y ∈R
such that:
max
f∈Q |f(x) −f(y)| ≤α

76
Releasing Linear Queries with Correlated Error
As a result of this theorem, we get the analogue of Theorem 4.5
with VC-dimension as our measure of query class complexity:
Theorem 4.9. Let y be the database output by SmallDB(x, Q, ε, α
2 ).
Then with probability 1 −β:
max
f∈Q |f(x) −f(y)| ≤O




log |X|VC-DIM(Q) + log

1
β

ε∥x∥1


1/3


Equivalently, for any database x with
∥x∥1 ≥O

log |X|VC-DIM(Q) + log

1
β

εα3


with probability 1 −β: maxf∈Q |f(x) −f(y)| ≤α.
An analogous (although more cumbersome) measure of query com-
plexity, the “Fat Shattering Dimension,” deﬁnes the complexity of a
class of linear queries, as opposed to simply counting queries. The Fat
Shattering Dimension controls the size of the smallest “α-net” (Deﬁni-
tion 5.2 in Section 5) for a class of linear queries Q as VC-dimension
does for counting queries. This measure can similarly be used to give
more reﬁned bounds for mechanisms designed to privately release linear
queries.
4.2
An online mechanism: private multiplicative weights
We will now give a mechanism for answering queries that arrive online
and may be interactively chosen. The algorithm will be a simple com-
bination of the sparse vector algorithm (which can answer threshold
queries adaptively), and the exponentiated gradient descent algorithm
for learning linear predictors online.
This latter algorithm is also known as Hedge or more generally the
multiplicative weights technique. The idea is the following: When we
view the database D ∈N|X| as a histogram and are interested only
in linear queries (i.e., linear functions of this histogram), then we can
view the problem of answering linear queries as the problem of learn-
ing the linear function D that deﬁnes the query answers ⟨D, q⟩, given

4.2. An online mechanism: private multiplicative weights
77
a query q ∈[0, 1]|X|. If the learning algorithm only needs to access
the data using privacy-preserving queries, then rather than having a
privacy cost that grows with the number of queries we would like to
answer, we can have a privacy cost that grows only with the number
of queries the learning algorithm needs to make. The “multiplicative
weights” algorithm which we present next is a classical example of such
a learning algorithm: it can learn any linear predictor by making only
a small number of queries. It maintains at all times a current “hypoth-
esis predictor,” and accesses the data only by requiring examples of
queries on which its hypothesis predictor diﬀers from the (true) private
database by a large amount. Its guarantee is that it will always learn
the target linear function up to small error, given only a small number
of such examples. How can we ﬁnd these examples? The sparse vector
algorithm that we saw in the previous section allows us to do this on
the ﬂy, while paying for only those examples that have high error on
the current multiplicative weights hypothesis. As queries come in, we
ask whether the true answer to the query diﬀers substantially from the
answer to the query on the current multiplicative weights hypothesis.
Note that this is a threshold query of the type handled by the sparse
vector technique. If the answer is “no” — i.e., the diﬀerence, or error, is
“below threshold,” — then we can respond to the query using the pub-
licly known hypothesis predictor, and have no further privacy loss. If the
answer is “yes,” meaning that the currently known hypothesis predictor
gives rise to an error that is above threshold, then we have found an
example appropriate to update our learning algorithm. Because “above
threshold” answers correspond exactly to queries needed to update our
learning algorithm, the total privacy cost depends only on the learning
rate of the algorithm, and not on the total number of queries that we
answer.
First we give the multiplicative weights update rule and prove a the-
orem about its convergence in the language of answering linear queries.
It will be convenient to think of databases x as being probability dis-
tributions over the data universe X. That is, letting ∆([X]) denote the
set of probability distributions over the set [|X|], we have x ∈∆([X]).

78
Releasing Linear Queries with Correlated Error
Note that we can always scale a database to have this property without
changing the normalized value of any linear query.
Algorithm 5 The Multiplicative Weights (MW) Update Rule. It is
instantiated with a parameter η ≤1. In the following analysis, we will
take η = α/2, where α is the parameter specifying our target accuracy.
MW(xt, ft, vt):
if vt < ft(xt) then
Let rt = ft
else
Let rt = 1 −ft
(i.e., for all χi, rt(χi) = 1 −ft[χi])
end if
Update: For all i ∈[|X|] Let
ˆxt+1
i
= exp(−ηrt[i]) · xt
i
xt+1
i
=
ˆxt+1
i
P|X|
j=1 ˆxt+1
j
Output xt+1.
Theorem 4.10. Fix a class of linear queries Q and a database x ∈
∆([X]), and let x1 ∈∆([X]) describe the uniform distribution over
X: x1
i = 1/|X| for all i. Now consider a maximal length sequence
of databases xt for t ∈{2, . . . , L} generated by setting xt+1 =
MW(xt, ft, vt) as described in Algorithm 5, where for each t, ft ∈Q
and vt ∈R are such that:
1. |ft(x) −ft(xt)| > α, and
2. |ft(x) −vt| < α.
Then it must be that:
L ≤1 + 4 log |X|
α2
.
Note that if we prove this theorem, we will have proven that for
the last database xL+1 in the sequence it must be that for all f ∈Q:

4.2. An online mechanism: private multiplicative weights
79
|f(x) −f(xL+1)| ≤α, as otherwise it would be possible to extend
the sequence, contradicting maximality. In other words, given distin-
guishing queries ft, the multiplicative weights update rule learns the
private database x with respect to any class of linear queries Q, up
to some tolerance α, in only a small number (L) of steps. We will
use this theorem as follows. The Private Online Multiplicative Weights
algorithm, described (twice!) below, will at all times t have a pub-
lic approximation xt to the database x. Given an input query f,
the algorithm will compute a noisy approximation to the diﬀerence
|f(x) −f(xt)|. If the (noisy) diﬀerence is large, the algorithm will pro-
vide a noisy approximation f(x)+λt to the true answer f(x), where λt
is drawn from some appropriately chosen Laplace distribution, and the
Multiplicative Weights Update Rule will be invoked with parameters
(xt, f, f(x)+λt). If the update rule is invoked only when the diﬀerence
|f(x) −f(xt)| is truly large (Theorem 4.10, condition 1), and if the
approximations f(x) + λt are suﬃciently accurate (Theorem 4.10, con-
dition 2), then we can apply the theorem to conclude that updates are
not so numerous (because L is not so large) and the resulting xL+1 gives
accurate answers to all queries in Q (because no distinguishing query
remains).
Theorem 4.10 is proved by keeping track of a potential function Ψ
measuring the similarity between the hypothesis database xt at time t,
and the true database D. We will show:
1. The potential function does not start out too large.
2. The potential function decreases by a signiﬁcant amount at each
update round.
3. The potential function is always non-negative.
Together, these 3 facts will force us to conclude that there cannot be
too many update rounds.
Let us now begin the analysis for the proof of the convergence
theorem.
Proof. We must show that any sequence {(xt, ft, vt)}t=1,...,L with the
property that |ft(xt) −ft(x)| > α and |vt −ft(x)| < α cannot have
L > 4 log |X|
α2
.

80
Releasing Linear Queries with Correlated Error
We deﬁne our potential function as follows. Recall that we here view
the database as a probability distribution — i.e., we assume ∥x∥1 = 1.
Of course this does not require actually modifying the real database.
The potential function that we use is the relative entropy, or KL diver-
gence, between x and xt (when viewed as probability distributions):
Ψt
def
= KL(x∥xt) =
|X|
X
i=1
x[i] log
 x[i]
xt[i]

.
We begin with a simple fact:
Proposition 4.11. For all t: Ψt ≥0, and Ψ1 ≤log |X|.
Proof. Relative entropy (KL-Divergence) is always a non-negative
quantity, by the log-sum inequality, which states that if a1, . . . , an and
b1, . . . , bn are non-negative numbers, then
X
i
ai log ai
bi
≥
 X
i
ai
! P
i ai
P
i bi
.
To see that Ψ1 ≤log |X|, recall that x1[i] = 1/|X| for all i, and so
Ψ1 = P|X|
i=1 x[i] log (|X|x[i]). Noting that x is a probability distribution,
we see that this quantity is maximized when x[1] = 1 and x[i] = 0 for
all i > 1, giving Ψi = log |X|.
We will now argue that at each step, the potential function drops
by at least α2/4. Because the potential begins at log |X|, and must
always be non-negative, we therefore know that there can be at most
L ≤4 log |X|/α2 steps in the database update sequence. To begin, let
us see exactly how much the potential drops at each step:
Lemma 4.12.
Ψt −Ψt+1 ≥η

⟨rt, xt⟩−⟨rt, x⟩

−η2

4.2. An online mechanism: private multiplicative weights
81
Proof. Recall that P|X|
i=1 x[i] = 1.
Ψt −Ψt+1 =
|X|
X
i=1
x[i] log
 
x[i]
xt
i
!
−
|X|
X
i=1
x[i] log
 
x[i]
xt+1
i
!
=
|X|
X
i=1
x[i] log
 
xt+1
i
xt
i
!
=
|X|
X
i=1
x[i] log
 
ˆxt+1
i
/ P
i ˆxt+1
i
xt
i
!
=
|X|
X
i=1
x[i]
"
log
 
xt
i exp(−ηrt[i]))
xt
i
!
−log


|X|
X
j=1
exp(−ηrt[j])xt
j




= −


|X|
X
i=1
x[i]ηrt[i]

−log


|X|
X
i=j
exp(−ηrt[j])xt
j


= −η⟨rt, x⟩−log


|X|
X
j=1
exp(−ηrt[j])xt
j


≥−η⟨rt, x⟩−log


|X|
X
j=1
xt
j(1 + η2 −ηrt[j])


= −η⟨rt, x⟩−log

1 + η2 −η⟨rt, xt⟩

≥η

⟨rt, xt⟩−⟨rt, x⟩

−η2.
The ﬁrst inequality follows from the fact that:
exp(−ηrt[j]) ≤1 −ηrt[j] + η2(rt[j])2 ≤1 −ηrt[j] + η2.
The second inequality follows from the fact that log(1 + y) ≤y for
y > −1.

82
Releasing Linear Queries with Correlated Error
The rest of the proof now follows easily. By the conditions of the
database/query sequence (described in the hypothesis for Theorem 4.10
above), for every t,
1. |ft(x) −ft(xt)| ≥α and
2. |vt −ft(x)| < α.
Thus, ft(x) < ft(xt) if and only if vt < ft(xt). In particular, rt = ft if
ft(xt) −ft(x) ≥α, and rt = 1 −ft if ft(x) −ft(xt) ≥α. Therefore, by
Lemma 4.12 and the choice of η = α/2 as described in the Update Rule,
Ψt −Ψt+1 ≥α
2

⟨rt, xt⟩−⟨rt, x⟩

−α2
4 ≥α
2 (α) −α2
4 = α2
4 .
Finally we know:
0 ≤ΨL ≤Ψ0 −L · α2
4 ≤log |X| −Lα2
4 .
Solving, we ﬁnd: L ≤4 log |X|
α2
. This completes the proof.
We can now combine the Multiplicative Weights Update Rule with
the NumericSparse algorithm to give an interactive query release mech-
anism. For (ϵ, 0) privacy, we essentially (with somewhat worse con-
stants) recover the bound for SmallDB. For (ϵ, δ)-diﬀerential privacy,
we obtain better bounds, by virtue of being able to use the compo-
sition theorem. The queries to NumericSparse are asking whether the
magnitude of the error given by estimating fi(x) by applying fi to the
current approximation xt to x is above an appropriately chosen thresh-
old T, that is, they are asking if |f(x) −f(xt)| is large. For technical
reasons this is done by asking about f(x)−f(xt) (without the absolute
value) and about f(xt) −f(x). Recall that the NumericSparse algo-
rithm responds with either ⊥or some (positive) value exceeding T. We
use the mnemonic E for the responses to emphasize that the query is
asking about an error.
Theorem 4.13. The Online Multiplicative Weights Mechanism (via
NumericSparse) is (ϵ, 0)-diﬀerentially private.

4.2. An online mechanism: private multiplicative weights
83
Algorithm 6 The Online Multiplicative Weights Mechanism (via
NumericSparse) takes as input a private database x, privacy param-
eters ϵ, δ, accuracy parameters α and β, and a stream of linear queries
{fi} that may be chosen adaptively from a class of queries Q. It outputs
a stream of answers {ai}.
OnlineMW via NumericSparse (x, {fi}, ϵ, δ, α, β)
Let c ←4 log |X|
α2
,
if δ = 0 then
Let T ←18c(log(2|Q|)+log(4c/β))
ϵ||x||1
else
Let T ←
(2+32
√
2)
p
c log 2
δ (log k+log 4c
β )
ϵ||x||1
end if
Initialize NumericSparse(x, {f′
i}, T, c, ϵ, δ) with a stream of queries
{f′
i}, outputting a stream of answers Ei.
Let t ←0, and let x0 ∈∆([X]) satisfy x0
i = 1/|X| for all i ∈[|X|].
for each query fi do
Let f′
2i−1(·) = fi(·) −fi(xt).
Let f′
2i(·) = fi(xt) −fi(·)
if E2i−1 = ⊥and E2i = ⊥then
Let ai = fi(xt)
else
if E2i−1 ∈R then
Let ai = fi(xt) + E2i−1
else
Let ai = fi(xt) −E2i
end if
Let xt+1 = MW(xt, fi, ai)
Let t ←t + 1.
end if
end for
Proof. This follows directly from the privacy analysis of Numeric-
Sparse, because the OnlineMW algorithm accesses the database only
through NumericSparse.

84
Releasing Linear Queries with Correlated Error
Speaking informally, the proof of utility for the Online Multi-
plicative Weights Mechanism (via NumericSparse) uses the utility
theorem for the NumericSparse (Theorem 3.28) to conclude that,
with high probability, the Multiplicative Weights Update Rule is only
invoked when the query ft is truly a distinguishing query, meaning,
|fi(x)−ft(xt)| is “large,” and the released noisy approximations to fi(x)
are “accurate.” Under this assumption, we can apply the convergence
theorem (Theorem 4.10) to conclude that the total number of updates
is small and therefore the algorithm can answer all queries in Q.
Theorem 4.14. For δ = 0, with probability at least 1−β, for all queries
fi, the Online Multiplicative Weights Mechanism (via NumericSparse)
returns an answer ai such that |fi(x) −ai| ≤3α for any α such that:
α ≥
32 log |X|

log(|Q|) + log
 32 log |X|
α2β

ϵα2||x||1
Proof. Recall that, by Theorem 3.28, given k queries and a maximum
number c of above-threshold queries, NumericSparse is (α, β)-accurate
for any α such that:
α ≥9c(log k + log(4c/β))
ϵ
.
In our case c = 4 log |X|/α2 and k = 2|Q|, and we have been normal-
izing, which reduces α by a factor of ||x||1. With this in mind, we can
take
α =
32 log |X|

log(|Q|) + log
 32 log |X|
α2β

ϵα2||x||1
and note that with this value we get T = 2α for the case δ = 0.
Assume we are in this high (1 −β) probability case. Then for all i
such that fi triggers an update, |fi(x) −fi(xt)| ≥T −α = α (The-
orem 4.10, condition 1). Thus, fi, ai form a valid pair of query/value
updates as required in the hypothesis of Theorem 4.10 and so, by that
theorem, there can be at most c = 4 log |X|
α2
such update steps.
In addition, still by the accuracy properties of the Sparse Vector
algorithm,
1. at most one of E2i−1, E2i will have value ⊥;

4.2. An online mechanism: private multiplicative weights
85
2. for all i such that no update is triggered (ai = fi(xt)) we have
|fi(x) −fi(xt)| ≤T + α = 3α; and
3. for all i such that an update is triggered we have |fi(x)−ai| ≤α
(Theorem 4.10, condition 2).
Optimizing the above expression for α and removing the normal-
ization factor, we ﬁnd that the OnlineMW mechanism can answer each
linear query to accuracy 3α except with probability β for:
α = ||x||2/3
1




36 log |X|

log(|Q|) + log

32 log |X|1/3||x||2/3
1
β

ϵ




1/3
which is comparable to the SmallDB mechanism.
By repeating the same argument, but instead using the utility the-
orem for the (ϵ, δ)-private version of Sparse Vector (Theorem 3.28), we
obtain the following theorem.
Theorem 4.15. For δ > 0, with probability at least 1−β, for all queries
fi, OnlineMW returns an answer ai such that |fi(x) −ai| ≤3α for any
α such that:
α ≥
(2 + 32
√
2) ·
q
log |X| log 2
δ

log |Q| + log
 32 log |X|
α2β

αϵ||x||1
Again optimizing the above expression for α and removing the nor-
malization factor, we ﬁnd that the OnlineMW mechanism can answer
each linear query to accuracy 3α except with probability β, for:
α = ||x||1/2
1



(2 + 32
√
2) ·
q
log |X| log 2
δ

log |Q| + log
 32||x||1
β

ϵ



1/2
which gives better accuracy (as a function of ||x||1) than the SmallDB
mechanism. Intuitively, the greater accuracy comes from the iterative
nature of the mechanism, which allows us to take advantage of our
composition theorems for (ϵ, δ)-privacy. The SmallDB mechanism runs

86
Releasing Linear Queries with Correlated Error
in just a single shot, and so there is no opportunity to take advantage
of composition.
The accuracy of the private multiplicative weights algorithm has
dependencies on several parameters, which are worth further discus-
sion. In the end, the algorithm answers queries using the sparse vec-
tor technique paired with a learning algorithm for linear functions. As
we proved in the last section, the sparse vector technique introduces
error that scales like O(c log k/(ϵ∥x∥1)) when a total of k sensitivity
1/∥x∥1 queries are made, and at most c of them can have “above thresh-
old" answers, for any threshold T. Recall that these error terms arise
because the privacy analysis for the sparse vector algorithm allows us
to “pay” only for the above threshold queries, and therefore can add
noise O(c/(ϵ∥x∥1)) to each query. On the other hand, since we end up
adding independent Laplace noise with scale Ω(c/(ϵ∥x∥1)) to k queries
in total, we expect that the maximum error over all k queries is larger
by a log k factor. But what is c, and what queries should we ask? The
multiplicative weights learning algorithm gives us a query strategy and
a guarantee that no more than c = O(log |X|/α2) queries will be above a
threshold of T = O(α), for any α. (The queries we ask are always: “ How
much does the real answer diﬀer from the predicted answer of the cur-
rent multiplicative weights hypothesis.” The answers to these questions
both give us the true answers to the queries, as well as instructions how
to update the learning algorithm appropriately when a query is above
threshold.) Together, this leads us to set the threshold to be O(α),
where α is the expression that satisﬁes: α = O(log |X| log k/(ϵ∥x∥1α2)).
This minimizes the two sources of error: error from the sparse vector
technique, and error from failing to update the multiplicative weights
hypothesis.
4.3
Bibliographical notes
The oﬄine query release mechanism given in this section is from Blum
et al. [8], which gave bounds in terms of the VC-Dimension of the query
class (Theorem 4.9). The generalization to fat shattering dimension is
given in [72].

4.3. Bibliographical notes
87
The online query release mechanism given in this section is from
Hardt and Rothblum [44]. This mechanism uses the classic multiplica-
tive weights update method, for which Arora, Hazan and Kale give an
excellent survey [1]. Slightly improved bounds for the private multi-
plicative weights mechanism were given by Gupta et al. [39], and the
analysis here follows the presentation from [39].

5
Generalizations
In this section we generalize the query release algorithms of the previous
section. As a result, we get bounds for arbitrary low sensitivity queries
(not just linear queries), as well as new bounds for linear queries. These
generalizations also shed some light on a connection between query
release and machine learning.
The SmallDB oﬄine query release mechanism in Section 4 is a
special case of what we call the net mechanism. We saw that both
mechanisms in that section yield synthetic databases, which provide a
convenient means for approximating the value of any query in Q on the
private database: just evaluate the query on the synthetic database and
take the result as the noisy answer. More generally, a mechanism can
produce a data structure of arbitrary form, that, together with a ﬁxed,
public, algorithm (independent of the database) provides a method for
approximating the values of queries.
The Net mechanism is a straightforward generalization of the
SmallDB mechanism: First, ﬁx, independent of the actual database, an
α-net of data structures such that evaluation of any query in Q using
the released data structure gives a good (within an additive α error)
estimate of the value of the query on the private database. Next, apply
88

5.1. Mechanisms via α-nets
89
the exponential mechanism to choose an element of this net, where the
quality function minimizes the maximum error, over the queries in Q,
for the elements of the net.
We also generalize the online multiplicative weights algorithm so
that we can instantiate it with any other online learning algorithm
for learning a database with respect to a set of queries. We note that
such a mechanism can be run either online, or oﬄine, where the set
of queries to be asked to the “online” mechanism is instead selected
using a “private distinguisher,” which identiﬁes queries on which the
current hypothesis of the learner diﬀers substantially from the real
database. These are queries that would have yielded an update step in
the online algorithm. A “distinguisher” turns out to be equivalent to an
agnostic learning algorithm, which sheds light on a source of hardness
for eﬃcient query release mechanisms.
In the following sections, we will discuss data structures for classes
of queries Q.
Deﬁnition 5.1. A data structure D drawn from some class of data
structures D for a class of queries Q is implicitly endowed with an
evaluation function Eval : D × Q →R with which we can evaluate any
query in Q on D. However, to avoid being encumbered by notation, we
will write simply f(D) to denote Eval(D, f) when the meaning is clear
from context.
5.1
Mechanisms via α-nets
Given a collection of queries Q, we deﬁne an α-net as follows:
Deﬁnition 5.2 (α-net). An α-net of data structures with respect to a
class of queries Q is a set N ⊂N|X| such that for all x ∈N|X|, there
exists an element of the α-net y ∈N such that:
max
f∈Q |f(x) −f(y)| ≤α .
We write Nα(Q) to denote an α-net of minimum cardinality among the
set of all α-nets for Q.

90
Generalizations
That is, for every possible database x, there exists a member of the
α-net that “looks like” x with respect to all queries in Q, up to an error
tolerance of α.
Small α-nets will be useful for us, because when paired with the
exponential mechanism, they will lead directly to mechanisms for
answering queries with high accuracy. Given a class of functions Q,
we will deﬁne an instantiation of the exponential mechanism known as
the Net mechanism. We ﬁrst observe that the Net mechanism preserves
ε-diﬀerential privacy.
Algorithm 7 The Net Mechanism
NetMechanism(x, Q, ε, α)
Let R ←Nα(Q)
Let q : N|X| × R →R be deﬁned to be:
q(x, y) = −max
f∈Q |f(x) −f(y)|
Sample And Output y ∈R with the exponential mechanism
ME(x, q, R)
Proposition 5.1. The Net mechanism is (ε, 0) diﬀerentially private.
Proof. The Net mechanism is simply an instantiation of the exponential
mechanism. Therefore, privacy follows from Theorem 3.10.
We may similarly call on our analysis of the exponential mechanism
to begin understanding the utility guarantees of the Net mechanism:
Proposition 5.2. Let Q be any class of sensitivity 1/∥x∥1 queries. Let y
be the database output by NetMechanism(x, Q, ε, α). Then with prob-
ability 1 −β:
max
f∈Q |f(x) −f(y)| ≤α +
2

log (|Nα(Q)|) + log

1
β

ε∥x∥1
.

5.2. The iterative construction mechanism
91
Proof. By applying Theorem 3.11 and noting that S(q) =
1
∥x∥1 , and
that OPTq(D) ≤α by the deﬁnition of an α-net, we ﬁnd:
Pr

max
f∈Q |f(x) −f(y)| ≥α +
2
ε∥x∥1
(log (|Nα(Q)|) + t)

≤e−t.
Plugging in t = log

1
β

completes the proof.
We can therefore see that an upper bound on |Nα(Q)| for a collec-
tion of functions Q immediately gives an upper bound on the accuracy
that a diﬀerentially private mechanism can provide simultaneously for
all functions in the class Q.
This is exactly what we did in Section 4.1, where we saw that the
key quantity is the VC-dimension of Q, when Q is a class of linear
queries.
5.2
The iterative construction mechanism
In this section, we derive an oﬄine generalization of the private mul-
tiplicative weights algorithm, which can be instantiated with any
properly deﬁned learning algorithm. Informally, a database update
algorithm maintains a sequence of data structures D1, D2, . . . that
give increasingly good approximations to the input database x (in a
sense that depends on the database update algorithm). Moreover, these
mechanisms produce the next data structure in the sequence by consid-
ering only one query f that distinguishes the real database in the sense
that f(Dt) diﬀers signiﬁcantly from f(x). The algorithm in this section
shows that, up to small factors, solving the query-release problem in a
diﬀerentially private manner is equivalent to solving the simpler learn-
ing or distinguishing problem in a diﬀerentially private manner: given
a private distinguishing algorithm and a non-private database update
algorithm, we get a corresponding private release algorithm. We can
plug in the exponential mechanism as a canonical private distinguisher,
and the multiplicative weights algorithm as a generic database update
algorithm for the general linear query setting, but more eﬃcient dis-
tinguishers are possible in special cases.

92
Generalizations
Syntactically, we will consider functions of the form U : D×Q×R →
D, where D represents a class of data structures on which queries in
Q can be evaluated. The inputs to U are a data structure in D, which
represents the current data structure Dt; a query f, which represents
the distinguishing query, and may be restricted to a certain set Q;
and also a real number, which estimates f(x). Formally, we deﬁne a
database update sequence, to capture the sequence of inputs to U used
to generate the database sequence D1, D2, . . ..
Deﬁnition 5.3 (Database Update Sequence).
Let x ∈N|X| be any
database and let
(Dt, ft, vt)
	
t=1,...,L ∈(D × Q × R)L be a sequence of
tuples. We say the sequence is a (U, x, Q, α, T)-database update sequence
if it satisﬁes the following properties:
1. D1 = U(⊥, ·, ·),
2. for every t = 1, 2, . . . , L,
ft(x) −ft(Dt)
 ≥α,
3. for every t = 1, 2, . . . , L, |ft(x) −vt| < α,
4. and for every t = 1, 2, . . . , L −1, Dt+1 = U(Dt, ft, vt).
We note that for all of the database update algorithms we consider,
the approximate answer vt is used only to determine the sign of ft(x)−
ft(Dt), which is the motivation for requiring that the estimate of ft(x)
(vt) have error smaller than α. The main measure of eﬃciency we’re
interested in from a database update algorithm is the maximum number
of updates we need to perform before the database Dt approximates x
well with respect to the queries in Q. To this end we deﬁne a database
update algorithm as follows:
Deﬁnition 5.4 (Database Update Algorithm).
Let U : D × Q × R →D
be an update rule and let T : R →R be a function. We say U is a T(α)-
database update algorithm for query class Q if for every database x ∈
N|X|, every (U, x, Q, α, L)-database update sequence satisﬁes L ≤T(α).
Note that the deﬁnition of a T(α)-database update algorithm
implies that if U is a T(α)-database update algorithm, then given any
maximal (U, x, Q, α, U)-database update sequence, the ﬁnal database
DL must satisfy maxf∈Q
f(x) −f(DL)
 ≤α or else there would exist

5.2. The iterative construction mechanism
93
another query satisfying property 2 of Deﬁnition 5.3, and thus there
would exist a (U, x, Q, α, L + 1)-database update sequence, contradict-
ing maximality. That is, the goal of a T(α) database update rule is
to generate a maximal database update sequence, and the ﬁnal data
structure in a maximal database update sequence necessarily encodes
the approximate answers to every query f ∈Q.
Now that we have deﬁned database update algorithms, we can
remark that what we really proved in Theorem 4.10 was that the Mul-
tiplicative Weights algorithm is a T(α)-database update algorithm for
T(α) = 4 log |X|/α2.
Before we go on, let us build some intuition for what a database
update algorithm is. A T(α)-database update algorithm begins with
some initial guess D1 about what the true database x looks like.
Because this guess is not based on any information, it is quite likely that
D1 and x bear little resemblance, and that there is some f ∈Q that is
able to distinguish between these two databases by at least α: that is,
that f(x) and f(D1) diﬀer in value by at least α. What a database
update algorithm does is to update its hypothesis Dt given evidence
that its current hypothesis Dt−1 is incorrect: at each stage, it takes as
input some query in Q which distinguishes its current hypothesis from
the true database, and then it outputs a new hypothesis. The parame-
ter T(α) is an upper bound on the number of times that the database
update algorithm will have to update its hypothesis: it is a promise
that after at most T(α) distinguishing queries have been provided, the
algorithm will ﬁnally have produced a hypothesis that looks like the
true database with respect to Q, at least up to error α.1 For a database
update algorithm, smaller bounds T(α) are more desirable.
Database Update Algorithms and Online Learning Algorithms:
We
remark that database update algorithms are essentially online learning
1Imagine that the database update algorithm is attempting to sculpt x out of a
block of clay. Initially, its sculpture D1 bears no resemblance to the true database: it
is simply a block of clay. However, a helpful distinguisher points out to the sculptor
places in which the clay juts out much farther than the true target database: the
sculptor dutifully pats down those bumps. If the distinguisher always ﬁnds large
protrusions, of magnitude at least α, the sculpture will be ﬁnished soon, and the
distinguisher’s time will not be wasted!

94
Generalizations
algorithms in the mistake bound model. In the setting of online learning,
unlabeled examples arrive in some arbitrary order, and the learning
algorithm must attempt to label them.
Background from Learning Theory.
In the mistake bound model of
learning, labeled examples (xi, yi) ∈X × {0, 1} arrive one at a time,
in a potentially adversarial order. At time i, the learning algorithm
A observes xi, and must make a prediction ˆyi about the label for
xi. It then sees the true label yi, and is said to make a mistake if
its prediction was wrong: i.e., if yi ̸= ˆyi. A learning algorithm A for
a class of functions C is said to have a mistake bound of M, if for
all f ∈C, and for all adversarially selected sequences of examples
(x1, f(x1)), . . . , (xi, f(xi)), . . ., A never makes more than M mistakes.
Without loss of generality, we can think of such a learning algorithm
as maintaining some hypothesis ˆf : X →{0, 1} at all times, and
updating it only when it makes a mistake. The adversary in this model
is quite powerful — it can choose the sequence of labeled examples
adaptively, knowing the current hypothesis of the learning algorithm,
and its entire history of predictions. Hence, learning algorithms
that have ﬁnite mistake bounds can be useful in extremely general
settings.
It is not hard to see that mistake bounded online learning algo-
rithms always exist for ﬁnite classes of functions C. Consider, for exam-
ple, the halving algorithm. The halving algorithm initially maintains a
set S of functions from C consistent with the examples that it has seen
so far: Initially S = C. Whenever a new unlabeled example arrives,
it predicts according to the majority vote of its consistent hypothe-
ses: that is, it predicts label 1 whenever |{f ∈S : f(xi) = 1}| ≥|S|/2.
Whenever it makes a mistake on an example xi, it updates S by remov-
ing any inconsistent function: S ←{f ∈S : f(xi) = yi}. Note that
whenever it makes a mistake, the size of S is cut in half! So long as
all examples are labeled by some function f ∈C, there is at least
one function f ∈C that is never removed from S. Hence, the halving
algorithm has a mistake bound of log |C|.
Generalizing beyond boolean labels, we can view database update
algorithms as online learning algorithms in the mistake bound model:

5.2. The iterative construction mechanism
95
here, examples that arrive are the queries (which may come in adver-
sarial order). The labels are the approximate values of the queries when
evaluated on the database. The database update algorithm hypothesis
Dt makes a mistake on query f if |f(Dt) −f(x)| ≥α, in which case
we learn the label of f (that is, vt) and allow the database update
algorithm to update the hypothesis. Saying that an algorithm U is a
T(α)-database update algorithm is akin to saying that it has a mis-
take bound of T(α): no adversarially chosen sequence of queries can
ever cause it to make more than T(α)-mistakes. Indeed, the database
update algorithms that we will see are taken from the online learning
literature. The multiplicative weights mechanism is based on an online
learning algorithm known as Hedge, which we have already discussed.
The Median Mechanism (later in this section) is based on the Halving
Algorithm, and the Perceptron algorithm is based (coincidentally) on
an algorithm known as Perceptron. We won’t discuss Perceptron here,
but it operates by making additive updates, rather than the multiplica-
tive updates used by multiplicative weights.
A database update algorithm for a class Q will be useful together
with a corresponding distinguisher, whose job is to output a function
that behaves diﬀerently on the true database x and the hypothesis Dt,
that is, to point out a mistake.
Deﬁnition 5.5 ((F(ε), γ)-Private Distinguisher). Let Q be a set of
queries, let γ ≥0 and let F(ε) : R →R be a function. An algo-
rithm Distinguishε : N|X| × D →Q is an (F(ε), γ)-Private Distin-
guisher for Q if for every setting of the privacy parameter ε, on every
pair of inputs x ∈N|X|, D ∈D it is (ε, 0)-diﬀerentially private with
respect to x and it outputs an f∗∈Q such that |f∗(x) −f∗(D)| ≥
maxf∈Q |f(x) −f(D)| −F(ε) with probability at least 1 −γ.
Remark 5.1. In machine learning, the goal is to ﬁnd a function f :
X →{0, 1} from a class of functions Q that best labels a collection of
labeled examples (x1, y1), . . . , (xm, ym) ∈X × {0, 1}. (Examples (x, 0)
are known as negative examples, and examples (x, 1) are known as pos-
itive examples). Each example xi has a true label yi, and a function
f correctly labels xi if f(xi) = yi. An agnostic learning algorithm for
a class Q is an algorithm that can ﬁnd the function in Q that labels

96
Generalizations
all of the data points approximately as well as the best function in Q,
even if no function in Q can perfectly label them. Note that equiva-
lently, an agnostic learning algorithm is one that maximizes the number
of positive examples labeled 1 minus the number of negative exam-
ples labeled 1. Phrased in this way, we can see that a distinguisher as
deﬁned above is just an agnostic learning algorithm: just imagine that
x contains all of the “positive” examples, and that y contains all of the
“negative examples.” (Note that it is ok of x and y are not disjoint —
in the learning problem, the same example can occur with both a pos-
itive and a negative label, since agnostic learning does not require that
any function perfectly label every example.) Finally, note also that for
classes of linear queries Q, a distinguisher is simply an optimization
algorithm. Because for linear queries f, f(x) −f(y) = f(x −y), a
distinguisher simply seeks to ﬁnd arg maxf∈Q |f(x −y)|.
Note that, a priori, a diﬀerentially private distinguisher is a weaker
object than a diﬀerentially private release algorithm: A distinguisher
merely ﬁnds a query in a set Q with the approximately largest value,
whereas a release algorithm must ﬁnd the answer to every query in Q. In
the algorithm that follows, however, we reduce release to optimization.
We will ﬁrst analyze the IC algorithm, and then instantiate it with
a speciﬁc distinguisher and database update algorithm. What follows
is a formal analysis, but the intuition for the mechanism is simple:
we simply run the iterative database construction algorithm to con-
struct a hypothesis that approximately matches x with respect to the
queries Q. If at each round our distinguisher succeeds in ﬁnding a query
that has high discrepancy between the hypothesis database and the true
database, then our database update algorithm will output a database
that is β-accurate with respect to Q. If the distinguisher ever fails to
ﬁnd such a query, then it must be that there are no such queries, and our
database update algorithm has already learned an accurate hypothesis
with respect to the queries of interest! This requires at most T itera-
tions, and so we access the data only 2T times using (ε0, 0)-diﬀerentially
private methods (running the given distinguisher, and then checking its
answer with the Laplace mechanism). Privacy will therefore follow from
our composition theorems.

5.2. The iterative construction mechanism
97
Algorithm 8 The Iterative Construction (IC) Mechanism. It takes as
input a parameter ε0, an (F(ε0), γ)-Private Distinguisher Distinguish
for Q, together with an T(α)-iterative database update algorithm U
for Q.
IC(x, α, ε0, Distinguish, U):
Let D0 = U(⊥, ·, ·).
for t = 1 to T(α/2) do
Let f(t) = Distinguish(x, Dt−1)
Let ˆv(t) = f(t)(x) + Lap

1
∥x∥1ε0

.
if |ˆv(t) −f(t)(Dt−1)| < 3α/4 then
Output y = Dt−1.
else
Let Dt = U(Dt−1, f(t), ˆv(t)).
end if
end for
Output y = DT(α/2).
The analysis of this algorithm just involves checking the tech-
nical details of a simple intuition. Privacy will follow because the
algorithm is just the composition of 2T(α) steps, each of which is
(ε0, 0)-diﬀerentially private. Accuracy follows because we are always
outputting the last database in a maximal database update sequence.
If the algorithm has not yet formed a maximal Database Update
Sequence, then the distinguishing algorithm will ﬁnd a distinguishing
query to add another step to the sequence.
Theorem 5.3. The IC algorithm is (ε, 0)-diﬀerentially private for
ε0 ≤ε/2T(α/2). The IC algorithm is (ε, δ)-diﬀerentially private for
ε0 ≤
ε
4√
T(α/2) log(1/δ).
Proof. The algorithm runs at most 2T(α/2) compositions of ε0-
diﬀerentially private algorithms. Recall from Theorem 3.20 that ε0
diﬀerentially private algorithms are 2kε0 diﬀerentially private under
2k-fold composition, and are (ε′, δ) private for ε′ =
p
4k ln(1/δ′)ε0 +
2kε0(eε0 −1). Plugging in the stated values for ε0 proves the claim.

98
Generalizations
Theorem 5.4. Given an (F(ε), γ)-private distinguisher, a parameter ε0,
and a T(α)-Database Update Algorithm, with probability at least 1−β,
the IC algorithm returns a database y such that: maxf∈Q |f(x)−f(y)| ≤
α for any α such that where:
α ≥max
8 log(2T(α/2)/β)
ε0∥x∥1
, 8F (ε0)

so long as γ ≤β/(2T(α/2)).
Proof. The analysis is straightforward.
Recall that if Yi ∼Lap(1/(ε∥x∥1)), we have: Pr[|Yi| ≥t/(ε∥x∥1)] =
exp(−t). By a union bound, if Y1, . . . , Yk ∼Lap(1/(ε∥x∥1)), then
Pr[maxi |Yi| ≥t/(ε∥x∥1)] ≤k exp(−t). Therefore, because we make
at most T(α/2) draws from Lap(1/(ε0∥x∥1)), except with probability
at most β/2, for all t:
|ˆv(t) −f(t)(x)| ≤
1
ε0∥x∥1
log 2T(α/2)
β
≤α
8 .
Note that by assumption, γ ≤β/(2T(α/2)), so we also have that except
with probability β/2:
|f(t)(x) −f(t)(Dt−1)| ≥max
f∈Q |f(x) −f(Dt−1)| −F(ε0)
≥max
f∈Q |f(x) −f(Dt−1)| −α
8 .
For the rest of the argument, we will condition on both of these events
occurring, which is the case except with probability β.
There are two cases. Either a data structure D′ = DT(α/2) is out-
put, or data structure D′ = Dt for t < T(α/2) is output. First, sup-
pose D′ = DT(α/2). Since for all t < T(α/2) it must have been the
case that |ˆv(t) −f(t)(Dt−1)| ≥3α/4 and by our conditioning, |ˆv(t) −
f(t)(x)| ≤α
8 , we know for all t: |f(t)(x) −f(t)(Dt−1)| ≥α/2. Therefore,
the sequence (Dt, f(t), ˆv(t)), formed a maximal (U, x, Q, α/2, T(α/2))-
Database Update Sequence (recall Deﬁnition 5.3), and we have that
maxf∈Q |f(x) −f(x′)| ≤α/2 as desired.
Next, suppose D′ = Dt−1 for t < T(α/2). Then it must have been
the case that for t, |ˆv(t) −f(t)(Dt−1)| < 3α/4. By our conditioning, in

5.2. The iterative construction mechanism
99
this case it must be that |f(t)(x)−f(t)(Dt−1)| < 7α
8 , and that therefore
by the properties of an (F(ε0), γ)-distinguisher:
max
f∈Q |f(x) −f(D′)| < 7α
8 + F(ε0) ≤α
as desired.
Note that we can use the exponential mechanism as a private dis-
tinguisher: take the domain to be Q, and let the quality score be:
q(D, f) = |f(D) −f(Dt)|, which has sensitivity 1/∥x∥1. Applying the
exponential mechanism utility theorem, we get:
Theorem 5.5. The exponential mechanism is an (F(ε), γ) distinguisher
for:
F(ε) =
2
∥x∥1ε

log |Q|
γ

.
Therefore, using the exponential mechanism as a distinguisher,
Theorem 5.4 gives:
Theorem 5.6. Given a T(α)-Database Update Algorithm and a param-
eter ε0 together with the exponential mechanism distinguisher, with
probability at least 1 −β, the IC algorithm returns a database y such
that: maxf∈Q |f(x) −f(y)| ≤α where:
α ≤max
8 log(2T(α/2)/β)
ε0∥x∥1
,
16
∥x∥1ε0

log |Q|
γ

so long as γ ≤β/(2T(α/2)).
Plugging in our values of ε0:
Theorem 5.7. Given a T(α)-Database Update Algorithm, together
with the exponential mechanism distinguisher, the IC mechanism is
ε-diﬀerentially private and with probability at least 1 −β, the IC algo-
rithm returns a database y such that: maxf∈Q |f(x)−f(y)| ≤α where:
α ≤8T(α/2)
∥x∥1ε

log |Q|
γ


100
Generalizations
and (ε, δ)-diﬀerentially private for:
α ≤16
p
T(α/2) log(1/δ)
∥x∥1ε

log |Q|
γ

so long as γ ≤β/(2T(α/2)).
Note that in the language of this section, what we proved in
Theorem 4.10 was exactly that the multiplicative weights algorithm
is a T(α)-Database Update Algorithm for T(α) = 4 log |X|
α2
. Plugging
this bound into Theorem 5.7 recovers the bound we got for the online
multiplicative weights algorithm. Note that now, however, we can plug
in other database update algorithms as well.
5.2.1
Applications: other database update algorithms
Here we give several other database update algorithms. The ﬁrst works
directly from α-nets, and therefore can get non-trivial bounds even
for nonlinear queries (unlike multiplicative weights, which only works
for linear queries). The second is another database update algorithm for
linear queries, but with bounds incomparable to multiplicative weights.
(In general, it will yield improved bounds when the dataset has size
close to the size of the data universe, whereas multiplicative weights
will give better bounds when the dataset is much smaller than the data
universe.)
We ﬁrst discuss the median mechanism, which takes advantage of
α-nets. The median mechanism does not operate on databases, but
instead on median data structures:
Deﬁnition 5.6 (Median Data Structure). A median data structure D is
a collection of databases: D ⊂N|X|. Any query f can be evaluated on
a median data structure as follows: f(D) = Median({f(x) : x ∈D}).
In words, a median data structure is just a set of databases. To
evaluate a query on it, we just evaluate the query on every database in
the set, and then return the median value. Note that the answers given
by the median data structure need not be consistent with any database!
However, it will have the useful property that whenever it makes an

5.2. The iterative construction mechanism
101
error, it will rule out at least half of the data sets in its collection as
being inconsistent with the true data set.
The median mechanism is then very simple:
Algorithm 9 The Median Mechanism (MM) Update Rule. It inputs
and outputs a median data structure. It is instantiated with an α-net
Nα(Q) for a query class Q, and its initial state is D = Nα(Q)
MMα,Q(Dt, ft, vt):
if Dt = ⊥then
Output D0 ←Nα(Q).
end if
if vt < ft(Dt) then
Output Dt+1 ←Dt \ {x ∈D : ft(x) ≥ft(Dt)}.
else
Output Dt+1 ←Dt \ {x ∈D : ft(x) ≤ft(Dt)}.
end if
The intuition for the median mechanism is as follows. It maintains
a set of databases that are consistent with the answers to the dis-
tinguishing queries it has seen so far. Whenever it receives a query
and answer that diﬀer substantially from the real database, it updates
itself to remove all of the databases that are inconsistent with the
new information. Because it always chooses its answer as the median
database among the set of consistent databases it is maintaining, every
update step removes at least half of the consistent databases! Moreover,
because the set of databases that it chooses initially is an α-net with
respect to Q, there is always some database that is never removed,
because it remains consistent on all queries. This limits how many
update rounds the mechanism can perform. How does the median
mechanism do?
Theorem 5.8. For any class of queries Q, The Median Mechanism is a
T(α)-database update algorithm for T(α) = log |Nα(Q)|.
Proof. We must show that any sequence {(Dt, ft, vt)}t=1,...,L with the
property that |ft(Dt) −ft(x)| > α and |vt −ft(x)| < α cannot have
L > log |Nα(Q)|. First observe that because D0 = Nα(Q) is an α-net

102
Generalizations
for Q, by deﬁnition there is at least one y such that y ∈Dt for all t
(Recall that the update rule is only invoked on queries with error at
least α. Since there is guaranteed to be a database y that has error less
than α on all queries, it is never removed by an update step). Thus,
we can always answer queries with Dt, and for all t, |Dt| ≥1. Next
observe that for each t, |Dt| ≤|Dt−1|/2. This is because each update
step removes at least half of the elements: all of the elements at least as
large as, or at most as large as the median element in Dt with respect
to query ft. Therefore, after L update steps, |DL| ≤1/2L · |Nα(Q)|.
Setting L > log |Nα(Q)| gives |DL| < 1, a contradiction.
Remark 5.2. For classes of linear queries Q, we may refer to the
upper bound on Nα(Q) given in Theorem 4.2 to see that the
Median Mechanism is a T(α)-database update algorithm for T(α) =
log |Q| log |X|/α2. This is worse than the bound we gave for the Multi-
plicative Weights algorithm by a factor of log |Q|. On the other hand,
nothing about the Median Mechanism algorithm is speciﬁc to linear
queries — it works just as well for any class of queries that admits a
small net. We can take advantage of this fact for nonlinear low sensi-
tivity queries.
Note that if we want a mechanism which promises (ε, δ)-privacy for
δ > 0, we do not even need a particularly small net. In fact, the trivial
net that simply includes every database of size ∥x∥1 will be suﬃcient:
Theorem 5.9. For every class of queries Q and every α ≥0, there is
an α-net for databases of size ∥x∥1 = n of size Nα(Q) ≤|X|n.
Proof. We can simply let Nα(Q) be the set of all |X|n databases y
of size ∥y∥1 = n. Then, for every x such that ∥x∥1 = n, we have
x ∈Nα(Q), and so clearly: miny∈Nα(Q) maxf∈Q |f(x) −f(y)| = 0.
We can use this fact to get query release algorithms for arbitrary
low sensitivity queries, not just linear queries. Applying Theorem 5.7
to the above bound, we ﬁnd:

5.2. The iterative construction mechanism
103
Theorem 5.10. Using the median mechanism, together with the
exponential mechanism distinguisher, the IC mechanism is (ε, δ)-
diﬀerentially private and with probability at least 1 −β, the IC algo-
rithm returns a database y such that: maxf∈Q |f(x)−f(y)| ≤α where:
α ≤
16
q
log |X| log 1
δ log
 2|Q|n log |X|
β

√nε
,
where Q can be any family of sensitivity 1/n queries, not necessarily
linear.
Proof. This follows simply by combining Theorems 5.8 and 5.9 to ﬁnd
that the Median Mechanism is a T(α)-Database Update Algorithm
for T(α) = n log |X| for databases of size ∥x∥1 = n for every α > 0
and every class of queries Q. Plugging this into Theorem 5.7 gives the
desired bound.
Note that this bound is almost as good as we were able to achieve
for the special case of linear queries in Theorem 4.15! However, unlike
in the case of linear queries, because arbitrary queries may not have
α-nets which are signiﬁcantly smaller than the trivial net used here,
we are not able to get nontrivial accuracy guarantees if we want (ε, 0)-
diﬀerential privacy.
The next database update algorithm we present is again for linear
queries, but achieves incomparable bounds to those of the multiplica-
tive weights database update algorithm. It is based on the Perceptron
algorithm from online learning (just as multiplicative weights is derived
from the hedge algorithm from online learning). Since the algorithm
is for linear queries, we treat each query ft ∈Q as being a vector
ft ∈[0, 1]|X|. Note that rather than doing a multiplicative update,
Algorithm 10 The Perceptron update rule
Perceptronα,Q(xt, ft, vt):
If: xt = ⊥then: output xt+1 = 0|X|
Else if: ft(xt) > vt then: output xt+1 = xt −
α
|X| · ft
Else if: ft(xt) ≤vt then: output xt+1 = xt +
α
|X| · ft

104
Generalizations
as in the MW database update algorithm, here we do an additive
update. In the analysis, we will see that this database update algo-
rithm has an exponentially worse dependence (as compared to multi-
plicative weights) on the size of the universe, but a superior dependence
on the size of the database. Thus, it will achieve better performance
for databases that are large compared to the size of the data universe,
and worse performance for databases that are small compared to the
size of the data universe.
Theorem 5.11. Perceptron is a T(α)-database update algorithm for:
T(α) =
∥x∥2
∥x∥1
2
· |X|
α2 .
Proof. Unlike for multiplicative weights, it will be more convenient to
analyze the Perceptron algorithm without normalizing the database to
be a probability distribution, and then prove that it is a T(α′) database
update algorithm for T(α′) = ∥x∥2
2|X|
α′2
. Plugging in α′ = α∥x∥1 will then
complete the proof. Recall that since each query ft is linear, we can
view ft ∈[0, 1]|X| as a vector with the evaluation of ft(x) being equal
to ⟨ft, x⟩.
We must show that any sequence {(xt, ft, vt)}t=1,...,L with the prop-
erty that |ft(xt) −ft(x)| > α′ and |vt −ft(x)| < α′ cannot have
L > ∥x∥2
2|X|
α′2
.
We use a potential argument to show that for every t = 1, 2, . . . , L,
xt+1 is signiﬁcantly closer to x than xt. Speciﬁcally, our potential func-
tion is the L2
2 norm of the database x −xt, deﬁned as
∥x∥2
2 =
X
i∈X
x(i)2.
Observe that ∥x −x1∥2
2 = ∥x∥2
2 since x1 = 0, and ∥x∥2
2 ≥0. Thus it
suﬃces to show that in every step, the potential decreases by α′2/|X|.
We analyze the case where ft(xt) > vt, the analysis for the opposite
case will be similar. Let Rt = xt −x. Observe that in this case we have
ft(Rt) = ft(xt) −ft(x) ≥α′.

5.2. The iterative construction mechanism
105
Now we can analyze the drop in potential.
∥Rt∥2
2 −∥Rt+1∥2
2 = ∥Rt∥2
2 −∥Rt −(α′/|X|) · ft∥2
2
=
X
i∈X
((Rt(i))2 −(Rt(i) −(α′/|X|) · ft(i))2)
=
X
i∈X
 
2α′
|X| · Rt(i)ft(i) −α′2
|X|2 ft(i)2
!
= 2α′
|X|ft(Rt) −α′2
|X|2
X
i∈X
ft(i)2
≥2α′
|X|ft(Rt) −α′2
|X|2 |X|
≥2α′2
|X| −α′2
|X| = α′2
|X|.
This bounds the number of steps by ∥x∥2
2|X|/α′2, and completes the
proof.
We may now plug this bound into Theorem 5.7 to obtain the fol-
lowing bound on the iterative construction mechanism:
Theorem 5.12. Using the perceptron database update algorithm,
together with the exponential mechanism distinguisher, the IC mecha-
nism is (ε, δ)-diﬀerentially private and with probability at least 1 −β,
the IC algorithm returns a database y such that: maxf∈Q |f(x)−f(y)| ≤
α where:
α ≤
4
√
4
p
∥x∥2 (4|X| ln(1/δ))1/4
r
log(2|Q∥X|·∥x∥2
2)
β
√ϵ∥x∥1
,
where Q is a class of linear queries.
If the database x represents the edge set of a graph, for example,
we will have xi ∈[0, 1] for all i, and so:
p
∥x∥2
∥x∥1
≤

1
∥x∥1
3/4
.
Therefore, the perceptron database update algorithm will outperform
the multiplicative weights database update algorithm on dense graphs.

106
Generalizations
5.2.2
Iterative construction mechanisms and online algorithms
In this section, we generalize the iterative construction framework to
the online setting by using the NumericSparse algorithm. The online
multiplicative weights algorithm which saw in the last chapter is an
instantiation of this approach. One way of viewing the online algo-
rithm is that the NumericSparse algorithm is serving as the private
distinguisher in the IC framework, but that the “hard work” of distin-
guishing is being foisted upon the unsuspecting user. That is: if the
user asks a query that does not serve as a good distinguishing query,
this is a good case. We cannot use the database update algorithm to
update our hypothesis, but we don’t need to! By deﬁnition, the cur-
rent hypothesis is a good approximation to the private database with
respect to this query. On the other hand, if the user asks a query for
which our current hypothesis is not a good approximation to the true
database, then by deﬁnition the user has found a good distinguishing
query, and we are again in a good case — we can run the database
update algorithm to update our hypothesis!
The idea of this algorithm is very simple. We will use a database
update algorithm to publicly maintain a hypothesis database. Every
time a query arrives, we will classify it as either a hard query, or an
easy query. An easy query is one for which the answer given by the
hypothesis database is approximately correct, and no update step is
needed: if we know that a given query is easy, we can simply compute
its answer on the publicly known hypothesis database rather than on
the private database, and incur no privacy loss. If we know that a query
is hard, we can compute and release its answer using the Laplace mech-
anism, and update our hypothesis using the database update algorithm.
This way, our total privacy loss is not proportional to the number of
queries asked, but instead proportional to the number of hard queries
asked. Because the database update algorithm guarantees that there
will not need to be many update steps, we can be guaranteed that the
total privacy loss will be small.
Theorem 5.13. OnlineIC is (ε, δ)-diﬀerentially private.

5.2. The iterative construction mechanism
107
Algorithm 11 The Online Iterative Construction Mechanism param-
eterized by a T(α)-database update algorithm U. It takes as input a
private database x, privacy parameters ε, δ, accuracy parameters α and
β, and a stream of queries {fi} that may be chosen adaptively from a
class of queries Q. It outputs a stream of answers {ai}.
OnlineICU(x, {fi}, ε, δ, α, β)
Let c ←T(α),
if δ = 0 then
Let T ←18c(log(2|Q|)+log(4c/β))
ϵ||x||1
else
Let T ←
(2+32
√
2)
p
c log 2
δ (log k+log 4c
β )
ϵ||x||1
end if
Initialize NumericSparse(x, {f′
i}, T, c, ε, δ) with a stream of queries
{f′
i}, outputting a stream of answers a′
i.
Let t ←0, D0 ∈x be such that D0
i = 1/|X| for all i ∈[|X|].
for each query fi do
Let f′
2i−1(·) = fi(·) −fi(Dt).
Let f′
2i(·) = fi(Dt) −fi(·)
if a′
2i−1 = ⊥and a′
2i = ⊥then
Let ai = fi(Dt)
else
if a′
2i−1 ∈R then
Let ai = fi(Dt) + a′
2i−1
else
Let ai = fi(Dt) −a′
2i
end if
Let Dt+1 = U(Dt, fi, ai)
Let t ←t + 1.
end if
end for
Proof. This follows directly from the privacy analysis of Numeric-
Sparse, because the OnlineIC algorithm accesses the database only
through NumericSparse.

108
Generalizations
Theorem 5.14. For δ = 0, With probability at least 1 −β, for all
queries fi, OnlineIC returns an answer ai such that |fi(x) −ai| ≤3α
for any α such that:
α ≥9T(α)(log(2|Q|) + log(4T(α)/β))
ϵ||x||1
.
Proof. Recall that by Theorem 3.28 that given k queries and a maxi-
mum number of above-threshold queries of c, Sparse Vector is (α, β)-
accurate for:
α = 9c(log k + log(4c/β))
ϵ||x||1
.
Here, we have c = T(α) and k = 2|Q|. Note that we have set the
threshold T = 2α in the algorithm. First let us assume that the sparse
vector algorithm does not halt prematurely. In this case, by the utility
theorem, except with probability at most β, we have for all i such that
ai = fi(Dt): |fi(D)−fi(Dt)| ≤T +α = 3α, as we wanted. Additionally,
for all i such that ai = a′
2i−1 or ai = a′
2i, we have |fi(D) −a′
i| ≤α.
Note that we also have for all i such that ai = a′
2i−1 or ai = a′
2i:
|fi(D) −fi(D′)| ≥T −α = α, since T = 2α. Therefore, fi, ai form a
valid step in a database update sequence. Therefore, there can be at
most c = T(α) such update steps, and so the Sparse vector algorithm
does not halt prematurely.
Similarly, we can prove a corresponding bound for (ε, δ)-privacy.
Theorem 5.15. For δ > 0, With probability at least 1 −β, for all
queries fi, OnlineIC returns an answer ai such that |fi(x) −ai| ≤3α
for any α such that:
α ≥
(
√
512 + 1)(ln(2|Q|) + ln 4T(α)
β
)
q
T(α) ln 2
δ
ϵ||x||1
We can recover the bounds we proved for online multiplicative
weights by recalling that the MW database update algorithm is a
T(α)-database update algorithm for T(α) = 4 log |X|
α2
. More generally,
we have that any algorithm in the iterative construction framework
can be converted into an algorithm which works in the interactive
setting without loss in accuracy. (i.e., we could equally well plug in

5.3. Connections
109
the median mechanism database update algorithm or the Perceptron
database update algorithm, or any other). Tantalizingly, this means
that (at least in the iterative construction framework), there is no gap in
the accuracy achievable in the online vs. the oﬄine query release mod-
els, despite the fact that the online model seems like it should be more
diﬃcult.
5.3
Connections
5.3.1
Iterative construction mechanism and α-nets
The Iterative Construction mechanism is implemented diﬀerently than
the Net mechanism, but at its heart, its analysis is still based on the
existence of small α-nets for the queries C. This connection is explicit
for the median mechanism, which is parameterized by a net, but it holds
for all database update algorithms. Note that the database output by
the iterative database construction algorithm is entirely determined
by the at most T functions f1, . . . , fT ∈Q fed into it, as selected by
the distinguisher while the algorithm is running. Each of these func-
tions can be indexed by at most log |Q| bits, and so every database
output by the mechanism can be described using only T log |Q| bits.
In other words, the IC algorithm itself describes an α-net for Q of
size at most Nα(Q) ≤|Q|T . To obtain error α using the Multi-
plicative Weights algorithm as an iterative database constructor, it
suﬃces by Theorem 4.10 to take T = 4 log |X|/α2, which gives us
Nα(Q) ≤|Q|4 log |X|/α2 = |X|4 log |Q|/α2. Note that up to the factor of
4 in the exponent, this is exactly the bound we gave using a diﬀerent α-
net in Theorem 4.2! There, we constructed an α-net by considering all
collections of log |Q|/α2 data points, each of which could be indexed by
log |X| bits. Here, we considered all collections of log |X|/α2 functions
in Q, each of which could be indexed by log |Q| bits. Both ways, we
got α-nets of the same size! Indeed, we could just as well run the Net
mechanism using the α-net deﬁned by the IC mechanism, to obtain the
same utility bounds. In some sense, one net is the “dual” of the other:
one is constructed of databases, the other is constructed of queries, yet
both nets are of the same size. We will see the same phenomenon in the

110
Generalizations
“boosting for queries” algorithm in the next section — it too answers
a large number of linear queries using a data structure that is entirely
determined by a small “net” of queries.
5.3.2
Agnostic learning
One way of viewing what the IC mechanism is doing is that it is
reducing the seemingly (information theoretically) more diﬃcult prob-
lem of query release to the easier problem of query distinguishing or
learning. Recall that the distinguishing problem is to ﬁnd the query
f ∈Q which varies the most between two databases x and y. Recall
that in learning, the learner is given a collection of labeled examples
(x1, y1), . . . , (xm, ym) ∈X × {0, 1}, where yi ∈{0, 1} is the label of xi.
If we view x as representing the positive examples in some large data
set, and y as representing the negative examples in the same data set,
then we can see that the problem of distinguishing is exactly the prob-
lem of agnostic learning. That is, a distinguisher ﬁnds the query that
best labels the positive examples, even when there is no query in the
class that is guaranteed to perfectly label them (Note that in this set-
ting, the same example can appear with both a positive and a negative
label — so the reduction still makes sense even when x and y are not
disjoint). Intuitively, learning should be an information-theoretically
easier problem than query release. The query release problem requires
that we release the approximate value of every query f in some class Q,
evaluated on the database. In contrast, the agnostic learning problem
asks only that we return the evaluation and identity of a single query:
the query that best labels the dataset. It is clear that information the-
oretically, the learning problem is no harder than the query release
problem. If we can solve the query release problem on databases x and
y, then we can solve the distinguishing problem without any further
access to the true private dataset, merely by checking the approximate
evaluations of every query f ∈Q on x and y that are made available
to us with our query release algorithm. What we have shown in this
section is that the reverse is true as well: given access to a private distin-
guishing or agnostic learning algorithm, we can solve the query release
problem by making a small (i.e., only log |X|/α2) number of calls to the

5.3. Connections
111
private distinguishing algorithm, with no further access to the private
dataset.
What are the implications of this? It tells us that up to small factors,
the information complexity of agnostic learning is equal to the infor-
mation complexity of query release. Computationally, the reduction is
only as eﬃcient as our database update algorithm, which, depending
on our setting and algorithm, may or may not be eﬃcient. But it tells
us that any sort of information theoretic bound we may prove for the
one problem can be ported over to the other problem, and vice versa.
For example, most of the algorithms that we have seen (and most of
the algorithms that we know about!) ultimately access the dataset by
making linear queries via the Laplace mechanism. It turns out that any
such algorithm can be seen as operating within the so-called statisti-
cal query model of data access, deﬁned by Kearns in the context of
machine learning. But agnostic learning is very hard in the statistical
query model: even ignoring computational considerations, there is no
algorithm which can make only a polynomial number of queries to the
dataset and agnostically learn conjunctions to subconstant error. For
query release this means that, in the statistical query model, there is
no algorithm for releasing conjunctions (i.e., contingency tables) that
runs in time polynomial in 1/α, where α is the desired accuracy level. If
there is a privacy preserving query release algorithm with this run-time
guarantee, it must operate outside of the SQ model, and therefore look
very diﬀerent from the currently known algorithms.
Because privacy guarantees compose linearly, this also tells us that
(up to the possible factor of log |X|/α2) we should not expect to be
able to privately learn to signiﬁcantly higher accuracy than we can
privately perform query release, and vice versa: an accurate algorithm
for the one problem automatically gives us an accurate algorithm for
the other.
5.3.3
A game theoretic view of query release
In this section, we take a brief sojourn into game theory to interpret
some of the query release algorithms we have (and will see). Let us
consider an interaction between two adversarial players, Alice and Bob.

112
Generalizations
Alice has some set of actions she might take, A, and Bob has a set of
actions B. The game is played as follows: simultaneously, Alice picks
some action a ∈A (possibly at random), and Bob picks some action
b ∈B (possibly at random). Alice experiences a cost c(a, b) ∈[−1, 1].
Alice wishes to play so as to minimize this cost, and since he is adver-
sarial, Bob wishes to play so as to maximize this cost. This is what is
called a zero sum game.
So how should Alice play? First, we consider an easier question.
Suppose we handicap Alice and require that she announce her ran-
domized strategy to Bob before she play it, and allow Bob to respond
optimally using this information? If Alice announces that she will draw
some action a ∈A according to a probability distribution DA, then Bob
will respond optimally so as to maximize Alice’s expected cost. That
is, Bob will play:
b∗= arg max
b∈B Ea∼DA[c(a, b)].
Hence, once Alice announces her strategy, she knows what her cost
will be, since Bob will be able to respond optimally. Therefore, Alice
will wish to play a distribution over actions which minimizes her cost
once Bob responds. That is, Alice will wish to play the distribution DA
deﬁned as:
DA = arg min
D∈∆A max
b∈B Ea∼D[c(a, b)].
If she plays DA (and Bob responds optimally), Alice will experience the
lowest possible cost that she can guarantee, with the handicap that she
must announce her strategy ahead of time. Such a strategy for Alice is
called a min-max strategy. Let us call the cost that Alice achieves when
playing a min-max strategy Alice’s value for the game, denoted vA:
vA = min
D∈∆A max
b∈B Ea∼D[c(a, b)].
We can similarly ask what Bob should play if we instead place him at
the disadvantage and force him to announce his strategy ﬁrst to Alice.
If he does this, he will play the distribution DB over actions b ∈B
that maximizes Alice’s expected cost when Alice responds optimally.
We call such a strategy DB for Bob a max-min strategy. We can deﬁne

5.3. Connections
113
Bob’s value for the game, vB, as the maximum cost he can ensure by
any strategy he might announce:
vB = max
D∈∆B min
a∈A Eb∼D[c(a, b)].
Clearly, vB ≤vA, since announcing one’s strategy is only a handicap.
One of the foundational results of game theory is Von-Neumann’s
min-max Theorem, which states that in any zero sum game, vA = vB.2
In other words, there is no disadvantage to “going ﬁrst” in a zero sum
game, and if players play optimally, we can predict exactly Alice’s cost:
it will be vA = vb ≡v, which we refer to as the value of the game.
Deﬁnition 5.7. In a zero sum game deﬁned by action sets A, B and a
cost function c : A × B →[−1, 1], let v be the value of the game. An
α-approximate min-max strategy is a distribution DA such that:
max
b∈B Ea∼DA[c(a, b)] ≤v + α
Similarly, an α-approximate max-min strategy is a distribution DB
such that:
min
a∈A Eb∼DB[c(a, b)] ≥v −α
If DA and DB are both α-approximate min-max and max-min strategies
respectively, then we say that the pair (DA, DB) is an α-approximate
Nash equilibrium of the zero sum game.
So how does this relate to query release?
Consider a particular zero sum-game tailored to the problem of
releasing a set of linear queries Q over a data universe X. First, assume
without loss of generality that for every f ∈Q, there is a query ˆf ∈Q
such that ˆf = 1−f (i.e., for each χ ∈X, ˆf(χ) = 1−f(χ)). Deﬁne Alice’s
action set to be A = X and deﬁne Bob’s action set to be B = Q. We will
refer to Alice as the database player, and to Bob as the query player.
Finally, ﬁxing a true private database x normalized to be a probability
distribution (i.e., ∥x∥1 = 1), deﬁne the cost function c : A×B →[−1, 1]
2Von Neumann is quoted as saying “As far as I can see, there could be no theory
of games ... without that theorem . . . I thought there was nothing worth publishing
until the Minimax Theorem was proved” [10].

114
Generalizations
to be: c(χ, f) = f(χ) −f(x). Let us call this game the “Query Release
Game.”
We begin with a simple observation:
Proposition 5.16. The value of the query release game is v = 0.
Proof. We ﬁrst show that vA = v ≤0. Consider what happens if we let
the database player’s strategy correspond to the true database: DA = x.
Then we have:
vA ≤max
f∈B Eχ∼DA[c(χ, f)]
= max
f∈B
|X|
X
i=1
f(χi) · xi −f(x)
= f(x) −f(x)
= 0.
Next we observe that v = vB ≥0. For point of contradiction, assume
that v < 0. In other words, that there exists a distribution DA such
that for all f ∈Q
Eχ∼DAc(χ, f) < 0.
Here, we simply note that by deﬁnition, if Eχ∼DAc(χ, f) = c < 0 then
Eχ∼DAc(χ, ˆf) = −c > 0, which is a contradiction since ˆf ∈Q.
What we have established implies that for any distribution DA that
is an α-approximate min-max strategy for the database player, we have
that for all queries f ∈Q: |Eχ∼DAf(χ)−f(x)| ≤α. In other words, the
distribution DA can be viewed as a synthetic database that answers
every query in Q with α-accuracy.
How about for nonlinear queries? We can repeat the same argument
above if we change the query release game slightly. Rather than letting
the database player have strategies corresponding to universe elements
χ ∈X, we let the database player have strategies corresponding to
databases themselves! Then, c(f, y) = |f(x) −f(y)|. Its not hard to
see that this game still has value 0 and that α-approximate min-max
strategies correspond to synthetic data which give α-accurate answers
to queries in Q.

5.4. Bibliographical notes
115
So how do we compute approximate min-max strategies in zero
sum games? There are many ways! It is well known that if Alice plays
the game repeatedly, updating her distribution on actions using an
online-learning algorithm with a no-regret guarantee (deﬁned in Sec-
tion 11.2), and Bob responds at each round with an approximately-cost-
maximizing response, then Alice’s distribution will quickly converge to
an approximate min-max strategy. Multiplicative weights is such an
algorithm, and one way of understanding the multiplicative weights
mechanism is as a strategy for Alice to play in the query release game
deﬁned in this section. (The private distinguisher is playing the role of
Bob here, picking at each round the query that corresponds to approx-
imately maximizing Alice’s cost). The median mechanism is another
such algorithm, for the game in which Alice’s strategies correspond to
databases, rather than universe elements, and so is also computing an
approximate min-max solution to the query release game.
However, there are other ways to compute approximate equilibria
as well! For example, Bob, the query player, could play the game using
a no-regret learning algorithm (such as multiplicative weights), and
Alice could repeatedly respond at each round with an approximately-
cost-minimizing database! In this case, the average over the databases
that Alice plays over the course of this experiment will converge to
an approximate min-max solution as well. This is exactly what is
being done in Section 6, in which the private base-sanitizer plays the
role of Alice, at each round playing an approximately cost-minimizing
database given Bob’s distribution over queries.
In fact, a third way of computing an approximate equilibrium of
a zero-sum game is to have both Alice and Bob play according to no-
regret learning algorithms. We won’t cover this approach here, but
this approach has applications in guaranteeing privacy not just to the
database, but also to the set of queries being asked, and to privately
solving certain types of linear programs.
5.4
Bibliographical notes
The Iterative Construction Mechanism abstraction (together with
the perception based database update algorithm) was formalized by

116
Generalizations
Gupta et al. [39], generalizing the median mechanism of Roth and
Roughgarden [74] (initially presented as an online algorithm), the
online private multiplicative weights mechanism of Hardt and Roth-
blum [44], and its oﬄine variant of Gupta et al. [38]; see also Hardt
et al. [41]. All these algorithm can be seen to be instantiations. The
connection between query release and agnostic learning was observed
in [38]. The observation that the median mechanism, when analyzed
using the composition theorems of Dwork et al. [32] for (ε, δ) privacy,
can be used to answer arbitrary low sensitivity queries is due to Hardt
and Rothblum. The game theoretic view of query release, along with its
applications to analyst privacy, is due to Hsu, Roth, and Ullman [48].

6
Boosting for Queries
In the previous sections, we have focused on the problem of private
query release in which we insist on bounding the worst-case error over
all queries. Would our problem be easier if we instead asked only for
low error on average, given some distribution over the queries? In this
section, we see that the answer is no: given a mechanism which is
able to solve the query release problem with low average error given
any distribution on queries, we can “boost” it into a mechanism which
solves the query release problem to worst-case error. This both sheds
light on the diﬃculty of private query release, and gives us a new tool
for designing private query release algorithms.
Boosting is a general and widely used method for improving
the accuracy of learning algorithms. Given a set of labeled training
examples
{(x1, y1), (x2, y2), . . . , (xm, ym)},
where each xi is drawn from an underlying distribution D on a universe
U, and each yi ∈{+1, −1}, a learning algorithm produces a hypothesis
h : U →{+1, −1}. Ideally, h will not just “describe” the labeling on
the given samples, but will also generalize, providing a reasonably accu-
rate method of classifying other elements drawn from the underlying
117

118
Boosting for Queries
distribution. The goal of boosting is to convert a weak base learner,
which produces a hypothesis that may do just a little better than ran-
dom guessing, into a strong learner, which yields a very accurate pre-
dictor for samples drawn according to D. Many boosting algorithms
share the following basic structure. First, an initial (typically uniform)
probability distribution is imposed on the sample set. Computation
then proceeds in rounds. In each round t:
1. The base learner is run on the current distribution, denoted Dt,
producing a classiﬁcation hypothesis ht; and
2. The hypotheses h1, . . . , ht are used to re-weight the samples,
deﬁning a new distribution Dt+1.
The process halts either after a predetermined number of rounds or
when an appropriate combining of the hypotheses is determined to be
suﬃciently accurate. Thus, given a base learner, the design decisions
for a boosting algorithm are (1) are how to modify the probability
distribution from one round to the next, and (2) how to combine the
hypotheses {ht}t=1,...,T to form a ﬁnal output hypothesis.
In this section we will use boosting on queries — that is, for the
purposes of the boosting algorithm the universe U is a set of queries
Q — to obtain an oﬄine algorithm for answering large numbers of arbi-
trary low-sensitivity queries. This algorithm requires less space than the
median mechanism, and, depending on the base learner, is potentially
more time eﬃcient as well.
The
algorithm
revolves
around
a
somewhat
magical
fact
(Lemma 6.5): if we can ﬁnd a synopsis that provides accurate answers
on a few selected queries, then in fact this synopsis provides accurate
answers on most queries! We apply this fact to the base learner, which
samples from a distribution on Q and produces as output a “weak”
synopsis that yields “good” answers for a majority of the weight in Q,
boosting, in a diﬀerentially private fashion, to obtain a synopsis that
is good for all of Q.
Although the boosting is performed over the queries, the privacy is
still for the rows of the database. The privacy challenge in boosting for
queries comes from the fact that each row in the database aﬀects the

6.1. The boosting for queries algorithm
119
answers to all the queries. This will manifest in the reweighting of the
queries: adjacent databases could cause radically diﬀerent reweightings,
which will be observable in the generated ht that, collectively, will form
the synopsis.
The running time of the boosting procedure depends quasi-linearly
on the number |Q| of queries and on the running time of the base
synopsis generator, independent of the data universe size |X|. This
yields a new avenue for constructing eﬃcient and accurate privacy-
preserving mechanisms, analogous to the approach enabled by boost-
ing in the machine learning literature: an algorithm designer can tackle
the (potentially much easier) task of constructing a weak privacy-
preserving base synopsis generator, and automatically obtain a stronger
mechanism.
6.1
The boosting for queries algorithm
We will use the row representation for databases, outlined in Section 2,
where we think of the database as a multiset of rows, or elements of X.
Fix a database size n, a data universe X, and a query set Q = {q :
X ∗→R} of real-valued queries of sensitivity at most ρ.
We assume the existence of a base synopsis generator (in Section 6.2
we will see how to construct these). The property we will need of the
base generator, formulated next, is that, for any distribution D on the
query set Q, the output of base generator can be used for computing
accurate answers for a large fraction of the queries, where the “large
fraction” is deﬁned in terms of the weights given by D. The base gen-
erator is parameterized by k, the number of queries to be sampled; λ,
an accuracy requirement for its outputs; η, a measurement of “large”
describing what we mean by a large fraction of the queries, and β, a
failure probability.
Deﬁnition
6.1
((k, λ, η, β)-base
synopsis
generator). For
a
ﬁxed
database size n, data universe X and query set Q, consider a synopsis
generator M, that samples k queries independently from a distribution
D on Q and outputs a synopsis. We say that M is a (k, λ, η, β)-base syn-
opsis generator if for any distribution D on Q, with all but β probability

120
Boosting for Queries
over the coin ﬂips of M, the synopsis S that M outputs is λ-accurate
for a (1/2 + η)-fraction of the mass of Q as weighted by D:
Pr
q∼D[|q(S) −q(x)| ≤λ] ≥1/2 + η.
(6.1)
The query-boosting algorithm can be used for any class of queries
and any diﬀerentially private base synopsis generator. The running
time is inherited from the base synopsis generator. The booster invests
additional time that is quasi-linear in |Q|, and in particular its running
time does not depend directly on the size of the data universe.
To specify the boosting algorithm we will need to specify a stopping
condition, an aggregation mechanism, and an algorithm for updating
the current distribution on Q.
Stopping Condition.
We will run the algorithm for a ﬁxed number T
of rounds — this will be our stopping condition. T will be selected so
as to ensure suﬃcient accuracy (with very high probability); as we will
see, log |Q|/η2 rounds will suﬃce.
Updating the Distribution.
Although the distributions are never
directly revealed in the outputs, the base synopses A1, A2, . . . , AT
are revealed, and each Ai can in principle leak information about
the queries chosen, from Di, in constructing Ai. We therefore need
to constrain the max-divergence between the probability distributions
obtained on neighboring databases. This is technically challenging
because, given Ai, the database is very heavily involved in constructing
Di+1.
The initial distribution, D1, will be uniform over Q. A standard
method for updating Dt is to increase the weight of poorly handled
elements, in our case, queries for which |q(x) −q(At)| > λ, by a ﬁxed
factor, say, e, and decrease the weight of well-handled elements by the
same factor. (The weights are then normalized so as to sum to 1.) To
get a feel for the diﬃculty, let x = y ∪{ξ}, and suppose that all queries
q are handled well by At when the database is y, but the addition
of ξ causes this to fail for, say, a 1/10 fraction of the queries; that is,
|q(y)−q(At)| ≤λ for all queries q, but |q(x)−q(At)| > λ for some |Q|/10
queries. Note that, since At “does well” on 9/10 of the queries even

6.1. The boosting for queries algorithm
121
when the database is x, it could be returned from the base sanitizer no
matter which of x, y is the true data set. Our concern is with the eﬀects
of the updating: when the database is y all queries are well handled and
there is no reweighting (after normalization), but when the database
is x there is a reweighting: one tenth of the queries have their weights
increased, the remaining nine tenths have their weights decreased. This
diﬀerence in reweighting may be detected in the next iteration via At+1,
which is observable, and which will be built from samples drawn from
rather diﬀerent distributions depending on whether the database is
x or y.
For example, suppose we start from the uniform distribution D1.
Then D(y)
2
= D(y)
1 , where by D(z)
i
we mean the distribution at round i
when the database is z. This is because the weight of every query is
decreased by a factor of e, which disappears in the normalization. So
each q ∈Q is assigned weight
1/|Q| in D(y)
2 . In contrast, when the
database is x the “unhappy” queries have normalized weight
e
|Q|
9
10
1
|Q|
1
e + 1
10
e
|Q|
.
Consider any such unhappy query q. The ratio D(x)
2 (q)/D(y)
2 (q) is
given by
D(x)
2 (q)
D(y)
2 (q)
=
e
|Q|
9
10
1
|Q|
1
e + 1
10
e
|Q|
1
|Q|
=
10
1 + 9
e2
def
= F ≈4.5085.
Now, ln F ≈1.506, and even though the choice of queries used in
round 2 by the base generator are not explicitly made public, they
may be detectable from the resulting A2, which is made public. Thus,
there is a potential privacy loss of up to 1.506 per query (of course,
we expect cancellations; we are simply trying to explain the source of
the diﬃculty). This is partially addressed by ensuring that the number
of samples used by the base generator is relatively small, although we
still have the problem that, over multiple iterations, the distributions
Dt may evolve very diﬀerently even on neighboring databases.

122
Boosting for Queries
The solution will be to attenuate the re-weighting procedure.
Instead of always using a ﬁxed ratio either for increasing the weight
(when the answer is “accurate”) or decreasing it (when it is not), we
set separate thresholds for “accuracy” (λ) and “inaccuracy” (λ + µ, for
an appropriately chosen µ that scales with the bit size of the output
of the base generator; see Lemma 6.5 below). Queries for which the
error is below or above these thresholds have their weight decreased
or increased, respectively, by a factor of e. For queries whose error lies
between these two thresholds, we scale the natural logarithm of the
weight change linearly: 1 −2(|q(x) −q(At)| −λ)/µ, so queries with
errors of magnitude exceeding λ + µ/2 increase in weight, and those
with errors of magnitude less than λ + µ/2 decrease in weight.
The attenuated scaling reduces the eﬀect of any individual on the
re-weighting of any query. This is because an individual can only aﬀect
the true answer to a query — and thus also the accuracy of the base
synopsis generator’s output q(At) — by a small amount, and the atten-
uation divides this amount by a parameter µ which will be chosen to
compensate for the kT samples chosen (total) from the T distributions
obtained over the course of the execution of the boosting algorithm.
This helps to ensure privacy. Intuitively, we view each of these kT sam-
ples as a “mini-mechanism.” We ﬁrst bound the privacy loss of sampling
at any round (Claim 6.4) and then bound the cumulative loss via the
composition theorem.
The larger the gap (µ) between the thresholds for “accurate” and
“inaccurate,” the smaller the eﬀect of each individual on a query’s
weight can be. This means that larger gaps are better for privacy. For
accuracy, however, large gaps are bad. If the inaccuracy threshold is
large, we can only guarantee that queries for which the base synopsis
generator is very inaccurate have their weight substantially increased
during re-weighting. This degrades the accuracy guarantee of the boost-
ing algorithm: the errors are roughly equal to the “inaccuracy” thresh-
old (λ + µ).
Aggregation.
For t ∈[T] we will run the base generator to obtain a
synopsis At. The synopses will be aggregated by taking the median:
given A1, . . . , AT , the quantity q(x) is estimated by taking the T

6.1. The boosting for queries algorithm
123
approximate values for q(x) computed using each of the Ai, and then
computing their median. With this aggregation method we can show
accuracy for query q by arguing that a majority of the Ai, 1 ≤i ≤T
provide λ + µ accuracy (or better) for q. This implies that the median
value of the T approximations to q(x) will be within λ + µ of the true
value.
Notation.
1. Throughout the algorithm’s operation, we keep track of several
variables (explicitly or implicitly). Variables indexed by q ∈Q
hold information pertaining to query q in the query set. Variables
indexed by t ∈[T], usually computed in round t, will be used to
construct the distribution Dt+1 used for sampling in time period
t + 1.
2. For a predicate P we use [[P]] to denote 1 if the predicate is true
and 0 if it is false.
3. There is a ﬁnal tuning parameter α used in the algorithm. It will
be chosen (see Corollary 6.3 below) to have value
α = α(η) = (1/2) ln
1 + 2η
1 −2η

.
The algorithm appears in Figure 6.1. The quantity ut,q in Step 2(2b)
is the new, un-normalized, weight of the query. For the moment, let us
set α = 1 (just so that we can ignore any α factors). Letting aj,q be
the natural logarithm of the weight change in round j, 1 ≤j ≤t, the
new weight is given by:
ut,q ←exp

−
t
X
j=1
aj,q

.
Thus, at the end of the previous step the un-normalized weight was
ut−1,q = exp(−Pt−1
j=1 aj,q) and the update corresponds to multiplication
by e−aj,t. When the sum Pt
j=1 aj,q is large, the weight is small. Every
time a synopsis gives a very good approximation to q(x), we add 1 to
this sum; if the approximation is only moderately good (between λ and

124
Boosting for Queries
Figure 6.1: Boosting for queries.
λ + µ/2), we add a positive amount, but less than 1. Conversely, when
the synopsis is very bad (worse than λ + µ accuracy), we subtract 1;
when it is barely acceptable (between λ + µ/2 and λ + µ), we subtract
a smaller amount.
In the theorem below we see an inverse relationship between privacy
loss due to sampling, captured by εsample, and the gap µ between the
thresholds for accurate and inaccurate.
Theorem 6.1. Let Q be a query family with sensitivity at most ρ. For
an appropriate setting of parameters, and with T = log |Q|/η2 rounds,
the algorithm of Figure 6.1 is an accurate and diﬀerentially private
query-boosting algorithm:
1. When instantiated with a (k, λ, η, β)-base synopsis generator, the
output of the boosting algorithm gives (λ + µ)-accurate answers
to all the queries in Q with probability at least 1 −Tβ, where
µ ∈O(((log3/2 |Q|)
√
k
q
log(1/β)ρ)/(εsample · η3)).
(6.2)

6.1. The boosting for queries algorithm
125
2. If the base synopsis generator is (εbase, δbase)-diﬀerentially pri-
vate, then the boosting algorithm is (εsample + T · εbase, δsample +
Tδbase)-diﬀerentially private.
Allowing the constant η to be swallowed up into the big-O nota-
tion, and taking ρ = 1 for simplicity, we get µ = O(((log3/2 |Q|)
√
k
p
log(1/β))/εsample). Thus we see that reducing the number k of input
queries needed by the base sanitizer improves the quality of the output.
Similarly, from the full statement of the theorem, we see that improv-
ing the generalization power of the base sanitizer, which corresponds to
having a larger value of η (a bigger “strong majority”), also improves
the accuracy.
Proof of Theorem 6.1. We ﬁrst prove accuracy, then privacy.
We introduce the notation a−
t,q and a+
t,q, satisfying
1. a−
t,q, a+
t,q ∈{−1, 1}; and
2. a−
t,q ≤at,q ≤a+
t,q.
Recall that a larger at,q indicates a higher quality of the approximation
of the synopsis At for q(x).
1. a−
t,q is 1 if At is λ-accurate on q, and −1 otherwise. To check that
a−
t,q ≤at,q, note that if a−
t,q = 1 then At is λ-accurate for q, and
so by deﬁnition at,q = 1 as well. If instead we have a−
t,q = −1 then
since we always have at,q ∈[−1, 1], we are done.
We will use the a−
t,q to lower bound a measure of the quality
of the output of the base generator. By the promise of the base
generator, At is λ-accurate for at least a 1/2 + η fraction of the
mass of Dt. Thus,
rt ≜
X
q∈Q
Dt[q] · a−
t,q ≥(1/2 + η) −(1/2 −η) = 2η.
(6.3)
2. a+
t,q is −1 if At is (λ + µ)-inaccurate for q, and 1 otherwise. To
check that at,q ≤a+
t,q, note that if a+
t,q = −1 then At is (λ + µ)-
inaccurate for q, so by deﬁnition at,q = −1 as well. If instead
a+
t,q = 1 then since we always have at,q ∈[−1, 1], we are done.
Thus a+
t,q is positive if and only if At is at least minimally
adequately accurate for q. We will use the a+
t,q to prove accuracy

126
Boosting for Queries
of the aggregation. When we sum the values a+
t,q, we get a pos-
itive number if and only if the majority of the At are providing
passable — that is, within λ + µ — approximations to q(x). In
this case the median value will be within λ + µ.
Lemma 6.2. After T rounds of boosting, with all but Tβ probability,
the answers to all but an exp(−η2T)-fraction of the queries are (λ+µ)-
accurate.
Proof. In the last round of boosting, we have:
DT+1[q] = uT,q
ZT
.
(6.4)
Since at,q ≤a+
t,q we have:
u+
T,q ≜e−αPT
t=1 a+
t,q ≤e−αPT
t=1 at,q = uT,q.
(6.5)
(The superscript “+” reminds us that this unweighted value was com-
puted using the terms a+
t,q.) Note that we always have u+
T,q ≥0. Com-
bining Equations (6.4) and (6.5), for all q ∈Q:
DT+1[q] ≥
u+
T,q
ZT
.
(6.6)
Recalling that [[P]] denotes the boolean variable that has value 1
if and only if the predicate P is true, we turn to examining the value
[[A is (λ+µ)-inaccurate for q]]. If this predicate is 1, then it must be the
case that the majority of {Aj}T
j=1 are (λ + µ)-inaccurate, as otherwise
their median would be (λ + µ)-accurate.
From our discussion of the signiﬁcance of the sign of PT
t=1 a+
t,q, we
have:
A is (λ + µ)-inaccurate for q ⇒
T
X
t=1
a+
t,q ≤0
⇔e−αPT
t=1 a+
t,q ≥1
⇔u+
T,q ≥1
Since u+
T,q ≥0, We conclude that:
[[A is (λ + µ)-inaccurate for q]] ≤u+
T,q

6.1. The boosting for queries algorithm
127
Using this together with Equation (6.6) yields:
1
|Q| ·
X
q∈Q
[[A is (λ + µ)-inaccurate for q]] ≤
1
|Q| ·
X
q∈Q
u+
T,q
≤
1
|Q| ·
X
q∈Q
DT+1[q] · ZT
= ZT
|Q|.
Thus the following claim completes the proof:
Claim 6.3. In round t of boosting, with all but tβ probability:
Zt ≤exp(−η2 · t) · |Q|
Proof. By deﬁnition of a base synopsis generator, with all but β prob-
ability, the synopsis generated is λ-accurate for at least a (1/2 + η)-
fraction of the mass of the distribution Dt. Recall that a−
t,q ∈{−1, 1}
is 1 if and only if At is λ-accurate on q, and that a−
t,q ≤at,q and recall
further the quantity rt ≜P
q∈Q Dt[q] · a−
t,q deﬁned in Equation (6.3).
As discussed above, rt measures the “success” of the base synopsis gen-
erator in round t, where by “success” we mean the stricter notion of
λ-accuracy. As summarized in Equation (6.3), if a (1/2 + η)-fraction of
the mass of Dt is computed with λ-accuracy, then rt ≥2η. Now observe
also that for t ∈[T], assuming the base sanitizer did not fail in round t:
Zt =
X
q∈Q
ut,q
=
X
q∈Q
ut−1,q · e−α·at,q
=
X
q∈Q
Zt−1 · Dt[q] · e−α·at,q
≤
X
q∈Q
Zt−1 · Dt[q] · e−α·a−
t,q
= Zt−1 ·
X
q∈Q
Dt[q] ·
  
1 + a−
t,q
2
!
· e−α +
 
1 −a−
t,q
2
!
· eα
!
(case analysis)

128
Boosting for Queries
= Zt−1
2
(eα + e−α) + rt(e−α −eα)

≤Zt−1
2
(eα + e−α) + 2η(e−α −eα)
 (rt ≥2η and (e−α −eα) ≤0)
By simple calculus we see that (eα+e−α)+2η(e−α−eα) is minimized
when
α = (1/2) ln
1 + 2η
1 −2η

.
Plugging this into the recurrence, we get
Zt ≤(
q
1 −4η2)t|Q| ≤exp(−2η2t)|Q|.
This completes the proof of Lemma 6.2.
The lemma implies that accuracy for all queries simultaneously can
be achieved by setting
T > ln |Q|
η2
.
Privacy.
We will show that the entire sequence (S1, A1, . . . , ST , AT )
can be output while preserving diﬀerential privacy. Note that this is
stronger than we need — we do not actually output the sets S1, . . . , ST .
By our adaptive composition theorems, the privacy of each Ai will be
guaranteed by the privacy guarantees of the base synopsis generator,
together with the fact that Si−1 was computed in a diﬀerentially private
way. Therefore, it suﬃces to prove that given that (S1, A1, . . . , Si, Ai)
is diﬀerentially private, Si+1 is as well. We can then combine the pri-
vacy parameters using our composition theorems to compute a ﬁnal
guarantee.
Lemma 6.4. Let ε∗= 4αTρ
µ . For all i ∈[T], once (S1, A1, . . . , Si, Ai) is
ﬁxed, the computation of each element of Si+1 is (ε∗, 0)-diﬀerentially
private.
Proof. Fixing A1, . . . , Ai, for every j ≤i, the quantity dq,j has sensi-
tivity ρ, since Aj(q) is database independent (because Aj is ﬁxed), and

6.1. The boosting for queries algorithm
129
every q ∈Q has sensitivity bounded by ρ. Therefore, for every j ≤i,
aj,q is 2ρ/µ sensitive by construction, and so
gi(q) def
=
i
X
j=1
aj,q
has sensitivity at most 2iρ/µ ≤2Tρ/µ. Then ∆gi
def
= 2Tρ/µ is an upper
bound on the sensitivity of gi.
To argue privacy, we will show that the selection of queries for Si+1
is an instance of the exponential mechanism. Think of −gi(q) as the
utility of a query q during the selection process at round i + 1. The
exponential mechanism says that to achieve (ε∗, 0)-diﬀerential privacy
we should choose q with probability proportional to
exp

−gi(q) ε∗
2∆gi

.
Since ε∗/2∆gi = α and the algorithm selects q with probability pro-
portional to e−αgi(q), we see that this is exactly what the algorithm
does!
We bound the privacy loss of releasing the Sis by treating each selec-
tion of a query as a “mini-mechanism” that, over the course of T rounds
of boosting, is invoked kT times. By Lemma 6.4 each mini-mechanism
is (4αTρ/µ, 0)-diﬀerentially private. By Theorem 3.20, for all β > 0
the composition of kT mechanisms, each of which is (α4Tρ/µ, 0)-
diﬀerentially private, is (εsample, δsample)-diﬀerentially private, where
εsample
def
=
q
2kT log(1/δsample)(α4Tρ/µ) + kT
α4Tρ
µ
2
.
(6.7)
Our total privacy loss comes from the composition of T calls to the
base sanitizer and the cumulative loss from the kT samples. We con-
clude that the boosting algorithm in its entirety is: (εboost, δboost)-
diﬀerentially private, where
εboost = Tεbase + εsample
δboost = Tδbase + δsample

130
Boosting for Queries
To get the parameters claimed in the statement of the theorem, we can
take:
µ ∈O((T 3/2√
k
q
log(1/β)αρ)/εsample).
(6.8)
6.2
Base synopsis generators
Algorithm SmallDB (Section 4) is based on the insight that a small
randomly selected subset of database rows provides good answers to
large sets of fractional counting queries. The base synopsis generators
described in the current section have an analogous insight: a small
synopsis that gives good approximations to the answers to a small
subset of queries also yields good approximations to most queries. Both
of these are instances of generalization bounds. In the remainder of this
section we ﬁrs prove a generalization bound and then use it to construct
diﬀerentially base synopsis generators.
6.2.1
A generalization bound
We have a distribution D over a large set Q of queries to be approx-
imated. The lemma below says that a suﬃciently small synopsis that
gives suﬃciently good approximations to the answers of a randomly
selected subset S ⊂Q of queries, sampled according to the distribution
D on Q, will, with high probability over the choice of S, also give good
approximations to the answers to most queries in Q (that is, to most
of the mass of Q, weighted by D). Of course, to make any sense the
synopsis must include a method of providing an answer to all queries
in Q, not just the subset S ⊆Q received as input. Our particular
generators, described in Sections 6.2.2 and Theorem 6.6 will produce
synthetic databases; to answer any query one can simply apply the
query to the synthetic database, but the lemma will be stated in full
generality.
Let R(y, q) denote the answer given by the synopsis y (when used as
input for the reconstruction procedure) on query q. A synopsis y λ-ﬁts a
database x w.r.t a set S of queries if maxq∈S |R(y, q)−q(x)| ≤λ. Let |y|

6.2. Base synopsis generators
131
denote the number of bits needed to represent y. Since our synopses will
be synthetic databases, |y| = N log2 |X| for some appropriately chosen
number N of universe elements. The generalization bound shows that
if y λ-ﬁts x with respect to a large enough (larger than |y|) randomly
chosen set S of queries sampled from a distribution D, then with high
probability y λ-ﬁts x for most of the mass of D.
Lemma 6.5. Let D be an arbitrary distribution on a query set Q =
{q : X ∗→R}. For all m ∈N, γ ∈(0, 1), η ∈[0, 1/2), let a =
2(log(1/γ) + m)/(m(1 −2η)). Then with probability at least 1−γ over
the choice of S ∼Da·m, every synopsis y of size at most m bits that
λ-ﬁts x with respect to the query set S, also λ-ﬁts x with respect to at
least a (1/2 + η)-fraction of D.
Before proving the lemma we observe that a is a compression factor:
we are squeezing the answers to am queries into an m-bit output, so
larger a corresponds to more compression. Typically, this means better
generalization, and indeed we see that if a is larger then, keeping m and
γ ﬁxed, we would be able to have larger η. The lemma also says that,
for any given output size m, the number of queries needed as input to
obtain an output that does well on a majority (1/2 + η fraction) of D
is only O(log(1/γ) + m). This is interesting because a smaller number
of queries k needed by the base generator leads, via the privacy loss
εsample due to sampling of kT queries and its inverse relationship to the
slackness µ (Equation 6.7), to improved accuracy of the output of the
boosting algorithm.
Proof of Lemma 6.5. Fix a set of queries S ⊂Q chosen independently
according to Da·m. Examine an arbitrary m-bit synopsis y. Note that y
is described by an m-bit string. Let us say y is bad if |R(y, q) −q(x)| >
λ for at least a (log(1/γ) + m)/(a · m) fraction of D, meaning that
Prq∼D[|R(y, q) −q(x)| > λ] ≥(log(1/γ) + m)/(a · m).
In other words, y is bad if there exists a set Qy ⊂Q of fractional
weight at least (log(1/γ)+m)/(a·m) such that |R(y, q)−q(x)| > λ for
q ∈Qy. For such a y, what is the probability that y gives λ-accurate
answers for every q ∈S? This is exactly the probability that none of

132
Boosting for Queries
the queries in S is in Qy, or
(1 −(log(1/γ) + m)/(a · m))a·m ≤e−(log(1/γ)+m) ≤γ · 2−m
Taking a union bound over all 2m possible choices for y, the probability
that there exists an m-bit synopsis y that is accurate on all the queries
in S but inaccurate on a set of fractional weight (log(1/β)+m)/(a·m)
is at most γ. Letting k = am = |S| we see that it is suﬃcient to
have
a > 2(log(1/γ) + m)
m · (1 −2η)
.
(6.9)
This simple lemma is extremely powerful. It tells us that when
constructing a base generator at round t, we only need to worry about
ensuring good answers for the small set of random queries sampled
from Dt; doing well for most of Dt will happen automatically!
6.2.2
The base generator
Our ﬁrst generator works by brute force. After sampling a set S of k
queries independently according to a distribution D, the base gener-
ator will produce noisy answers for all queries in S via the Laplace
mechanism. Then, making no further use of the actual database, the
algorithm searches for any database of size n for which these noisy
answers are suﬃciently close, and outputs this database. Privacy will
be immediate because everything after the k invocations of the Laplace
mechanism is in post-processing. Thus the only source of privacy loss is
the cumulative loss from these k invocations of the Laplace mechanism,
which we know how to analyze via the composition theorem. Utility will
follow from the utility of the Laplace mechanism — which says that we
are unlikely to have “very large” error on even one query — coupled
with the fact that the true database x is an n-element database that
ﬁts these noisy responses.1
1This argument assumes the size n of the database is known. Alternatively we
can include a noisy query of the form “How many rows are in the database?” and
exhaustively search all databases of size close to the response to this query.

6.2. Base synopsis generators
133
Theorem 6.6 (Base Synopsis Generator for Arbitrary Queries). For any
data universe X, database size n, and class Q : {X ∗→R} of queries
of sensitivity at most ρ, for any εbase, δbase > 0, there exists an
(εbase, δbase)-diﬀerentially private (k, λ, η = 1/3, β)-base synopsis gen-
erator for Q, where k = am > 6(m+log(2/β)) = 6(n log |X|+log(2/β))
and λ > 2b(log k + log(2/β)), where b = ρ
p
am log(1/δbase)/εbase.
The running time of the generator is
|X|n · poly(n, log(1/β), log(1/εbase), log(1/δbase)).
Proof. We ﬁrst describe the base generator at a high level, then deter-
mine the values for k and λ. The synopsis y produced by the base
generator will be a synthetic database of size n. Thus m = |y| =
n · log |X|. The generator begins by choosing a set S of k queries,
sampled independently according to D. It computes a noisy answer
for each query q ∈S using the Laplace mechanism, adding to each
true answer an independent draw from Lap(b) for an appropriate b to
be determined later. Let { d
q(x)}q∈Q be the collection of noisy answers.
The generator enumerates over all |X|n databases of size n, and out-
puts the lexicographically ﬁrst database y such that for every q ∈S
we have |q(y) −d
q(x)| ≤λ/2. If no such database is found, it outputs
⊥instead, and we say it fails. Note that if | d
q(x) −q(x)| < λ/2 and
|q(y) −d
q(x)| < λ/2, then |q(y) −q(x)| < λ.
There are two potential sources of failure for our particular gener-
ator. One possibility is that y fails to generalize, or is bad as deﬁned in
the proof of Lemma 6.5. A second possibility is that one of the samples
from the Laplace distribution is of excessively large magnitude, which
might cause the generator to fail. We will choose our parameters so
as to bound the probability of each of these events individually by at
most β/2.
Substituting η = 1/3 and m = n log |X| into Equation 6.9 shows
that taking a > 6(1 + log(2/β)/m) suﬃces in order for the probability
of failure due to the choice of S to be bounded by β/2. Thus, taking
k = am > 6(m + log(2/β)) = 6(n log |X| + log(2/β)) suﬃces.
We have k queries of sensitivity at most ρ. Using the Laplace
mechanism with parameter b = 2
p
2k log(1/δbase)ρ/εbase, ensures that
each query incurs privacy loss at most εbase/
p
2k ln(1/δbase), which by

134
Boosting for Queries
Corollary 3.21 ensures that the entire procedure will be (εbase, δbase)-
diﬀerentially private.
We will choose λ so that the probability that any draw from Lap(b)
has magnitude exceeding λ/2 is at most β/2. Conditioned on the event
that all k draws have magnitude at most λ we know that the input
database itself will λ-ﬁt our noisy answers, so the procedure will not fail.
Recall that the concentration properties of the Laplace distribution
ensure that with probability at least 1−et a draw from Lap(b) will have
magnitude bounded by tb. Setting λ/2 = tb, the probability that a given
draw will have magnitude exceeding λ/2 is bounded by e−t = e−λ/2b.
To ensure that none of the k draws has magnitude exceeding λ/2 it
suﬃces, by a union bound, to have
ke−λ/2b < β/2
⇔eλ/2b > k 2
β
⇔λ/2 > b(log k + log(2/β))
⇔λ > 2b(log k + log(2/β)).
The Special Case of Linear Queries.
For the special case of lin-
ear queries it is possible to avoid the brute force search for a
small database. The technique requires time that is polynomial in
(|Q|, |X|, n, log(1/β)). We will focus on the case of counting queries
and sketch the construction.
As in the case of the base generator for arbitrary queries, the base
generator begins by selecting a set S of k = am queries according to
D and computing noisy answers using Laplace noise. The generator for
linear queries then runs a syntheticizer on S which, roughly speaking,
transforms any synopsis giving good approximations to any set R of
queries into a synthetic database yielding approximations of similar
quality on the set R. The input to the syntheticizer will be the noisy
values for the queries in S, that is, R = S. (Recall that when we modify
the size of the database we always think in terms of the fractional
version of the counting queries: “What fraction of the database rows
satisﬁes property P?”)

6.2. Base synopsis generators
135
The resulting database may be quite large, meaning it may
have many rows. The base generator then subsamples only n′ =
(log k log(1/β))/α2 of the rows of the synthetic database, creating a
smaller synthetic database that with probability at least 1 −β has
α-accuracy with respect to the answers given by the large synthetic
database. This yields an m = ((log k log(1/β))/α2) log |X|-bit synopsis
that, by the generalization lemma, with probability (1−log(1/β)) over
the choice of the k queries, answers well on a (1/2 + η) fraction of Q
(as weighted by D).
As in the case of the base generator for arbitrary queries, we require
k = am > 6 log(1/β) + 6m. Taking α2 = (log Q)/n we get that
k > 6 log(1/β) + 6log k log(1/β) log |X|
α2
= 6 log(1/β) + 6n log k log(1/β)log |X|
log |Q|.
The syntheticizer is nontrivial. Its properties are summarized by
the following theorem.
Theorem 6.7. Let X be a data universe, Q a set of fractional counting
queries, and A an (ε, δ)-diﬀerentially private synopsis generator with
utility (α, β, 0) and arbitrary output. Then there exists a syntheticizer
A′ that is (ε, δ)-diﬀerentially private and has utility (3α, β, 0). A′ out-
puts a (potentially large) synthetic database. Its running time is poly-
nomial in the running time of A and (|X|, |Q|, 1/α, log(1/β)).
In our case, A is the Laplace mechanism, and the synopsis is sim-
ply the set of noisy answers. The composition theorem says that for A
to be (εbase, δbase)-diﬀerentially private the parameter to the Laplace
mechanism should be ρ/(εbase/
p
2k log(1/δbase)). For fractional count-
ing queries the sensitivity is ρ = 1/n.
Thus, when we apply the Theorem we will have an α of order
(
p
k log(1/β)/εbase)ρ. Here, ρ is the sensitivity. For counting queries
it is 1, but we will shift to fractional counting queries, so ρ = 1/n.
Proof Sketch for Theorem 6.7. Run A to get (diﬀerentially private)
(fractional) counts on all the queries in R. We will then use linear pro-
gramming to ﬁnd a low-weight fractional database that approximates

136
Boosting for Queries
these fractional counts, as explained below. Finally, we transform this
fractional database into a standard synthetic database by rounding the
fractional counts.
The output of A yields a fractional count for each query q ∈Q. The
input database x is never accessed again and so A′ is (ε, δ)-diﬀerentially
private. Let v be the resulting vector of counts, i.e., vq is the fractional
count that A’s output gives on query q. With probability 1 −β, all of
the entries in v are α-accurate.
A “fractional” database z that approximates these counts is
obtained as follows. Recall the histogram representation of a database,
where for each element in the universe X the histogram contains the
number of instances of this element in the database. Now, for every
i ∈X, we introduce a variable ai ≥0 that will “count” the (fractional)
number of occurrences of i in the fractional database z. We will impose
the constraint
X
i∈X
ai = 1.
We represent the count of query q in z as the sum of the count of items
i that satisfy q:
X
i∈X s.t. q(i)=1
ai
We want all of these counts to be within a an additive α accuracy of
the respective counts in vq. Writing this as a linear inequality we get:
(vq −α)
X
i∈X
ai ≤
X
i∈X s.t. q(i)=1
ai ≤(vq + α)
X
i∈X
ai.
When the counts are all α-accurate with respect to the counts in vc, it
is also the case that (with probability 1 −β) they are all 2α-accurate
with respect to the true counts on the original database x.
We write a linear program with two such constraints for each query
(a total of 2|Q| constraints). A′ tries to ﬁnd a fractional solution to
this linear program. To see that such a solution exists, observe that the
database x itself is α-close to the vector of counts v, and so there exists
a solution to the linear program (in fact even an integer solution), and
hence A′ will ﬁnd some fractional solution.

6.2. Base synopsis generators
137
We conclude that A′ can generate a fractional database with
(2α, β, 0)-utility, but we really want a synthetic (integer) database. To
transform the fractional database into an integer one, we round down
each ai, for i ∈X, to the closest multiple of α/|X|, this changes each
fractional count by at most a α/|X| additive factor, and so the rounded
counts have (3α, β, 0) utility. Now we can treat the rounded fractional
database (which has total weight 1), as an integer synthetic database
of (polynomial) size at most |X|/α.
Recall that in our application of Theorem 6.7 we deﬁned A
to be the mechanism that adds Laplace noise with parameter
ρ/(εbase/
p
2k log(1/δbase)). We have k draws, so by taking
α′ = ρ
q
2k log(1/δbase)(log k + log(1/β))
we have that A is (α′, β, 0)-accurate. For the base generator we chose
error α2 = (log |Q|)/n. If the output of the syntheticizer is too large,
we subsample
n′ = log |Q| log(1/β)
α2
= log k log(1/β)
α2
rows. With probability 1 −β the resulting database maintains
O(ρ
p
(log |Q|)/n + (
p
2k log(1/δbase)/εbase)(log k + log(1/β))-accuracy
on all of the concepts simultaneously.
Finally, the base generator can fail if the choice of queries S ∈Dk
does not lead to good generalization. With the parameters we have
chosen this occurs with probability at most β, leading to a total failure
probability of the entire generator of 3β.
Theorem 6.8 (Base Generator for Fractional Linear Queries). For any
data universe X, database size n, and class Q : {X n →R} of fractional
linear queries (with sensitivity at most 1/n), for any εbase, δbase > 0,
there exists an (εbase, δbase)-diﬀerentially private (k, λ, 1/3, 3β)-base
synopsis generator for Q, where
k = O
n log(|X|) log(1/β)
log |Q|

λ = O
 
log(1/β)
√n
 q
log |Q| +
s
log|X|
log|Q| ·
1
εbase
!!
.

138
Boosting for Queries
The running time of the base generator is poly(|X|, n, log(1/β),
log(1/εbase)).
The sampling bound used here is the same as that used in the
construction of the SmallDB mechanism, but with diﬀerent parameters.
Here we are using these bounds for a base generator in a complicated
boosting algorithm with a very small query set; there we are using them
for a single-shot generation of a synthetic database with an enormous
query set.
6.2.3
Assembling the ingredients
The total error comes from the choice of µ (see Equation 6.2) and λ,
the accuracy parameter for the based generator.
Let us recall Theorem 6.1:
Theorem 6.9 (Theorem 6.1). Let Q be a query family with sensitiv-
ity at most ρ. For an appropriate setting of parameters, and with
T = log |Q|/η2 rounds, the algorithm of Figure 6.1 is an accurate and
diﬀerentially private query-boosting algorithm:
1. When instantiated with a (k, λ, η, β)-base synopsis generator, the
output of the boosting algorithm gives (λ + µ)-accurate answers
to all the queries in Q with probability at least 1 −Tβ, where
µ ∈O(((log3/2 |Q|)
√
k
q
log(1/β)ρ)/(εsample · η3)).
(6.10)
2. If the base synopsis generator is (εbase, δbase)-diﬀerentially pri-
vate, then the boosting algorithm is ((εsample + T · εbase), T(β +
δbase))-diﬀerentially private.
By Equation 6.7,
εsample
def
=
q
2kT log(1/β)(α4Tρ/µ) + kT
α4Tρ
µ
2
,
where α = (1/2)(ln(1 + 2η)(1 −2η)) ∈O(1). We always have T =
(log |Q|)/η2, so substituting in this value into the above equation we
see that the bound
µ ∈O(((log3/2 |Q|)
√
k
q
log(1/β)ρ)/(εsample · η3))
in the statement of the theorem is acceptable.

6.3. Bibliographical notes
139
For the case of arbitrary queries, with η a constant, we have
λ ∈O

ρ
εbase
(
q
n log |X| log(1/δbase)(log(n log |X|) + log(2/β)))

.
Now, εboost = Tεbase + εsample. Set these two terms equal, so Tεbase =
εboost/2 = εsample, whence we can replace the 1/εbase term with
2T/εboost = (log |Q|/η2)/2εboost. Now our terms for λ and µ have sim-
ilar denominators, since η is constant. We may therefore conclude that
the total error is bounded by:
λ + µ ∈˜O
 p
n log |X|ρ log3/2 |Q|(log(1/β))3/2
εboost
!
.
With similar reasoning, for the case of fractional counting queries
we get
λ + µ ∈˜O
 p
log |X| log |Q| log(1/β)3/2
εboost
√n
!
.
To convert to a bound for ordinary, non-fractional, counting queries we
multiply by n to obtain
λ + µ ∈˜O
 p
n log |X| log |Q| log(1/β)3/2
εboost
!
.
6.3
Bibliographical notes
The boosting algorithm (Figure 6.1) is a variant of AdaBoost algorithm
of Schapire and Singer [78]. See Schapire [77] for an excellent survey
of boosting, and the textbook “Boosting” by Freund and Schapire [79]
for a thorough treatment. The private boosting algorithm covered in
this section is due to Dwork et al. [32], which also contains the base
generator for linear queries. This base generator, in turn, relies on the
syntheticizer of Dwork et al. [28]. In particular, Theorem 6.7 comes
from [28]. Dwork, Rothblum, and Vadhan also addressed diﬀerentially
private boosting in the usual sense.

7
When Worst-Case Sensitivity is Atypical
In this section, we brieﬂy describe two general techniques, both enjoying
unconditional privacy guarantees, that can often make life easier for the
data analyst, especially when dealing with a function that has arbitrary,
or diﬃcult to analyze, worst-case sensitivity. These algorithms are most
useful in computing functions that, for some exogenous reason, the
analyst has reason to believe are “usually” insensitive in practice.
7.1
Subsample and aggregate
The Subsample and Aggregate technique yields a method for “forcing”
the computation of a function f(x) to be insensitive, even for an arbi-
trary function f. Proving privacy will be trivial. Accuracy depends on
properties of the function f and the speciﬁc data set x; in particular,
if f(x) can be accurately estimated, with high probability, on f(S),
where S is a random subset of the elements in x, then accuracy should
be good. Many maximum likelihood statistical estimators enjoy this
property on “typical” data sets — this is why these estimators are
employed in practice.
140

7.1. Subsample and aggregate
141
Figure 7.1: Subsample and Aggregate with a generic diﬀerentially private aggre-
gation algorithm M.
In Subsample and Aggregate, the n rows of the database x are
randomly partitioned into m blocks B1, . . . , Bm, each of size n/m. The
function f is computed exactly, without noise, independently on each
block. The intermediate outcomes f(B1), . . . , f(Bm) are then combined
via a diﬀerentially private aggregation mechanism — typical examples
include standard aggregations, such as the α-trimmed mean,1 the Win-
sorized mean,2 and the median, but there are no restrictions — and
then adding Laplace noise scaled to the sensitivity of the aggregation
function in question; see Figure 7.1.
The key observation in Subsample and Aggregate is that any single
element can aﬀect at most one block, and therefore the value of just
a single f(Bi). Thus, changing the data of any individual can change
at most a single input to the aggregation function. Even if f is arbi-
trary, the analyst chooses the aggregation function, and so is free to
choose one that is insensitive, provided that choice is independent of the
database! Privacy is therefore immediate: For any δ ≥0 and any func-
tion f, if the aggregation mechanism M is (ε, δ)-diﬀerentially private
1The α-trimmed mean is the mean after the top and bottom α fraction of the
inputs have been discarded.
2The Winsorized mean is similar to the α-trimmed mean except that, rather than
being discarded, the top and bottom α fraction are replaced with the most extreme
remaining values.

142
When Worst-Case Sensitivity is Atypical
then so is the Subsample and Aggregate technique when instantiated
with f and M.3
Utility is a diﬀerent story, and it is frustratingly diﬃcult to argue
even for the case in which data are plentiful and large random sub-
sets are very likely to give similar results. For example, the data may
be labeled training points in high dimensional space and the function
is logistic regression, which produces a vector v and labels a point p
with +1 if and only if p · v ≥T for some (say, ﬁxed) threshold T.
Intuitively, if the samples are suﬃciently plentiful and typical then all
blocks should yield similar vectors v. The diﬃculty comes in getting a
good bound on the worst-case sensitivity of the aggregation function —
we may need to use the size of the range as a fallback. Nonetheless,
some nice applications are known, especially in the realm of statisti-
cal estimators, where, for example, it can be shown that, under the
assumption of “generic normality,” privacy can be achieved at no addi-
tional cost in statistical eﬃciency (roughly, accuracy as the number
of samples grows). We do not deﬁne generic normality here, but note
that estimators ﬁtting these assumptions include the maximum likeli-
hood estimator for “nice” parametric families of distributions such as
gaussians, and maximum-likelihood estimators for linear regression and
logistic regression.
Suppose the function f has a discrete range of cardinality m, say,
[m]. In this case Subsample and Aggregate will need to aggregate a set
of b elements drawn from [m], and we can use Report Noisy Arg-Max to
ﬁnd the most popular outcome. This approach to aggregation requires
b ≥log m to obtain meaningful results even when the intermediate
outcomes are unanimous. We will see an alternative below with no
such requirement.
Example 7.1 (Choosing a Model). Much work in statistics and machine
learning addresses the problem of model selection: Given a data set and
a discrete collection of “models,” each of which is a family of proba-
bility distributions, the goal is to determine the model that best “ﬁts”
3The choice of aggregation function can even depend on the database, but the
selection must be made in a diﬀerentially private fashion. The privacy cost is then
the cost of composing the choice operation with the aggregation function.

7.2. Propose-test-Release
143
the data. For example, given a set of labeled d-dimensional data, the
collection of models might be all subsets of at most s ≪d features, and
the goal is to ﬁnd the set of features that best permits prediction of the
labels. The function f might be choosing the best model from the given
set of of m models, a process known as model ﬁtting, via an arbitrary
learning algorithm. Aggregation to ﬁnd the most popular value could
be done via Report Noisy Max, which also yields an estimate of its
popularity.
Example 7.2 (Signiﬁcant Features). This is a special case of model ﬁt-
ting. The data are a collection of points in Rd and the function is the
very popular LASSO, which yields as output a list L ∈[d]s of at most
s ≪d signiﬁcant features. We can aggregate the output in two ways:
feature by feature — equivalent to running d executions of Subsample
and Aggregate, one for each feature, each with a range of size 2 — or
on the set as a whole, in which case the cardinality of the range is
 d
s
.
7.2
Propose-test-Release
At this point one might ask: what is the meaning of the aggregation if
there is not substantial agreement among the blocks? More generally,
for any reasonably large-scale statistical analysis in real life, we expect
the results to be fairly stable, independent of the presence or absence
of any single individual. Indeed, this is the entire intuition behind the
signiﬁcance of a statistic and underlying the utility of diﬀerential pri-
vacy. We can even go further, and argue that if a statistic is not stable,
we should have no interest in computing it. Often, our database will
in fact be a sample from a larger population, and our true goal is not
to compute the value of the statistic on the database itself, but rather
estimate it for the underlying population. Implicitly, therefore, when
computing a statistic we are already assuming that the statistic is sta-
ble under subsampling!
Everything we have seen so far has provided privacy even on very
“idiosyncratic” datasets, for which “typically” stable algorithms my be
highly unstable. In this section we introduce a methodology, Propose-
Test-Release, which is motivated by the philosophy that if there is

144
When Worst-Case Sensitivity is Atypical
insuﬃcient stability then the analysis can be abandoned because the
results are not in fact meaningful. That is, the methodology allows
the analyst to check that, on the given dataset, the function satisﬁes
some “robustness” or “stability” criterion and, if it does not, to halt
the analysis.
The goal of our ﬁrst application of Propose-Test-Release is to come
up with a variant of the Laplace mechanism that adds noise scaled
to something strictly smaller than the sensitivity of a function. This
leads to the notion of local sensitivity, which is deﬁned for a (function,
database) pair, say, (f, x). Quite simply, the local sensitivity of f with
respect to x is the amount by which the f(y) can diﬀer from f(x) for
any y adjacent to x.
Deﬁnition 7.1 (Local Sensitivity). The local sensitivity of a function
f : X n →Rk with respect to a database x is:
max
y adjacent to x ∥f(x) −f(y)∥1.
The Propose-Test-Release approach is to ﬁrst propose a bound,
say b, on local sensitivity — typically the data analyst has some idea
of what this should be — and then run a diﬀerentially private test
to ensure that the database is “far” from any database for which this
bound fails to hold. If the test is passed, then the sensitivity is assumed
to be bounded by b, and a diﬀerentially private mechanism such as, for
example, the Laplace mechanism with parameter b/ϵ, is used to release
the (slightly) noisy response to the query.
Note that we can view this approach as a two-party algorithm where
one party plays an honest data analyst and the other is the Laplace
mechanism. There is an interplay between the honest analyst and the
mechanism in which the algorithm asks for an estimate of the sensitiv-
ity and then “instructs” the mechanism to use this estimated sensitivity
in responding to subsequent queries. Why does it need to be so compli-
cated? Why can’t the mechanism simply add noise scaled to the local
sensitivity without playing this private estimation game? The reason
is that the local sensitivity may itself be sensitive. This fact, combined
with some auxiliary information about the database, can lead to pri-
vacy problems: the adversary may know that the database is one of x,

7.2. Propose-test-Release
145
which has very low local sensitivity for the computation in question,
and a neighboring y, for which the function has very high local sensi-
tivity. In this case the adversary may be able to guess rather accurately
which of x and y is the true database. For example, if f(x) = f(y) = s
and the reponse is far from s, then the adversary would guess y.
This is captured by the math of diﬀerential privacy. There are neigh-
boring instances of the median function which have the same median,
say, m, but arbitrarily large gaps in the local sensitivity. Suppose the
response R to the median query is computed via the Laplace mechanism
with noise scaled to the local sensitivity. When the database is x the prob-
ability mass is close to m, because the sensitivity is small, but when the
database is y the mass is far ﬂung, because the sensitivity is large. As
an extreme case, suppose the local sensitivity on x is exactly zero, for
example, X
= {0, 106}, n is even, and x, which has size n + 1, con-
tains 1 + n/2 zeros. Then the median of x is zero and the local sensitivity
of the median, when the database is x, is 0. In contrast, the neighbor-
ing database y has size n, contains n/2 zeros, has median zero (we have
deﬁned median to break ties in favor of the smaller value), and the local
sensitivity of the median, when the database is y, is 106. On x all the mass
of the Laplace mechanism (with parameter 0/ε = 0) is concentrated on
the single point 0; but on y the probability distribution has standard devi-
ation
√
2 · 106. This destroys all hope of diﬀerential privacy.
To test that the database is “far” from one with local sensitivity
greater than the proposed bound b, we may pose the query: “What is
the distance of the true database to the closest one with local sensi-
tivity exceeding b?” Distance to a ﬁxed set of databases is a (global)
sensitivity 1 query, so this test can be run in a diﬀerentially private
fashion by adding noise Lap(1/ε) to the true answer. To err on the side
of privacy, the algorithm can compare this noisy distance to a conser-
vative threshold — one that is only negligibly likely to be exceeded due
to a freak event of very large magnitude Laplace noise. For example,
if the threshold used is, say, ln2 n, the probability of a false positive
(passing the test when the local sensitivity in fact exceeds b) is at most
O(n−ε ln n), by the properties of the Laplace distribution. Because of
the negligible probability of a false positive, the technique cannot yield
(ε, 0)-diﬀerential privacy for any ε.

146
When Worst-Case Sensitivity is Atypical
To apply this methodology to consensus on blocks, as in our dis-
cussion of Subsample and Aggregate, view the intermediate results
f(B1), . . . , f(Bm) as a data set and consider some measure of the con-
centration of these values. Intuitively, if the values are tightly concen-
trated then we have consensus among the blocks. Of course, we still
need to ﬁnd the correct notion of concentration, one that is meaningful
and that has a diﬀerentially private instantiation. In a later section
we will deﬁne and weave together two notions of stability that seem
relevant to Subsample and Aggregate: insensitivity (to the removal or
addition of a few data points) and stability under subsampling, cap-
turing the notion that a subsample should yield similar results to the
full data set.
7.2.1
Example: the scale of a dataset
Given a dataset, a natural question to ask is, “What is the scale, or dis-
persion, of the dataset?” This is a diﬀerent question from data location,
which might be captured by the median or the mean. The data scale is
more often captured by the variance or an interquantile range. We will
focus on the interquartile range (IQR), a well-known robust estimator
for the scale of the data. We begin with some rough intuition. Suppose
the data are i.i.d. samples drawn from a distribution with cumulative
distribution function F. Then IQR(F), deﬁned as F −1(3/4)−F −1(1/4),
is a constant, depending only on F. It might be very large, or very tiny,
but either way, if the density of F is suﬃciently high at the two quar-
tiles, then, given enough samples from F, the empirical (that is, sample)
interquartile distance should be close to IQR(F).
Our Propose-Test-Release algorithm for the interquartile distance
ﬁrst tests how many database points need to be changed to obtain a
data set with a “suﬃciently diﬀerent” interquartile distance. Only if
the (noisy) reply is “suﬃciently large” will the algorithm release an
approximation to the interquartile range of the dataset.
The deﬁnition of “suﬃciently diﬀerent” is multiplicative, as an addi-
tive notion for diﬀerence of scale makes no sense — what would be the
right scale for the additive amount? The algorithm therefore works
with the logarithm of the scale, which leads to a multiplicative noise

7.2. Propose-test-Release
147
on the IQR. To see this, suppose that, as in what might be the typical
case, the sample interquartile distance cannot change by a factor of 2
by modifying a single point. Then the logarithm (base 2) of the sample
interquartile has local sensitivity bounded by 1. This lets us privately
release an approximation to the logarithm of the sample interquartile
range by adding to this value a random draw from Lap(1/ε).
Let IQR(x) denote the sample interquartile range when the the
data set is x.
The algorithm is (implicitly) proposing to add noise
drawn from Lap(1/ε) to the value log2(IQR(x)). To test whether this
magnitude of noise is suﬃcient for diﬀerential privacy, we discretize R
into disjoint bins {[k ln 2, (k+1) ln 2)}k∈Z and ask how many data points
must be modiﬁed in order to obtain a new database, the logarithm
(base 2) of whose interquartile range is in a diﬀerent bin than that
of log2(IQR(x)). If the answer is at least two then the local sensitivity
(of the logarithm of the interquartile range) is bounded by the bin
width. We now give more details.
To understand the choice of bin size, we write
log2(IQR(x)) = ln IQR(x)
ln 2
= c ln 2
ln 2 ,
whence we ﬁnd that looking at ln(IQR(x)) on the scale of ln 2 is equiv-
alent to looking at log2(IQR(x)) on the scale of 1. Thus we have scaled
bins which are intervals whose endpoints are a pair of adjacent inte-
gers: Bk = [k, k + 1), k ∈Z, and we let k1 = ⌊log2(IQR(x))⌋, so
log2(IQR(x)) ∈[k1, k1 + 1) and we say informally that the logarithm
of the IQR is in bin k1. Consider the following testing query:
Q0 : How many data points need to change in order to get
a new database z such that log2(IQR(z)) /∈Bk1?
Let A0(x) be the true answer to Q0 when the database is x.
If A0(x)
≥
2, then neighbors y of x satisfy | log2(IQR(y)) −
log2(IQR(x))| ≤1. That is, they are close to each other. This is
not equivalent to being in the same interval in the discretization:
log2(IQR(x)) may lie close to one of the endpoints of the interval
[k1, k1 + 1) and log2(IQR(y)) may lie just on the other side of the
endpoint. Letting R0 = A0(x) + Lap(1/ε), a small R0, even when the

148
When Worst-Case Sensitivity is Atypical
draw from the Laplace distribution has small magnitude, might not
actually indicate high sensitivity of the interquartile range. To cope
with the case that the local sensitivity is very small, but log2(IQR(x))
is very close to the boundary, we consider a second discretization
{B(2)
k
= [k−0.5, k+0.5)}k∈Z. We denote the two discretizations by B(1)
and B(2) respectively. The value log2(IQR(x)) — indeed, any value —
cannot be close to a boundary in both discretizations. The test is passed
if R0 is large in at least one discretization.
The Scale algorithm (Algorithm 12) below for computing database
scale assumes that n, the size of the database, is known, and the dis-
tance query (“How far to a database whose interquartile range has
sensitivity exceeding b?”) is asking how many points must be moved
to reach a database with high sensitivity of the IQR. We can avoid
this assumption by having the algorithm ﬁrst ask the (sensitivity 1)
query: “How many data points are in x?” We remark that, for techni-
cal reasons, to cope with the case IQR(x) = 0, we deﬁne log 0 = −∞,
⌊−∞⌋= −∞, and let [−∞, −∞) = {−∞}.
Algorithm 12 The Scale Algorithm (releasing the interquartile
range)
Require: dataset: x ∈X ∗, privacy parameters: ϵ, δ > 0
1: for the jth discretization (j = 1, 2) do
2:
Compute R0(x) = A0(x) + z0, where z0 ∈R Lap(1/ε).
3:
if R0 ≤1 + ln(1/δ) then
4:
Let s(j) = ⊥.
5:
else
6:
Let s(j) = IQR(x) × 2z(j)
s , where z(j)
s
∼Lap(1/ε).
7:
end if
8: end for
9: if s(1) ̸= ⊥then
10:
Return s(1).
11: else
12:
Return s(2).
13: end if

7.2. Propose-test-Release
149
Note that the algorithm is eﬃcient: let x(1), x(2), . . . , x(n) denote
the n database points after sorting, and let x(m) denote the median, so
m = ⌊(n+1)/2⌋. Then the local sensitivity of the median is max{x(m)−
x(m −1), x(m + 1) −x(m)} and, more importantly, one can compute
A0(x) by considering O(n) sliding intervals with width 2k1 and 2k1+1,
each having one endpoint in x. The computational cost for each interval
is constant.
We will not prove convergence bounds for this algorithm because,
for the sake of simplicity, we have used a base for the logarithm that
is far from optimal (a better base is 1 + 1/ ln n). We brieﬂy outline the
steps in the proof of privacy.
Theorem 7.1. Algorithm Scale (Algorithm 12) is (4ε, δ)-diﬀerentially
private.
Proof. (Sketch.) Letting s be shorthand for the result obtained with
a single discretization, and deﬁning D0 = {x : A0(x) ≥2}, the proof
shows:
1. The worst-case sensitivity of query Q0 is at most 1.
2. Neighboring databases are almost equally likely to result in ⊥:
For all neighboring database x, y:
Pr[s = ⊥|x] ≤eε Pr[s = ⊥|y].
3. Databases not in D0 are unlikely to pass the test:
∀x /∈D0 : Pr[s ̸= ⊥|x] ≤δ
2 .
4. ∀C ∈R+, x ∈D0 and all neighbors y of x:
Pr[s ∈C|x] ≤e2ε Pr[s ∈C|y] .
Thus, we get (2ε, δ/2)-diﬀerential privacy for each discretization.
Applying Theorem 3.16 (Appendix B), which says that “the epsilons
and the deltas add up,” yields (4ε, δ)-diﬀerential privacy.

150
When Worst-Case Sensitivity is Atypical
7.3
Stability and privacy
7.3.1
Two notions of stability
We begin by making a distinction between the two notions of stability
intertwined in this section: stability under subsampling, which yields
similar results under random subsamples of the data, and perturbation
stability, or low local sensitivity, for a given dataset. In this section we
will deﬁne and make use of extreme versions of both of these.
• Subsampling stability: We say f is q-subsampling stable on x if
f(ˆx) = f(x) with probability at least 3/4 when ˆx is a random
subsample from x which includes each entry independently with
probability q. We will use this notion in Algorithm Asamp, a vari-
ant of Sample and Aggregate.
• Perturbation Stability: We say that f is stable on x if f takes the
value f(x) on all of the neighbors of x (and unstable otherwise).
In other words, f is stable on x if the local sensitivity of f on x
is zero. We will use this notion (implemented in Algorithm Adist
below) for the aggregation step of Asamp.
At the heart of Algorithm Asamp is a relaxed version of perturba-
tion stability, where instead of requiring that the value be unchanged
on neighboring databases — a notion that makes sense for arbitrary
ranges, including arbitrary discrete ranges — we required only that the
value be “close” on neighboring databases — a notion that requires a
metric on the range.
Functions f with arbitrary ranges, and in particular the problem
of aggregating outputs in Subsample and Aggregate, motivate the next
algorithm, Adist. On input f, x, Adist outputs f(x) with high probability
if x is at distance at least
2 log(1/δ)
ε
from the nearest unstable data
set. The algorithm is conceptually trivial: compute the distance to the
nearest unstable data set, add Laplace noise Lap(1/ε), and check that
this noisy distance is at least 2 log(1/δ)
ε
. If so, release f(x), otherwise
output ⊥. We now make this a little more formal.
We begin by deﬁning a quantitative measure of perturbation
stability.

7.3. Stability and privacy
151
Deﬁnition 7.2. A function f : X ∗→R is k-stable on input x if adding
or removing any k elements from x does not change the value of f, that
is, f(x) = f(y) for all y such that |x△y| ≤k. We say f is stable on x
if it is (at least) 1-stable on x, and unstable otherwise.
Deﬁnition 7.3. The distance to instability of a data set x ∈X ∗with
respect to a function f is the number of elements that must be added
to or removed from y to reach a data set that is not stable under f.
Note that f is k-stable on x if and only if the distance of x to
instability is at least k.
Algorithm Adist, an instantiation of Propose-Test-Release for
discrete-valued functions g, appears in Figure 13.
Algorithm 13 Adist (releasing g(x) based on distance to instability)
Require: dataset: x ∈X ∗, privacy parameters: ϵ, δ > 0, function g :
X ∗→R
1: d ←distance from x to nearest unstable instance
2: ˆd ←d + Lap(1/ε)
3: if ˆd > log(1/δ)
ε
then
4:
Output g(x)
5: else
6:
Output ⊥
7: end if
The proof of the following propostion is immediate from the prop-
erties of the Laplace distribution.
Proposition 7.2. For every function g:
1. Adist is (ε, δ)-diﬀerentially private.
2. For all β > 0: if g is ln(1/δ)+ln(1/β)
ε
-stable on x, then Adist(x) =
g(x) with probability at least 1 −β, where the probability space
is the coin ﬂips of Adist.
This distance-based result is the best possible, in the following
sense: if there are two data sets x and y for which Adist outputs diﬀerent

152
When Worst-Case Sensitivity is Atypical
values g(x) and g(y), respectively, with at least constant probability,
then the distance from x to y must be Ω(log(1/δ)/ε).
Distance to instability can be diﬃcult to compute, or even to lower
bound, so this is not in general a practical solution. Two examples
where distance to instability turns out to be easy to bound are the
median and the mode (most frequently occurring value).
Adist may also be unsatisfactory if the function, say f, is not stable
on the speciﬁc datasets of interest. For example, suppose f is not stable
because of the presence of a few outliers in x. Instances of the average
behave this way, although for this function there are well know robust
alternatives such as the winsorized mean, the trimmed mean, and the
median. By what about for general functions f? Is there a method of
“forcing” an arbitrary f to be stable on a database x?
This will be the goal of Asamp, a variant of Subsample and Aggre-
gate that outputs f(x) with high probability (over its own random
choices) whenever f is subsampling stable on x.
7.3.2
Algorithm Asamp
In Asamp, the blocks B1, . . . , Bm are chosen with replacement, so that
each block has the same distribution as the inputs (although now an ele-
ment of x may appear in multiple blocks). We will call these subsampled
datasets ˆx1, . . . , ˆxm. The intermediate outputs z = {f(ˆx1), . . . , f(ˆxm)}
are then aggregated via Adist with function g = mode. The distance
measure used to estimate the stability of the mode on z is a scaled ver-
sion of the diﬀerence between the popularity of the mode and that of
the second most frequent value. Algorithm Asamp, appears in Figure 14.
Its running time is dominated by running f about 1/q2 times; hence it
is eﬃcient whenever f is.
The key property of Algorithm Asamp is that, on input f, x, it out-
puts f(x) with high probability, over its own random choices, when-
ever f is q-subsampling stable on x for q =
ε
64 log(1/δ). This result has
an important statistical interpretation. Recall the discussion of model
selection from Example 7.1. Given a collection of models, the sample
complexity of model selection is the number of samples from a dis-
tribution in one of the models necessary to select the correct model

7.3. Stability and privacy
153
with probability at least 2/3. The result says that diﬀerentially private
model selection increases the sample complexity of (non-private) model
selection by a problem-independent (and range-independent) factor of
O(log(1/δ)/ε).
Algorithm 14 Asamp: Bootstrapping for Subsampling-Stable f
Require: dataset: x, function f : X ∗→R, privacy parameters ϵ, δ >
0.
1: q ←
ϵ
64 ln(1/δ), m ←log(n/δ)
q2
.
2: Subsample m data sets ˆx1, ..., ˆxm from x, where ˆxi includes each
position of x independently with probability q.
3: if some element of x appears in more than 2mq sets ˆxi then
4:
Halt and output ⊥.
5: else
6:
z ←{f(ˆx1), · · · , f(ˆxm)}.
7:
For each r ∈R, let count(r) = #{i : f(ˆxi) = r}.
8:
Let count(i) denote the the ith largest count, i = 1, 2.
9:
d ←(count(1) −count(2))/(4mq) −1
10:
Comment Now run Adist(g, z) using d to estimate distance to
instability:
11:
ˆd ←d + Lap(1
ϵ).
12:
if ˆd > ln(1/δ)/ε then
13:
Output g(z) = mode(z).
14:
else
15:
Output ⊥.
16:
end if
17: end if
Theorem 7.3.
1. Algorithm Asamp is (ε, δ)-diﬀerentially private.
2. If f is q-subsampling stable on input x where q =
ε
64 ln(1/δ), then
algorithm Asamp(x) outputs f(x) with probability at least 1−3δ.
3. If f can be computed in time T(n) on inputs of length n, then
Asamp runs in expected time O(log n
q2 )(T(qn) + n).

154
When Worst-Case Sensitivity is Atypical
Note that the utility statement here is an input-by-input guaran-
tee; f need not be q-subsampling stable on all inputs. Importantly,
there is no dependence on the size of the range R. In the context of
model selection, this means that one can eﬃciently satisfy diﬀerential
privacy with a modest blowup in sample complexity (about log(1/δ)/ε)
whenever there is a particular model that gets selected with reasonable
probability.
The proof of privacy comes from the insensitivity of the compu-
tation of d, the privacy of the Propose-Test-Release technique, and
the privacy of Subsample and Aggregate, modiﬁed slightly to allow for
the fact that this algorithm performs sampling with replacement and
thus the aggregator has higher sensitivity, since any individual might
aﬀect up to 2mq blocks. The main observation for analyzing the util-
ity of this approach is that the stability of the mode is a function
of the diﬀerence between the frequency of the mode and that of the
next most popular element. The next lemma says that if f is subsam-
pling stable on x, then x is far from unstable with respect to the mode
g(z) = g(f(ˆx1), . . . , f(ˆxm)) (but not necessarily with respect to f), and
moreover one can estimate the distance to instability of x eﬃciently and
privately.
Lemma 7.4. Fix q ∈(0, 1). Given f : X ∗→R, let ˆf : X ∗→R be
the function ˆf = mode(f(ˆx1), ..., f(ˆxm)) where each ˆxi includes each
element of x independently with probability q and m = ln(n/δ)/q2. Let
d(z) = (count(1) −count(2))/(4mq) −1; that is, given a “database” z of
values, d(z)+1 isa scaled diﬀerence between the number of occurrences
of the two most popular values. Fix a data set x. Let E be the event
that no position of x is included in more than 2mq of the subsets ˆxi.
Then, when q ≤ε/64 ln(1/δ) we have:
1. E occurs with probability at least 1 −δ.
2. Conditioned on E, d lower bounds the stability of ˆf on x, and d
has global sensitivity 1.
3. If f is q-subsampling stable on x, then with probability at least
1 −δ over the choice of subsamples, we have ˆf(x) = f(x),
and, conditioned on this event, the ﬁnal test will be passed with

7.3. Stability and privacy
155
probability at least 1 −δ, where the probability is over the draw
from Lap(1/ε).
The events in Parts 2 and 3 occur simultaenously with probability at
least 1 −2δ.
Proof. Part 1 follows from the Chernoﬀbound. To prove Part 2, notice
that, conditioned on the event E, adding or removing one entry in
the original data set changes any of the counts count(r) by at most
2mq. Therefore, count(1) −count(2) changes by at most 4mq. This in
turn means that d(f(ˆx1), . . . , f(ˆxm)) changes by at most one for any x
and hence has global sensitivity of one. This also implies that d lower
bounds the stability of ˆf on x.
We now turn to part 3. We want to argue two facts:
1. If f is q-subsampling stable on x, then there is likely to be a large
gap between the counts of the two most popular bins. Speciﬁcally,
we want to show that with high probability count(1) −count(2) ≥
m/4. Note that if the most popular bin has count at least 5m/8
then the second most popular bin can have count at most 3m/8,
with a diﬀerence of m/4. By deﬁnition of subsampling stability
the most popular bin has an expected count of at least 3m/4 and
hence, by the Chernoﬀbound, taking α = 1/8, has probability
at most e−2mα2 = e−m/32 of having a count less than 5m/8. (All
the probabilities are over the subsampling.)
2. When the gap between the counts of the two most popular bins
is large, then the algorithm is unlikely to fail; that is, the test is
likely to succeed. The worry is that the draw from Lap(1
ε) will be
negative and have large absolute value, so that ˆd falls below the
threshold (ln(1/δ)/ε) even when d is large. To ensure this happens
with probability at most δ it suﬃces that d > 2 ln(1/δ)/ε.
By deﬁnition, d = (count(1) −count(2))/(4mq)−1, and, assuming
we are in the high probability case just described, this implies
d ≥m/4
4mq −1 =
1
16q −1

156
When Worst-Case Sensitivity is Atypical
so it is enough to have
1
16q > 2 ln(1/δ)/ε.
Taking q ≤ε/64 ln(1/δ) suﬃces.
Finally, note that with these values of q and m we have e−m/32 < δ.
Example 7.3. [The Raw Data Problem] Suppose we have an analyst
whom we can trust to follow instructions and only publish information
obtained according to these instructions. Better yet, suppose we have
b such analysts, and we can trust them not to communicate among
themselves. The analysts do not need to be identical, but they do need
to be considering a common set of options. For example, these options
might be diﬀerent statistics in a ﬁxed set S of possible statistics, and in
this ﬁrst step the analyst’s goal is to choose, for eventual publication,
the most signiﬁcant statistic in S. Later, the chosen statistic will be
recomputed in a diﬀerentially private fashion, and the result can be
published.
As described the procedure is not private at all: the choice of statis-
tic made in the ﬁrst step may depend on the data of a single individual!
Nonetheless, we can use the Subsample-and-Aggregate framework to
carry out the ﬁrst step, with the ith analyst receiving a subsample of
the data points and applying to this smaller database the function fi
to obtain an option. The options are then aggregated as in algorithm
Asamp; if there is a clear winner this is overwhelmingly likely to be the
selected statistic. This was chosen in a diﬀerentially private manner,
and in the second step it will be computed with diﬀerential privacy.
Bibliographic Notes
Subsample and Aggregate was invented by Nissim, Raskhodnikova, and
Smith [68], who were the ﬁrst to deﬁne and exploit low local sensitivity.
Propose-Test-Release is due to Dwork and Lei [22], as is the algorithm
for releasing the interquartile range. The discussion of stability and
privacy, and Algorithm Asamp which blends these two techniques, is
due to Smith and Thakurta [80]. This paper demonstrates the power of

7.3. Stability and privacy
157
Asamp by analyzing the subsampling stability conditions of the famous
LASSO algorithm and showing that diﬀerential privacy can be obtained
“for free,” via (a generalization of Asamp), precisely under the (ﬁxed
data as well as distributional) conditions for which LASSO is known
to have good explanatory power.

8
Lower Bounds and Separation Results
In this section, we investigate various lower bounds and tradeoﬀs:
1. How inaccurate must responses be in order not to completely
destroy any reasonable notion of privacy?
2. How does the answer to the previous question depend on the
number of queries?
3. Can we separate (ε, 0)-diﬀerential privacy from (ε, δ)-diﬀerential
privacy in terms of the accuracy each permits?
4. Is there an intrinsic diﬀerence between what can be achieved for
linear queries and for arbitrary low-sensitivity queries while main-
taining (ε, 0)-diﬀerential privacy?
A diﬀerent ﬂavor of separation result distinguishes the compu-
tational complexity of generating a data structure handling all the
queries in a given class from that of generating a synthetic database
that achieves the same goal. We postpone a discussion of this result to
Section 9.
158

8.1. Reconstruction attacks
159
8.1
Reconstruction attacks
We argued in Section 1 that any non-trivial mechanism must be ran-
domized. It follows that, at least for some database, query, and choice of
random bits, the response produced by the mechanism is not perfectly
accurate. The question of how inaccurate answers must be in order to
protect privacy makes sense in all computational models: interactive,
non-interactive, and the models discussed in Section 12.
For the lower bounds on distortion, we assume for simplicity that
the database consists of a single — but very sensitive — bit per
person, so we can think of the database as an n-bit Boolean vector
d = (d1, . . . , dn). This is an abstraction of a setting in which the
database rows are quite complex, for example, they may be medical
records, but the attacker is interested in one speciﬁc ﬁeld, such as the
presence or absence of the sickle cell trait. The abstracted attack con-
sists of issuing a string of queries, each described by a subset S of the
database rows. The query is asking how many 1’s are in the selected
rows. Representing the query as the n-bit characteristic vector S of
the set S, with 1s in all the positions corresponding to rows in S and
0s everywhere else, the true answer to the query is the inner product
A(S) = Pn
i=1 diSi.
Fix an arbitrary privacy mechanism. We will let r(S) denote the
response to the query S. This may be obtained explicitly, say, if the
mechanism is interactive and the query S is issued, or if the mecha-
nism is given all the queries in advance and produces a list of answers,
or implicitly, which occurs if the mechanism produces a synopsis from
which the analysts extracts r(S). Note that r(S) may depend on ran-
dom choices made by the mechanism and the history of queries. Let
E(S, r(S)) denote the error, also called noise or distortion, of the
response r(S), so E(S, r(S)) = |A(S) −r(S)|.
The question we want to ask is, “How much noise is needed in
order to preserve privacy?” Diﬀerential privacy is a speciﬁc privacy
guarantee, but one might also consider weaker notions, so rather than
guaranteeing privacy the modest goal in the lower bound arguments
will simply be to prevent privacy catastrophes.

160
Lower Bounds and Separation Results
Deﬁnition 8.1. A mechanism is blatantly non-private if an adversary
can construct a candidate database c that agrees with the real database
d in all but o(n) entries, i.e., ∥c −d∥0 ∈o(n).
In other words, a mechanism is blatantly non-private if it permits
a reconstruction attack that allows the adversary to correctly guess
the secret bit of all but o(n) members of the database. (There is no
requirement that the adversary know on which answers it is correct.)
Theorem 8.1. Let M be a mechanism with distortion of magnitude
bounded by E. Then there exists an adversary that can reconstruct the
database to within 4E positions.
An easy consequence of the theorem is that a privacy mechanism
adding noise with magnitude always bounded by, say, n/401, permits
an adversary to correctly reconstruct 99% of the entries.
Proof. Let d be the true database. The adversary attacks in two phases:
1. Estimate the number of 1s in all possible sets: Query M
on all subsets S ⊆[n].
2. Rule out “distant” databases: For every candidate database
c ∈{0, 1}n, if ∃S ⊆[n] such that | P
i∈S ci −M(S)| > E, then
rule out c. If c is not ruled out, then output c and halt.
Since M(S) never errs by more than E, the real database will not be
ruled out, so this simple (but ineﬃcient!) algorithm will output some
candidate database c. We will argue that the number of positions in
which c and d diﬀer is at most 4 · E.
Let I0 be the indices in which di = 0, that is, I0 = {i | di = 0}.
Similarly, deﬁne I1 = {i | di = 1}. Since c was not ruled out, |M(I0) −
P
i∈I0 ci| ≤E. However, by assumption |M(I0) −P
i∈I0 di| ≤E. It
follows from the triangle inequality that c and d diﬀer in at most 2E
positions in I0; the same argument shows that they diﬀer in at most 2E
positions in I1. Thus, c and d agree on all but at most 4E positions.
What if we consider more realistic bounds on the number of queries?
We think of √n as an interesting threshold on noise, for the following
reason: if the database contains n people drawn uniformly at random

8.1. Reconstruction attacks
161
from a population of size N ≫n, and the fraction of the population
satisfying a given condition is p, then we expect the number of rows
in the database satisfying the property to be roughly np ± Θ(√n), by
the properties of the binomial distribution. That is, the sampling error
is on the order of √n. We would like that the noise introduced for
privacy is smaller than the sampling error, ideally o(√n). The next
result investigates the feasibility of such small error when the number
of queries is linear in n. The result is negative.
Ignoring computational complexity, to see why there might exist
a query-eﬃcient attack we modify the problem slightly, looking at
databases d ∈{−1, 1}n and query vectors v ∈{−1, 1}n. The true
answer is again deﬁned to be d · v, and the response is a noisy version
of the true answer. Now, consider a candidate database c that is far
from d, say, ∥c−d∥0 ∈Ω(n). For a random v ∈R {−1, 1}n, with constant
probability we have (c −d) · v ∈Ω(√n). To see this, ﬁx x ∈{−1, 1}n
and choose v ∈R {−1, 1}n. Then x · v is a sum of independent random
variables xivi ∈R {−1, 1}, which has expectation 0 and variance n, and
is distributed according to a scaled and shifted binomial distribuiton.
For the same reason, if c and d diﬀer in at least αn rows, and v is
chosen at random, then (c −d) · v is binomially distributed with mean
0 and variance at least αn. Thus, we expect c·v and d·v to diﬀer by at
least α√n with constant probability, by the properties of the binomial
distribution. Note that we are using the anti-concentration property of
the distribution, rather than the usual appeal to concentration.
This opens an attack for ruling out c when the noise is constrained
to be o(√n): compute the diﬀerence between c·v and the noisy response
r(v). If the magnitude of this diﬀerence exceeds √n — which will
occur with constant probability over the choice of v — then rule out c.
The next theorem formalizes this argument and further shows that
the attack is resilient even to a large fraction of completely arbitrary
responses: Using a linear number of ±1 questions, an attacker can
reconstruct almost the whole database if the curator is constrained to
answer at least 1
2 +η of the questions within an absolute error of o(√n).
Theorem 8.2. For any η > 0 and any function α = α(n), there is
constant b and an attack using bn ±1 questions that reconstructs a

162
Lower Bounds and Separation Results
database that agrees with the real database in all but at most (2α
η )2
entries, if the curator answers at least 1
2 + η of the questions within an
absolute error of α.
Proof. We begin with a simple lemma.
Lemma 8.3. Let Y = Pk
i=1 Xi where each Xi is a ±2 independent
Bernoulli random variable with mean zero. Then for any y and any
ℓ∈N, Pr[Y ∈[2y, 2(y + ℓ)]] ≤ℓ+1
√
k .
Proof. Note that Y is always even and that Pr[Y = 2y] =
 k
(k+y)/2
(1
2)k.
This expression is at most
 k
⌈k/2⌉
(1
2)k. Using Stirling’s approxima-
tion, which says that n! can be approximated by
√
2nπ(n/e)n, this
is bounded by
q
2
πk. The claim follows by a union bound over the ℓ+ 1
possible values for Y in [2y, 2(y + ℓ)].
The adversary’s attack is to choose bn random vectors v ∈{−1, 1}n,
obtain responses (y1, . . . , ybn), and then output any database c such
that |yi −(Ac)i| ≤α for at least 1
2 + η of the indices i, where A is the
bn × n matrix whose rows are the random query vectors v.
Let the true database be d and let c be the reconstructed database.
By assumption on the behavior of the mechanism, |(Ad)i−yi| ≤α for a
1/2+η fraction of i ∈[bn]. Since c was not ruled out, we also have that
|(Ac)i−yi| ≤α for a 1/2+η fraction of i ∈[bn]. Since any two such sets
of indices agree on at least a 2η fraction of i ∈[bn], we have from the
triangle inequality that for at least 2ηbn values of i, |[(c −d)A]i| ≤2α.
We wish to argue that c agrees with d in all but (2α
η )2 entries. We
will show that if the reconstructed c is far from d, disagreeing on at least
(2α/η)2 entries, the probability that a randomly chosen A will satisfy
|[A(c−d)]i| ≤2α for at least 2ηbn values of i will be extremely small —
so small that, for a random A, it is extremely unlikely that there even
exists a c far from d that is not eliminated by the queries in A.
Assume the vector z = (c −d) ∈{−2, 0, 2}n has Hamming weight
at least (2α
η )2, so c is far from d. We have argued that, since c is
produced by the attacker, |(Az)i| ≤2α for at least 2ηbn values of i.
We shall call such a z bad with respect to A. We will show that, with
high probability over the choice of A, no z is bad with respect to A.

8.1. Reconstruction attacks
163
For any i, viz is the sum of at least (2α
η )2 ±2 random values.
Letting k = (2α/η)2 and ℓ= 2α, we have by Lemma 8.3 that the
probability that viz lies in an interval of size 4α is at most η, so
the expected number of queries for which |viz| ≤2α is at most ηbn.
Chernoﬀbounds now imply that the probability that this number
exceeds 2ηbn is at most exp(−ηbn
4 ). Thus the probability of a particular
z = c −d being bad with respect to A is at most exp(−ηbn
4 ).
Taking a union bound over the atmost 3n possible zs, we get that
with probability at least 1−exp(−n(ηb
4 −ln 3)), no bad z exists. Taking
b > 4 ln 3/η, the probability that such a bad z exists is exponentially
small in n.
Preventing blatant non-privacy is a very low bar for a privacy mech-
anism, so if diﬀerential privacy is meaningful then lower bounds for pre-
venting blatant non-privacy will also apply to any mechanism ensuring
diﬀerential privacy. Although for the most part we ignore computa-
tional issues in this monograph, there is also the question of the eﬃ-
ciency of the attack. Suppose we were able to prove that (perhaps under
some computational assumption) there exist low-distortion mechanisms
that are “hard” to break; for example, mechanisms for which producing
a candidate database c close to the original database is hard? Then,
although a low-distortion mechanism might fail to be diﬀerentially pri-
vate in theory, it could conceivably provide privacy against bounded
adversaries. Unfortunately, this is not the case. In particular, when the
noise is always in o(√n), there is an eﬃcient attack using exactly n
ﬁxed queries; moreover, there is even a computationally eﬃcient attack
requiring a linear number of queries in which a 0.239 fraction may be
answered with wild noise.
In the case of “internet scale” data sets, obtaining responses to
n queries is infeasible, as n is extremely large, say, n ≥108. What
happens if the curator permits only a sublinear number of questions?
This inquiry led to the ﬁrst algorithmic results in (what has evolved to
be) (ε, δ)-diﬀerential privacy, in which it was shown how to maintain
privacy against a sublinear number of counting queries by adding bino-
mial noise of order o(√n) — less than the sampling error! — to each
true answer. Using the tools of diﬀerential privacy we can do this either

164
Lower Bounds and Separation Results
using either (1) the Gaussian mechanism or (2) the Laplace mechanism
and advanced composition.
8.2
Lower bounds for diﬀerential privacy
The results of the previous section yielded lower bounds on distortion
needed to ensure any reasonable notion of privacy. In contrast, the
result in this section is speciﬁc to diﬀerential privacy. Although some
of the details in the proof are quite technical, the main idea is elegant:
suppose (somehow) the adversary has narrowed down the set of possible
databases to a relatively small set S of 2s vectors, where the L1 distance
between each pair of vectors is some large number ∆. Suppose further
that we can ﬁnd a k-dimensional query F, 1-Lipschitz in each of its
output coordinates, with the property that the true answers to the
query look very diﬀerent (in L∞norm) on the diﬀerent vectors in our
set; for example, the distance on any two elements in the set may be
Ω(k). It is helpful to think geometrically about the “answer space” Rk.
Each element x in the set S gives rise to a vector F(x) in answer space.
The actual response will be a perturbation of this point in answer
space. Then a volume-based pigeon hole argument (in answer space)
shows that, if with even moderate probability the (noisy) responses are
“reasonably” close to the true answers, then ϵ cannot be very small.
This stems from the fact that for (ε, 0)-diﬀerentially private mech-
anisms M, for arbitrarily diﬀerent databases x, y, any response in the
support of M(x) is also in the support of M(y). Taken together with
the construction of an appropriate collection of vectors and a (con-
trived, non-counting) query, the result yields a lower bound on distor-
tion that is linear k/ε. The argument appeals to Theorem 2.2, which
discusses group privacy. In our case the group in question corresponds
to the indices contributing to the (L1) distance between a pair of vec-
tors in S.
8.2.1
Lower bound by packing arguments
We begin with an observation which says, intuitively, that if the “likely”
response regions, when the query is F, are disjoint, then we can bound

8.2. Lower bounds for diﬀerential privacy
165
ϵ from below, showing that privacy can’t be too good. When ∥F(xi) −
F(xj)∥∞is large, this says that to get very good privacy, even when
restricted to databases that diﬀer in many places, we must get very
erroneous responses on some coordinate of F.
The argument uses the histogram representation of databases. In
the sequel, d = |X| denotes the size of the universe from which database
elements are drawn.
Lemma 8.4. Assume the existence of a set S = {x1, . . . , x2s}, where
each xi ∈Nd, such that for i ̸= j, ∥xi −xj∥1 ≤∆. Further, let F :
Nd →Rk be a k-dimensional query. For 1 ≤i ≤2s, let Bi denote a
region in Rk, the answer space, and assume that the Bi are mutually
disjoint. If M is an (ε, 0)-diﬀerentially private mechanism for F such
that, ∀1 ≤i ≤2s, Pr[M(xi) ∈Bi] ≥1/2, then ε ≥ln(2)(s−1)
∆
.
Proof. By assumption Pr[M(xj) ∈Bj] ≥2−1. Since the regions
B1, . . . , B2s are disjoint, ∃j ̸= i ∈[2s] such that Pr[M(xi) ∈Bj] ≤2−s.
That is, for at least one of the 2s −1 regions Bj, the probability that
M(xi) is mapped to this Bj is at most 2−s. Combining this with dif-
ferential privacy, we have
2−1
2−s ≤PrM[Bj|xj]
PrM[Bj|xi] ≤exp(ε∆).
Corollary 8.5. Let S = {x1, . . . , x2s} be as in Lemma 8.4, and assume
that for any i ̸= j, ∥F(xi)−F(xj)∥∞≥η. Let Bi denote the L∞ball in
Rk of radius η/2 centered at xi. Let M be any ε-diﬀerentially private
mechansim for F satisfying
∀1 ≤i ≤2s : Pr[M(xi) ∈Bi] ≥1/2.
Then ε ≥(ln 2)(s−1)
∆
.
Proof. The regions B1, . . . , B2s are disjoint, so the conditions of
Lemma 8.4 are satisﬁed. The corollary follows by applying the lemma
and taking logarithms.
In Theorem 8.8 below we will look at queries F that are sim-
ply k independently and randomly generated (nonlinear!) queries. For

166
Lower Bounds and Separation Results
suitable S and F (we will work to ﬁnd these) the corollary says that
if with probability at least 1/2 all responses simultaneously have small
error, then privacy can’t be too good. In other words,
Claim 8.6 (Informal Restatement of Corollary 8.5). To obtain (ε, 0)-
diﬀerential privacy for ε ≤ln(2)(s−1)
∆
, the mechanism must add noise
with L∞norm greater than η/2 with probability exceeding 1/2.
As a warm-up exercise, we prove an easier theorem that requires a
large data universe.
Theorem 8.7. Let X = {0, 1}k. Let M : X n →Rk be an (ε, 0)-
diﬀerentially private mechanism such that for every database x ∈X n
with probability at least 1/2 M(x) outputs all of the 1-way marginals
of x with error smaller than n/2. That is, for each j ∈[k], the jth
component of M(x) should approximately equal the number of rows of
x whose jth bit is 1, up to an error smaller than n/2. Then n ∈Ω(k/ε).
Note that this bound is tight to within a constant factor, by the sim-
ple composition theorem, and that it separates (ε, 0)-diﬀerential privacy
from (ε, δ)-diﬀerential privacy, for δ ∈2−o(n), since, by the advanced
composition theorem (Theorem 3.20), Laplace noise with parameter
b =
p
k ln(1/δ)/ε suﬃces for the former, in contrast to Ω(k/ε) needed
for the latter. Taking k ∈Θ(n) and, say, δ = 2−log2 n, yields the sepa-
ration.
Proof. For every string w ∈{0, 1}k, consider the database xw consisting
of n identical rows, all of which equal w. Let Bw ∈Rk consist of all
tuples of numbers that provide answers to the 1-way marginals on x
with error less than n/2. That is,
Bw = {(a1, . . . , ak)} ∈Rk : ∀i ∈[k] |ai −nwi| < n/2}.
Put diﬀerently, Bw is the open ℓ∞of radius n/2 around nw ∈{0, n}k.
Notice that the sets Bw are mutually disjoint.
If M is an accurate mechanism for answering 1-way marginals, then
for every w the probability of landing in Bw when the database is xw
should be at least 1/2: Pr[M(xw) ∈Bw] ≥1/2. Thus, setting ∆= n
and s = k in Corollary 8.5 we have ε ≥(ln 2)(s−1)
∆
.

8.2. Lower bounds for diﬀerential privacy
167
Theorem 8.8. For any k, d, n ∈N and ε ∈(0, 1/40], where n ≥
min{k/ε, d/ε}, there is a query F : Nd →Rk with per-coordinate sen-
sitivity at most 1 such that any (ε, 0)-diﬀerentially private mechanism
adds noise of L∞norm Ω(min{k/ε, d/ε}) with probability at least 1/2
on some databases of weight at most n.
Note that d = |X| need not be large here, in contrast to the require-
ment in Theorem 8.7.
Proof. Let ℓ= min{k, d}. Using error-correcting codes we can con-
struct a set S = {x1, . . . , x2s}, where s = ℓ/400, such that each xi ∈Nd
and in addition
1. ∀i : ∥xi∥1 ≤w = ℓ/(1280ε)
2. ∀i ̸= j, ∥xi −xj∥1 ≥w/10
We do not give details here, but we note that the databases in S are
of size at most w < n, and so ∥xi −xj∥1 ≤2w. Taking ∆= 2w the
set S satisﬁes the conditions of Corollary 8.5. The remainder of our
eﬀort is to obtain the queries F to which we will apply Corollary 8.5.
Given S = {x1, . . . , x2s}, where each xi ∈Nd, the ﬁrst step is to deﬁne a
mapping from the space of histograms to vectors in R2s, LS : Nd →R2s.
Intuitively (and imprecisely!), given a histogram x, the mapping lists,
for each xi ∈S, the L1 distance from x to xi. More precisely, letting w
be an upper bound on the weight of any xi in our collection we deﬁne
the mapping as follows.
• For every xi ∈S, there is a coordinate i in the mapping.
• The ith coordinate of LS(x) is max{w/30 −∥xi −z∥1, 0}.
Claim 8.9. If x1, . . . , x2s satisfy the conditions
1. ∀i∥xi∥1 ≤w; and
2. ∀i ̸= j∥xi −xj∥1 ≥w/10
then the map LS is 1-Lipschitz; in particular, if ∥z1 −z2∥1 = 1, then
∥LS(z1) −LS(z2)∥1 ≤1, assuming w ≥31.

168
Lower Bounds and Separation Results
Proof. Since we assume w ≥31 we have that if z ∈Nd is close to some
xi ∈S, meaning w/30 > ∥xi −z∥1, then z cannot be close to any other
xj ∈S, and the same is true for all ∥z′ −z∥1 ≤1. Thus, for any z1, z2
such that ∥z1 −z2∥≤1, if A denotes the set of coordinates where at
least one of LS(z1) or LS(z2) is non-zero, then A is either empty or is a
singleton set. Given this, the statement in the claim is immediate from
the fact that the mapping corresponding to any particular coordinate
is clearly 1-Lipschitz.
We can ﬁnally describe the queries F. Corresponding to any r ∈
{−1, 1}2s, we deﬁne fr : Nd →R, as
fr(x) =
d
X
i=1
LS(x)i · ri ,
which is simply the inner product LS · r. F will be a random map
F : Nd →Rk: Pick r1, . . . , rk ∈{−1, 1}2s independently and uniformly
at random and deﬁne
F(x) = (fr1(x), . . . , frk(x)) .
That is, F(x) is simply the result of the inner product of LS(x) with k
randomly chosen ±1 vectors.
Note that for any x ∈S LS(x) has one coordinate with value w/30
(and the others are all zero), so ∀ri ∈{−1, 1}2s and x ∈S we have
|fri(x)| = w/30. Now consider any xh, xj ∈S, where h ̸= j. It follows
that for any ri ∈{−1, 1}2s,
Pr
ri [|fri(xh) −fri(xj)| ≥w/15] ≥1/2
(this event occurs when (ri)h = −(ri)j). A basic application of the
Chernoﬀbound implies that
Pr
r1,...,rk[For at least 1/10 of the ris,
|fri(xh) −fri(xj)| ≥w/15] ≥1 −2−k/30 .
Now, the total number of pairs (xi, xj) of databases such that xi, xj ∈S
is at most 22s ≤2k/200. Taking a union bound this implies
Pr
r1,...,rk[∀h ̸= j,
For at least 1/10 of the ris,
|fri(xh) −fri(xj)| ≥w/15] ≥1 −2−k/40

8.2. Lower bounds for diﬀerential privacy
169
This implies that we can ﬁx r1, . . . , rk such that the following is true.
∀h ̸= j,
For at least 1/10 of the ris,
|fri(xh) −fri(xj)| ≥w/15
Thus, for any xh ̸= xj ∈S, ∥F(xh) −F(xj)∥∞≥w/15.
Setting ∆= 2w and s = ℓ/400 > 3εw (as we did above), and
η = w/15, we satisfy the conditions of Corollary 8.5 and conclude
∆≤(s −1)/ε, proving the theorem (via Claim 8.6).
The theorem is almost tight: if k ≤d then we can apply the
Laplace mechanism to each of the k sensitivity 1 component queries
in F with parameter k/ε, and we expect the maximum distortion to be
Θ(k ln k/ε). On the other hand, if d ≤k then we can apply the Laplace
mechanism to the d-dimensional histogram representing the database,
and we expect the maximum distortion to be Θ(d ln d/ε).
The theorem actually shows that, given knowledge of the set S and
knowledge that the actual database is an element x ∈S, the adversary
can completely determine x if the L∞norm of the distortion is too
small. How in real life might the adversary obtain a set S of the type
used in the attack? This can occur when a non-private database system
has been running on a dataset, say, x. For example, x could be a vector
in {0, 1}n and the adversary may have learned, through a sequence of
linear queries, that x ∈C, a linear code of distance, say n2/3. Of course,
if the database system is not promising privacy there is no problem.
The problem arises if the administrator decides to replace the existing
system with a diﬀerentially private mechanism — after several queries
have received noise-free responses. In particular, if the administrator
chooses to use (ε, δ)-diﬀerential privacy for subsequent k queries then
the distortion might fall below the Ω(k/ε) lower bound, permitting the
attack described in the proof of Theorem 8.8.
The theorem also emphasizes that there is a fundamental diﬀerence
between auxiliary information about (sets of) members of the database
and information about the database as a whole. Of course, we already
knew this: being told that the number of secret bits sums to exactly
5, 000 completely destroys diﬀerential privacy, and an adversary that
already knew the secret bit of every member of the database except
one individual could then conclude the secret bit of the remaining
individual.

170
Lower Bounds and Separation Results
Additional Consequences.
Suppose k ≤d, so ℓ= k in Theorem 8.8.
The linear in k/ε lower bound on noise for k queries sketched in
the previous section immediately yields a separation between counting
queries and arbitrary 1-sensitivity queries, as the SmallDB construction
answers (more than) n queries with noise roughly n2/3 while maintain-
ing diﬀerential privacy. Indeed, this result also permits us to conclude
that there is no small α-net for large sets of arbitrary low sensitivity
queries, for α ∈o(n) (as otherwise the net mechanism would yield an
(ε, 0) algorithm of desired accuracy).
8.3
Bibliographic notes
The ﬁrst reconstruction attacks, including Theorem 8.1, are due to
Dinur and Nissim [18], who also gave an attack requiring only poly-
nomial time computation and O(n log2 n) queries, provided the noise
is always o(√n). Realizing that attacks requiring n random linear
queries, when n is “internet scale,” are infeasible, Dinur, Dwork, and
Nissim gave the ﬁrst positive results, showing that for a sublinear
number of subset sum queries, a form of privacy (now known to
imply (ε, δ)-diﬀerential privacy) can be achieved by adding noise scaled
to o(√n) [18]. This was exciting because it suggested that, if we think
of the database as drawn from an underlying population, then, even
for a relatively large number of counting queries, privacy could be
achieved with distortion smaller than the sampling error. This even-
tulaly lead, via more general queries [31, 6], to diﬀerential privacy.
The view of these queries as a privacy-preserving programming prim-
itive [6] inspired McSherry’s Privacy Integrated Queries programming
platform [59].
The reconstruction attack of Theorem 8.2 appears in [24], where
Dwork, McSherry, and Talwar showed that polynomial time recon-
struction is possible even if a 0.239 fraction of the responses have wild,
arbitrary, noise, provided the others have noise o(√n).
The geometric approach, and in particular Lemma 8.4, is due to
Hardt and Talwar [45], who also gave a geometry-based algorithm
proving these bounds tight for small numbers k ≤n of queries, under a

8.3. Bibliographic notes
171
commonly believed conjecture. Dependence on the conjecture was later
removed by Bhaskara et al. [5]. The geometric approach was extended
to arbitrary numbers of queries by Nikolov et al. [66], who gave an algo-
rithm with instance-optimal mean squared error. For the few queries
case this leads, via a boosting argument, to low expected worst-case
error. Theorem 8.8 is due to De [17].

9
Diﬀerential Privacy
and Computational Complexity
Our discussion of diﬀerential privacy has so far ignored issues of compu-
tational complexity, permitting both the curator and the adversary to
be computationally unbounded. In reality, both curator and adversary
may be computationally bounded.
Conﬁning ourselves to a computationally bounded curator restricts
what the curator can do, making it harder to achieve diﬀerential pri-
vacy. And indeed, we will show an example of a class of counting queries
that, under standard complexity theoretic assumptions, does not per-
mit eﬃcient generation of a synthetic database, even though ineﬃcient
algorithms, such as SmallDB and Private Multiplicative Weights, are
known. Very roughly, the database rows are digital signatures, signed
with keys to which the curator does not have access. The intuition will
be that any row in a synthetic database must either be copied from
the original — violating privacy — or must be a signature on a new
message, i.e., a forgery — violating the unforgeability property of a
digital signature scheme. Unfortuately, this state of aﬀairs is not lim-
ited to (potentially contrived) examples based on digital signatures: it
is even diﬃcult to create a synthetic database that maintains relatively
172

173
accurate two-way marginals.1 On the positive side, given a set Q of
queries and an n-row database with rows drawn from a universe X,
a synthetic database can be generated in time polynomial in n, |X|,
and |Q|.
If we abandon the goal of a synthetic database and content ourselves
with a data structure from which we can obtain a relatively accurate
approximation to the answer to each query, the situation is much more
interesting. It turns out that the problem is intimately related to the
tracing traitors problem, in which the goal is to discourage piracy while
distributing digital content to paying customers.
If the adversary is restricted to polynomial time, then it becomes
easier to achieve diﬀerential privacy. In fact, the immensely power-
ful concept of secure function evaluation yields a natural way avoid
the trusted curator (while giving better accuracy than randomized
response), as well as a natural way to allow multiple trusted cura-
tors, who for legal reasons cannot share their data sets, to respond to
queries on what is eﬀectively a merged data set. Brieﬂy put, secure
function evaluation is a cryptographic primitive that permits a collec-
tion of n parties p1, p2, . . . , pn, of which fewer than some ﬁxed frac-
tion are faulty (the fraction varies according to the type of faults; for
“honest-but-curious” faults the fraction is 1), to cooperatively compute
any function f(x1, . . . , xn), where xi is the input, or value, of party pi,
in such a way that no coalition of faulty parties can either disrupt the
computation or learn more about the values of the non-faulty parties
than can be deduced from the function output and the values of the
members of the coalition. These two properties are traditionally called
correctness and privacy. This privacy notion, let us call it SFE pri-
vacy, is very diﬀerent from diﬀerential privacy. Let V be the set of
values held by the faulty parties, and let pi be a non-faulty party.2
SFE privacy permits the faulty parties to learn xi if xi can be deduced
from V ∪{f(x1, . . . , xn)}; diﬀerential privacy would therefore not per-
mit exact release of f(x1, . . . , xn). However, secure function evaluation
1Recall that the two-way marginals are the counts, for every pair of attribute
values, of the number of rows in the database having this pair of values.
2In the honest but curious case we can let V = {xj} for any party Pj.

174
Diﬀerential Privacy and Computational Complexity
protocols for computing a function f can easily be modiﬁed to obtain
diﬀerentially private protocols for f, simply by deﬁning a new function,
g, to be the result of adding Laplace noise Lap(∆f/ε) to the value of f.
In principle, secure function evaluation permits evaluation of g. Since
g is diﬀerentially private and the SFE privacy property, applied to g,
says that nothing can be learned about the inputs that is not learnable
from the value of g(x1, . . . , xn) together with V , diﬀerential privacy is
ensured, provided the faulty players are restricted to polynomial time.
Thus, secure function evaluation allows a computational notion of dif-
ferential privacy to be achieved, even without a trusted curator, at no
loss in accuracy when compared to what can be achieved with a trusted
curator. In particular, counting queries can be answered with constant
expected error while ensuring computational diﬀerential privacy, with
no trusted curator. We will see that, without cryptography, the error
must be Ω(n1/2), proving that computational assumptions provably
buy accuracy, in the multiparty case.
9.1
Polynomial time curators
In this section we show that, under standard cryptographic assump-
tions, it is computationally diﬃcult to create a synthetic database that
will yield accurate answers to an appropriately chosen class of counting
queries, while ensuring even a minimal notion of privacy.
This result has several extensions; for example, to the case in which
the set of queries is small (but the data universe remains large), and the
case in which the data universe is small (but the set of queries is large).
In addition, similar negative results have been obtained for certain
natural families of queries, such as those corresponding to conjunctions.
We will use the term syntheticize to denote the process of generating
a synthetic database in a privacy-preserving fashion3. Thus, the results
in this section concern the computational hardness of syntheticizing.
Our notion of privacy will be far weaker than diﬀerential privacy, so
hardness of syntheticizing will imply hardness of generating a synthetic
3In Section 6 a syntheticizer took as input a synopsis; here we are starting with
a database, which is a trivial synopsis.

9.1. Polynomial time curators
175
database in a diﬀerentially private fashion. Speciﬁcally, we will say that
syntheticizing is hard if it is hard even to avoid leaking input items in
their entirety. That is, some item is always completely exposed.
Note that if, in contrast, leaking a few input items is not considered
a privacy breach, then syntheticizing is easily achieved by releasing a
randomly chosen subset of the input items. Utility for this “synthetic
database” comes from sampling bounds: with high probability this sub-
set will preserve utility even with respect to a large set of counting
queries.
When introducing complexity assumptions, we require a security
parameter in order to express sizes; for example, sizes of sets, lengths
of messages, number of bits in a decryption key, and so on, as well as
to express computational diﬃculty. The security parameter, denoted
κ, represents “reasonable” sizes and eﬀort. For example, it is assumed
that it is feasible to exhaustively search a set whose size is (any ﬁxed)
polynomial in the security parameter.
Computational complexity is an asymptotic notion — we are con-
cerned with how the diﬃculty of a task increases as the sizes of the
objects (data universe, database, query family) grow. Thus, for exam-
ple, we therefore need to think not just of a distribution on databases
of a single size (what we have been calling n in the rest of this mono-
graph), but of an ensemble of distributions, indexed by the security
parameter. In a related vein, when we introduce complexity we tend
to “soften” claims: forging a signature is not impossible — one might
be lucky! Rather, we assume that no eﬃcient algorithm succeeds with
non-negligible probability, where “eﬃcient” and “non-negligible” are
deﬁned in terms of the security parameter. We will ignore these ﬁne
points in our intuitive discussion, but will keep them in the formal
theorem statements.
Speaking informally, a distribution of databases is hard to syn-
theticize (with respect to some family Q of queries) if for any eﬃ-
cient (alleged) syntheticizer, with high probability over a database
drawn from the distribution, at least one of the database items can be
extracted from the alleged syntheticizer’s output. Of course, to avoid
triviality, we will also require that when this leaked item is excluded
from the input database (and, say, replaced by a random diﬀerent item),

176
Diﬀerential Privacy and Computational Complexity
the probability that it can be extracted from the output is very small.
This means that any eﬃcient (alleged) syntheticizer indeed compro-
mises the privacy of input items in a strong sense.
Deﬁnition 9.1 below will formalize our utility requirements for a
syntheticizer. There are three parameters: α describes the accuracy
requirement (being within α is considered accurate); γ describes the
fraction of the queries on which a successful synthesis is allowed to be
inaccurate, and β will be the probability of failure.
For an algorithm A producing synthetic databases, we say that an
output A(x) is (α, γ)-accurate for a query set Q if |q(A(x))−q(x)| ≤α
for a 1 −γ fraction of the queries q ∈Q.
Deﬁnition 9.1 ((α, β, γ)-Utility). Let Q be a set of queries and X a data
universe. A syntheticizer A has (α, β, γ)-utility for n-item databases
with respect to Q and X if for any n-item database x:
Pr [A(x) is (α, γ)-accurate for Q] ≥1 −β
where the probability is over the coins of A.
Let Q = {Qn}n=1,2,... be a query family ensemble, X = {Xn}n=1,2,...
be a data universe ensemble. An algorithm is said to be eﬃcient if its
running time is poly(n, log(|Qn|), log(|Xn|)).
In the next deﬁnition we describe what it means for a family of
distributions to be hard to syntheticize. A little more speciﬁcally we
will say what it means to be hard to generate synthetic databases that
provide (α, γ)-accuracy. As usual, we have to make this an asymptotic
statement.
Deﬁnition 9.2 ((µ, α, β, γ, Q)-Hard-to-Syntheticize Database Distribu-
tion). Let Q
=
{Qn}n=1,2,... be a query family ensemble, X
=
{Xn}n=1,2,... be a data universe ensemble, and let µ, α, β, γ ∈[0, 1].
Let n be a database size and D an ensemble of distributions, where Dn
is over collections of n + 1 items from Xn.
We denote by (x, i, x′
i) ∼Dn the experiment of choosing an n-
element database, an index i chosen uniformly from [n], and an addi-
tional element x′
i from Xn. A sample from Dn gives us a pair of
databases: x and the result of replacing the ith element of x (under

9.2. Some hard-to-Syntheticize distributions
177
a canonical ordering) with x′
i. Thus, we think of Dn as specifying a
distribution on n-item databases (and their neighbors).
We say that D is (µ, α, β, γ, Q)-hard-to Syntheticize if there exists an
eﬃcient algorithm T such that for any alleged eﬃcient syntheticizer A
the following two conditions hold:
1. With probability 1−µ over the choice of database x ∼D and the
coins of A and T, if A(x) maintains α-utility for a 1 −γ fraction
of queries, then T can recover one of the rows of x from A(x):
Pr
(x,i,x′
i)∼Dn
coin ﬂips of A,T
[(A(x) maintains (α, β, γ)-utility) and (x ∩T(A(x)) = ∅)] ≤µ
2. For every eﬃcient algorithm A, and for every i ∈[n], if we draw
(x, i, x′
i) from D, and replace xi with x′
i to form x′, T cannot
extract xi from A(x′) except with small probability:
Pr
(x,i,x′
i)∼Dn
coin ﬂips of A, T
[xi ∈T(A(x′))] ≤µ.
Later, we will be interested in oﬄine mechanisms that produce arbi-
trary synopses, not necessarily synthetic databases. In this case we will
be interested in the related notion of hard to sanitize (rather than hard
to Syntheticize), for which we simply drop the requirement that A pro-
duce a synthetic database.
9.2
Some hard-to-Syntheticize distributions
We now construct three distributions that are hard to syntheticize.
A signature scheme is given by a triple of (possibly randomized)
algorithms (Gen, Sign, Verify):
• Gen : 1N →{(SK, VK)n}n=1,2,... is used to generate a pair con-
sisting of a (secret) signing key and a (public) veriﬁcation key.
It takes only the security parameter κ ∈N, written in unary,
as input, and produces a pair drawn from (SK, VK)κ, the distri-
bution on (signature,veriﬁcation) key pairs indexed by κ; we let

178
Diﬀerential Privacy and Computational Complexity
ps(κ), pv(κ), ℓs(κ) denote the lengths of the signing key, veriﬁca-
tion key, and signature, respectively.
• Sign : SKκ × {0, 1}ℓ(κ) →{0, 1}ℓs(κ) takes as input a signing key
from a pair drawn from (SK, VK)κ and a message m of length
ℓ(κ), and produces a signature on m;
• Verify : VKκ × {0, 1}∗× {0, 1}ℓ(κ) →{0, 1} takes as input a
veriﬁcation key, a string σ, and a message m of length ℓ(κ), and
checks that σ is indeed a valid signature of m under the given
veriﬁcation key.
Keys, message lengths, and signature lengths are all polynomial in κ.
The notion of security required is that, given any polynomial (in κ)
number of valid (message, signature) pairs, it is hard to forge any new
signature, even a new signature of a previously signed message (recall
that the signing algorithm may be randomized, so there may exist mul-
tiple valid signatures of the same message under the same signing key).
Such a signature scheme can be constructed from any one-way function.
Speaking informally, these are functions that are easy to compute —
f(x) can be computed in time polynomial in the length (number of
bits) of x, but hard to invert: for every probabilistic polynomial time
algorithm, running in time polyomial in the security parameter κ, the
probability, over a randomly chosen x in the domain of f, of ﬁnding
any valid pre-image of f(x), grows more slowly than the inverse of any
polynomial in κ.
Hard to Syntheticize Distribution I:
Fix an arbitrary signature
scheme. The set Qκ of counting queries contains one counting query qvk
for each veriﬁcation key vk ∈VKκ. The data universe Xκ consists of the
set of all possible (message, signature) pairs of the form for messages
of length ℓ(κ) signed with keys in VKκ.
The distribution Dκ on databases is deﬁned by the following sam-
pling procedure. Run the signature scheme generator Gen(1κ) to obtain
(sk, vk). Randomly choose n = κ messages in {0, 1}ℓ(κ) and run the
signing procedure for each one, obtaining a set of n (message, signa-
ture) pairs all signed with key sk. This is the database x. Note that all
the messages in the database are signed with the same signing key.

9.2. Some hard-to-Syntheticize distributions
179
A data universe item (m, σ) satisﬁes the predicate qvk if and only
if Verify(vk, m, σ) = 1, i.e., σ is a valid signature for m according to
veriﬁcation key vk.
Let x ∈R Dκ be a database, and let sk be the signing key used, with
corresponding veriﬁcation key vk. Assuming that the syntheticizer has
produced y, it must be the case that almost all rows of y are valid
signatures under vk (because the fractional count of x for the query
vk is 1). By the unforgeability properties of the signature scheme, all
of these must come from the input database x — the polynomial time
bounded curator, running in time poly(κ), cannot generate generate a
new valid (message, signature) pair. (Only slightly) more formally, the
probability that an eﬃcient algorithm could produce a (message, sig-
nature) pair that is veriﬁable with key vk, but is not in x, is negligible,
so with overwhelming probability any y that is produced by an eﬃ-
cient syntheticizer will only contain rows of x.4 This contradicts (any
reasonable notion of) privacy.
In this construction, both Qκ (the set of veriﬁcation keys) and Xκ
(the set of (message, signature) pairs) are large (superpolynomial in κ).
When both sets are small, eﬃcient diﬀerentially private generation of
synthetic datasets is possible. That is, there is a diﬀerentially private
syntheticizer whose running time is polynomial in n = κ, |Qκ| and |Xκ|:
compute noisy counts using the Laplace mechanism to obtain a synopsis
and then run the syntheticizer from Section 6. Thus, when both of
these have size polynomial in κ the running time of the syntheticizer
is polynomial in κ.
We now brieﬂy discuss generalizations of the ﬁrst hardness result
to the cases in which one of these sets is small (but the other remains
large).
Hard to Syntheticize Distribution II:
In the database distribution
above, we chose a single (sk, vk) key pair and generated a database of
4The quantiﬁcation order is important, as otherwise the syntheticizer could have
the signing key hardwired in. We ﬁrst ﬁx the syntheticizer, then run the generator
and build the database. The probability is over all the randomness in the experi-
ment: choice of key pair, construction of the database, and randomness used by the
syntheticizer.

180
Diﬀerential Privacy and Computational Complexity
messages, all signed using sk; hardness was obtained by requiring the
syntheticizer to generate a new signature under sk, in order for the
syntheticized database to provide an accurate answer to the query qvk.
To obtain hardness for syntheticizing when the size of the set of queries
is only polynomial in the security parameter, we again use digital
signatures, signed with a unique key, but we cannot aﬀord to have a
query for each possible veriﬁcation key vk, as these are too numerous.
To address this, we make two changes:
1. Database rows now have the form (veriﬁcation key, mes-
sage, signature). more precisely, the data universe consists of
(key,message,signature) triples X = {(vk, m, s) : vk ∈VKκ, m ∈
{0, 1}ℓ(κ), s ∈{0, 1}ℓs(κ)}.
2. We add to the query class exactly 2pv(κ) queries, where pv(κ)
is the length of the veriﬁcation keys produced by running the
generation algorithm Gen(1κ). The queries have the form (i, b)
where 1 ≤i ≤pv(κ) and b ∈{0, 1}. The meaning of the query
“(i, b)” is, “What fraction of the database rows are of the form
(vk, m, s) where Verify(vk, m, s) = 1 and the ith bit of vk is b?”
By populating a database with messages signed according to a
single key vk, we ensure that the responses to these queries should
be close to one for all 1 ≤i ≤p(κ) when vki = b, and close to
zero when vki = 1 −b.
With this in mind, the hard to syntheticize distribution on
databases is constructed by the following sampling procedure: Gen-
erate a signature-veriﬁcation key pair (sk, vk) ←Gen(1κ), and choose
n = κ messages m1, . . . , mn uniformly from {0, 1}ℓ(κ). The database x
will have n, rows; for j ∈[n] the jth row is the veriﬁcation key, the jth
message and its valid signature, i.e., the tuple (vk, mj, Sign(mj, sk)).
Next, choose i uniformly from [n]. To generate the (n + 1)st item x′
i,
just generate a new message-signature pair (using the same key sk).
Hard to Syntheticize Distribution III:
To prove hardness for the case
of a polynomial (in κ) sized message space (but superpolynomial sized
query set) we use a pseudorandom function. Roughly speaking, these
are polynomial time computable functions with small descriptions that

9.2. Some hard-to-Syntheticize distributions
181
cannot eﬃciently be distinguished, based only on their input-output
behavior, from truly random functions (whose descriptions are long).
This result only gives hardness of syntheticizing if we insist on main-
taining utility for all queries. Indeed, if we are interested only in ensur-
ing on-average utility, then the base generator for counting queries
described in Section 6 yields an eﬃcient algorithm for syntheticizing
when the universe X is of polynomial size, even when Q is exponentially
large.
Let {fs}s∈{0,1}κ be a family of pseudo-random functions from [ℓ] to
[ℓ], where ℓ∈poly(κ). More speciﬁcally, we need that the set of all
pairs of elements in [ℓ] is “small,” but larger than κ; this way the κ-bit
string describing a function in the family is shorter than the ℓlog2 ℓ
bits needed to describe a random function mapping [ℓ] to [ℓ]. Such a
family of pseudorandom functions can be constructed from any one-way
function.
Our data universe will be the set of all pairs of elements in [ℓ]:
X = {(a, b) : a, b ∈[ℓ]}. Qκ will contain two types of queries:
1. There will be one query for each function {fs}s∈{0,1}κ in the fam-
ily. A universe element (a, b) ∈X satisﬁes the query s if and only
if fs(a) = b.
2. There will be a relatively small number, say κ, truly random
queries. Such a query can be constructed by randomly choosing,
for each (a, b) ∈X, whether or not (a, b) will satisfy the query.
The hard to syntheticize distribution is generated as follows. First,
we select a random string s ∈{0, 1}κ, specifying a function in our
family. Next, we generate, for n = κ distinct values a1, . . . , an chosen at
random from [ℓ] without replacement, the universe element (a, fs(a)).
The intuition is simple, relies only on the ﬁrst type of query, and
does not make use of the distinctness of the ai. Given a database x gen-
erated according to our distribution, where the pseudo-random func-
tion is given by s, the syntheticizer must create a synthetic database
(almost) all of whose rows must satisfy the query s. The intuition is
that it can’t reliably ﬁnd input-output pairs that do not appear in x.
A little more precisely, for an arbitrary element a ∈[ℓ] such that no

182
Diﬀerential Privacy and Computational Complexity
row in x is of the form (a, fs(a)), the pseudo-randomness of fs says
that an eﬃcient syntheticizer should have probability at most negligi-
bly more than 1/ℓof ﬁnding fs(a). In this sense the pseudo-randomness
gives us properties similar to, although somewhat weaker than, what
we obtained from digital signatures.
Of course, for any given a ∈[ℓ], the syntheticizer can indeed guess
with probability 1/ℓthe value fs(a), so without the second type of
query, nothing obvious would stop it from ignoring x, choosing an
arbitrary a, and outputting a database of n copies of (a, b), where b
is chosen uniformly at random from [ℓ]. The intuition is now that such
a synthetic database would give the wrong fraction — either zero or
one, when the right answer should be about 1/2 — on the truly random
queries.
Formally, we have:
Theorem 9.1. Let f : {0, 1}κ →{0, 1}κ be a one-way function. For
every a > 0, and for every integer n = poly(κ), there exists a query
family Q of size exp(poly(κ)), a data universe X of size O(n2+2a),
and a distribution on databases of size n that is (µ, α, β, 0, Q)-hard-to-
syntheticize (i.e., hard to syntheticize for worst-case queries) for α ≤
1/3, β ≤1/10 and µ = 1/40n1+a.
The above theorem shows hardness of sanitizing with synthetic
data. Note, however, that when the query set is small one can always
simply release noisy counts for every query. We conclude that sanitiz-
ing for small query classes (with large data universes) is a task that
separates eﬃcient syntheticizing from eﬃcient synopsis generation (san-
itization with arbitrary outputs).
9.2.1
Hardness results for general synopses
The hardness results of the previous section apply only to syntheticiz-
ers — oﬄine mechanisms that create synthetic databases. There is a
tight connection between hardness for more general forms of privacy-
preserving oﬄine mechanisms, which we have been calling oﬄine query
release mechanisms or synopsis generators, and the existence of traitor
tracing schemes, a method of content distribution in which (short) key

9.2. Some hard-to-Syntheticize distributions
183
strings are distributed to subscribers in such a way that a sender can
broadcast encrypted messages that can be decrypted by any subscriber,
and any useful “pirate” decoder constructed by a coalition of malicious
subscribers can be traced to at least one colluder.
A (private-key, stateless) traitor-tracing scheme consists of algo-
rithms Setup, Encrypt, Decrypt and Trace. The Setup algorithm gen-
erates a key bk for the broadcaster and N subscriber keys k1, . . . , kN.
The Encrypt algorithm encrypts a given bit using the broadcaster’s
key bk. The Decrypt algorithm decrypts a given ciphertext using any
of the subscriber keys. The Trace algorithm gets the key bk and oracle
access to a (pirate, stateless) decryption box, and outputs the index
i ∈{1, . . . , N} of a key ki that was used to create the pirate box.
An important parameter of a traitor-tracing scheme is its collusion-
resistance: a scheme is t-resilient if tracing is guaranteed to work as long
as no more than t keys are used to create the pirate decoder. When
t = N, tracing works even if all the subscribers join forces to try and
create a pirate decoder. A more complete deﬁnition follows.
Deﬁnition 9.3. A scheme (Setup, Encrypt, Decrypt, Trace) as above is
a t-resilient traitor-tracing scheme if (i) the ciphertexts it generates
are semantically secure (roughly speaking, polynomial time algorithms
cannot distinguish encryptions of 0 from encryptions of 1), and (ii) no
polynomial time adversary A can “win” in the following game with
non-negligible probability (over the coins of Setup, A, and Trace):
A receives the number of users N and a security parameter κ and
(adaptively) requests the keys of up to t users {i1, . . . , it}. The adver-
sary then outputs a pirate decoder Dec. The Trace algorithm is run
with the key bk and black-box access5 to Dec; it outputs the name
i ∈[N] of a user or the error symbol ⊥. We say that an adversary A
“wins” if it is both the case that Dec has a non-negligible advantage in
decrypting ciphertexts (even a weaker condition than creating a usable
pirate decryption device), and the output of Trace is not in {i1, . . . , it},
meaning that the adversary avoided detection.
5Black-box access to an algorithm means that one has no access to the algorithm’s
internals; one can only feed inputs to the algorithm and observe its outputs.

184
Diﬀerential Privacy and Computational Complexity
The intuition for why traitor-tracing schemes imply hardness results
for counting query release is as follows. Fix a traitor tracing scheme. We
must describe databases and counting queries for which query release
is computationally hard.
For any given n = κ, the database x ∈{{0, 1}d}n will contain user
keys from the traitor tracing scheme of a colluding set of n users; here d
is the length of the decryption keys obtained when the Setup algorithm
is run on input 1κ. The query family Qκ will have a query qc for each
possible ciphertext c asking “For what fraction of the rows i ∈[n] does
c decrypt to 1 under the key in row i?” Note that, since every user
can decrypt, if the sender distributes an encryption c of the bit 1, the
answer will be 1: all the rows decrypt c to 1, so the fraction of such
rows is 1. If instead the sender distributes an encryption c′ of the bit 0,
the answer will be 0: since no row decrypts c′ to 1, the fraction of rows
decrypting c′ to 1 is 0. Thus, the exact answer to a query qc, where c
is an encryption of a 1-bit messages b, is b itself.
Now, suppose there were an eﬃcient oﬄine diﬀerentially private
query release mechanism for queries in Q. The colluders could use this
algorithm to eﬃciently produce a synopsis of the database enabling
a data analyst to eﬃciently compute approximate answers to the
queries qc. If these approximations are at all non-trivial, then the ana-
lyst can use these to correctly decrypt. That is, the colluders could use
this to form a pirate decoder box. But traitor tracing ensures that, for
any such box, the Trace algorithm can recover the key of at least one
user, i.e., a row of the database. This violates diﬀerential privacy, con-
tradicting the assumption that there is an eﬃcient diﬀerentially private
algorithm for releasing Q.
This direction has been used to rule out the existence of eﬃcient
oﬄine sanitizers for a particular class of 2 ˜O(√n) counting queries; this
can be extended to rule out the existence of eﬃcient on-line sanitiz-
ers answering ˜Θ(n2) counting queries drawn adaptively from a second
(large) class.
The intuition for why hardness of oﬄine query release for counting
queries implies traitor tracing is that failure to protect privacy immedi-
ately yields some form of traceability; that is, the diﬃculty of providing
an object that yields (approximate) functional equivalence for a set of

9.3. Polynomial time adversaries
185
rows (decryption keys) while preserving privacy of each individual row
(decryption key) — that is, the diﬃculty of producing an untraceable
decoder — is precisely what we are looking for in a traitor tracing
scheme.
In a little more detail, given a hard-to-sanitize database distribu-
tion and family of counting queries, a randomly drawn n-item database
can act like a “master key,” where the secret used to decrypt messages
is the counts of random queries on this database. For a randomly cho-
sen subset S of polylog(n) queries, a random set of polylog(n) rows
drawn from the database (very likely) yields good approximation to
all queries in S. Thus, individual user keys can be obtained by ran-
domly partitioning the database into n/polylog(n) sets of polylog(n)
rows and assigning each set to a diﬀerent user. These sets are large
enough that with overwhelming probability their counts on a random
collection of say polylog(n) queries are all close to the counts of the
original database.
To complete the argument, one designs an encryption scheme in
which decryption is equivalent to computing approximate counts on
small sets of random queries. Since by deﬁnition a pirate decryption
box can decrypt, the a pirate box can be used to compute approximate
counts. If we view this box as a sanitization of the database we conclude
(because sanitizing is hard) that the decryption box can be “traced”
to the keys (database items) that were used to create it.
9.3
Polynomial time adversaries
Deﬁnition 9.4 (Computational Diﬀerential Privacy). A randomized algo-
rithm Cκ : X n →Y is ε-computationally diﬀerentially private if and
only if for all databases x, y diﬀering in a single row, and for all nonuni-
form polynomial (in κ) algorithms T,
Pr[T(Cκ(x)) = 1] ≤eε Pr[T(Cκ(y)) = 1] + ν(κ),
where ν(·) is any function that grows more slowly than the inverse of
any polynomial and the agorithm Cκ runs in time polynomial in n,
log |X|, and κ.

186
Diﬀerential Privacy and Computational Complexity
Intuitively, this says that if the adversary is restricted to polyno-
mial time then computationally diﬀerentially private mechanisms pro-
vide the same degree of privacy as do (ε, ν(κ))-diﬀerentially private
algorithms. In general there is no hope of getting rid of the ν(κ) term;
for example, when encryption is involved there is always some (neglibly
small) chance of guessing the decryption key.
Once we assume the adversary is restricted to polynomial time,
we can use the powerful techniques of secure multiparty computation
to provide distributed online query release algorithms, replacing the
trusted server with a distributed protocoal that simulates a trusted
curator. Thus, for example, a set of hospitals, each holding the data
of many patients, can collaboratively carry out statistical analyses of
the union of their patients, while ensuring diﬀerential privacy for each
patient. A more radical implication is that individuals can maintain
their own data, opting in or out of each speciﬁc statistical query or
study, all the while ensuring diﬀerential privacy of their own data.
We have already seen one distributed solution, at least for the prob-
lem of computing a sum of n bits: randomized response. This solution
requires no computational assumptions, and has an expected error of
Θ(√n). In contrast, the use of cryptographic assumptions permits much
more accurate and extensive analyses, since by simulating the curator it
can run a distributed implementation of the Laplace mechanism, which
has constant expected error.
This leads to the natural question of whether there is some other
approach, not relying on cryptographic assumptions, that yields better
accuracy in the distributed setting than does randomized response. Or
more generally, is there a separation between what can be accomplished
with computational diﬀerential privacy and what can be achieved with
“traditional” diﬀerential privacy? That is, does cryptography provably
buy us something?
In the multiparty setting the answer is yes. Still conﬁning our atten-
tion to summing n bits, we have:
Theorem 9.2. For ε < 1, every n-party (ε, 0)-diﬀerentially private pro-
tocol for computing the sum of n bits (one per party) incurs error
Ω(n1/2) with high probability.

9.4. Bibliographic notes
187
A similar theorem holds for (ε, δ)-diﬀerential privacy provided δ ∈
o(1/n).
Proof. (sketch) Let X1, . . . , Xn be uniform independent bits. The tran-
script T of the protocol is a random variable T = T(P1(X1), . . . ,
Pn(Xn), where for i ∈[n] the protocol of player i is denoted Pi. Con-
ditioned on T = t, the bits X1, . . . , Xn are still independent bits, each
with bias O(ε). Further, by diﬀerential privacy, the uniformity of the
Xi, and Bayes’ Law we have:
Pr[Xi = 1|T = t]
Pr[Xi = 0|T = t] = Pr[T = t|Xi = 1]
Pr[T = t|Xi = 0] ≤eε < 1 + 2ε.
To ﬁnish the proof we note that the sum of n independent bits,
each with constant bias, falls outside any interval of size o(√n) with
high probability. Thus, with high probability, the sum P
i Xi is not in
the interval [output(T) −o(n1/2), output(T) + o(n1/2)].
A more involved proof shows a separation between computational
diﬀerential privacy and ordinary diﬀerential privacy even for the two-
party case. It is a fascinating open question whether computational
assumptions buy us anything in the case of the trusted curator. Initial
results are negative: for small numbers of real-valued queries, i.e., for
a number of queries that does not grow with the security parameter,
there is a natural class of utility measures, including Lp distances and
mean-squared errors, for which any computationally private mechanism
can be converted to a statistically private mechanism that is roughly
as eﬃcient and achieves almost the same utility.
9.4
Bibliographic notes
The negative results for polynomial time bounded curators and the
connection to traitor tracing are due to Dwork et al. [28]. The con-
nection to traitor tracing was further investigated by Ullman [82], who
showed that, assuming the existence of 1-way functions, it is computa-
tionally hard to answer n2+o(1) arbitrary linear queries with diﬀerential
privacy (even if without privacy the answers are easy to compute). In
“Our Data, Ourselves,” Dwork, Kenthapadi, McSherry, Mironov, and

188
Diﬀerential Privacy and Computational Complexity
Naor considered a distributed version of the precursor of diﬀerential
privacy, using techniques from secure function evaluation in place of
the trusted curator [21]. A formal study of computational diﬀerential
privacy was initiated in [64], and the separation between the accuracy
that can be achieved with (ε, 0)-diﬀerential privacy in the multiparty
and single curator cases in Theorem 9.2 is due to McGregor et al. [58].
The initial results regarding whether computational assumptions on
the adversary buys anything in the case of a trusted curator are due
to Groce et al. [37].
Construction of pseudorandom functions from any one-way function
is due to Håstad et al. [40].

10
Diﬀerential Privacy and Mechanism Design
One of the most fascinating areas of game theory is mechanism design,
which is the science of designing incentives to get people to do what you
want them to do. Diﬀerential privacy has proven to have interesting
connections to mechanism design in a couple of unexpected ways. It
provides a tool to quantify and control privacy loss, which is important
if the people the mechanism designer is attempting to manipulate care
about privacy. However, it also provides a way to limit the sensitivity
of the outcome of a mechanism to the choices of any single person,
which turns out to be a powerful tool even in the absence of privacy
concerns. In this section, we give a brief survey of some of these ideas.
Mechanism Design is the problem of algorithm design when the
inputs to the algorithm are controlled by individual, self-interested
agents, rather than the algorithm designer himself. The algorithm maps
its reported inputs to some outcome, over which the agents have pref-
erences. The diﬃculty is that the agents may mis-report their data if
doing so will cause the algorithm to output a diﬀerent, preferred out-
come, and so the mechanism designer must design the algorithm so
that the agents are always incentivized to report their true data.
189

190
Diﬀerential Privacy and Mechanism Design
The concerns of mechanism design are very similar to the concerns
of private algorithm design. In both cases, the inputs to the algorithm
are thought of as belonging to some third party1 which has preferences
over the outcome. In mechanism design, we typically think of individ-
uals as getting some explicit value from the outcomes of the mecha-
nism. In private algorithm design, we typically think of the individual
as experiencing some explicit harm from (consequences of) outcomes
of the mechanism. Indeed, we can give a utility-theoretic deﬁnition of
diﬀerential privacy which is equivalent to the standard deﬁnition, but
makes the connection to individual utilities explicit:
Deﬁnition 10.1. An algorithm A : N|X| →R is ϵ-diﬀerentially private
if for every function f : R →R+, and for every pair of neighboring
databases x, y ∈N|X|:
exp(−ϵ)Ez∼A(y)[f(z)] ≤Ez∼A(x)[f(z)] ≤exp(ϵ)Ez∼A(y)[f(z)].
We can think of f as being some function mapping outcomes to an
arbitrary agent’s utility for those outcomes. With this interpretation,
a mechanism is ϵ-diﬀerentially private, if for every agent it promises
that their participation in the mechanism cannot aﬀect their expected
future utility by more than a factor of exp(ϵ) independent of what their
utility function might be.
Let us now give a brief deﬁnition of a problem in mechanism design.
A mechanism design problem is deﬁned by several objects. There are n
agents i ∈[n], and a set of outcomes O. Each agent has a type, ti ∈T
which is known only to her, and there is a utility function over outcomes
u : T ×O →[0, 1]. The utility that agent i gets from an outcome o ∈O
is u(ti, o), which we will often abbreviate as ui(o). We will write t ∈T n
to denote vectors of all n agent types, with ti denoting the type of agent
i, and t−i ≡(t1, . . . , ti−1, ti+1, . . . , tn) denoting the vector of types of
all agents except agent i. The type of an agent i completely speciﬁes
her utility over outcomes — that is, two agents i ̸= j such that ti = tj
will evaluate each outcome identically: ui(o) = uj(o) for all o ∈O.
1In the privacy setting, the database administrator (such as a hospital) might
already have access to the data itself, but is nevertheless acting so as to protect the
interests of the agents who own the data when it endeavors to protect privacy.

10.1. Diﬀerential privacy as a solution concept
191
A mechanism M takes as input a set of reported types, one from each
player, and selects an outcome. That is, a mechanism is a mapping
M : T n →O. Agents will choose to report their types strategically
so as to optimize their utility, possibly taking into account what (they
think) the other agents will be doing. In particular, they need not report
their true types to the mechanism. If an agent is always incentivized to
report some type, no matter what her opponents are reporting, then
reporting that type is called a dominant strategy. If reporting one’s
true type is a dominant strategy for every agent, then the mechanism
is called truthful, or equivalently, dominant strategy truthful.
Deﬁnition 10.2. Given a mechanism M : T n →O, truthful reporting
is an ϵ-approximate dominant strategy for player i if for every pair of
types ti, t′
i ∈T, and for every vector of types t−i:
u(ti, M(ti, t−i)) ≥u(ti, M(t′
i, t−i)) −ϵ.
If truthful reporting is an ϵ-approximate dominant strategy for every
player, we say that M is ϵ-approximately dominant strategy truthful.
If ϵ = 0, then M is exactly truthful.
That is, a mechanism is truthful if no agent can improve her utility
by misrepresenting her type, no matter what the other players report.
Here we can immediately observe a syntactic connection to the
deﬁnition of diﬀerential privacy. We may identify the type space T
with the data universe X. The input to the mechanism therefore con-
sists of a database of size n, consisting of the reports of each agent.
In fact, when an agent is considering whether she should truthfully
report her type ti or lie, and misreport her type as t′
i, she is deciding
which of two databases the mechanism should receive: (t1, . . . , tn), or
(t1, . . . , ti−1, t′
i, ti+1, . . . , tn). Note that these two databases diﬀer only
in the report of agent i! That is, they are neighboring databases. Thus,
diﬀerential privacy gives a guarantee of approximate truthfulness!
10.1
Diﬀerential privacy as a solution concept
One of the starting points for investigating the connection between dif-
ferential privacy and game theory is observing that diﬀerential privacy

192
Diﬀerential Privacy and Mechanism Design
is a stronger condition than approximate truthfulness. Note that for
ϵ ≤1, exp(ϵ) ≤1 + 2ϵ and so the following proposition is immediate.
Proposition 10.1. If a mechanism M is ϵ-diﬀerentially private, then
M is also 2ϵ-approximately dominant strategy truthful.
As a solution concept, this has several robustness properties that
strategy proof mechanisms do not. By the composition property of
diﬀerential privacy, the composition of 2 ϵ-diﬀerentially private mech-
anisms remains 4ϵ-approximately dominant strategy truthful. In con-
trast, the incentive properties of general strategy proof mechanisms
may not be preserved under composition.
Another useful property of diﬀerential privacy as a solution con-
cept is that it generalizes to group privacy: suppose that t and t′ ∈
T n are not neighbors, but instead diﬀer in k indices. Recall that
by group privacy we then have for any player i: Eo∼M(t)[ui(o)] ≤
exp(kϵ)Eo∼M(t′)[ui(o)]. That is, changes in up to k types changes the
expected output by at most ≈(1+kϵ), when k ≪1/ϵ. Therefore, diﬀer-
entially private mechanisms make truthful reporting a 2kϵ-approximate
dominant strategy even for coalitions of k agents — i.e., diﬀerential pri-
vacy automatically provides robustness to collusion. Again, this is in
contrast to general dominant-strategy truthful mechanisms, which in
general oﬀer no guarantees against collusion.
Notably, diﬀerential privacy allows for these properties in very gen-
eral settings without the use of money! In contrast, the set of exactly
dominant strategy truthful mechanisms when monetary transfers are
not allowed is extremely limited.
We conclude with a drawback of using diﬀerential privacy as a solu-
tion concept as stated: not only is truthfully reporting one’s type an
approximate dominant strategy, any report is an approximate domi-
nant strategy! That is, diﬀerential privacy makes the outcome approx-
imately independent of any single agent’s report. In some settings, this
shortcoming can be alleviated. For example, suppose that M is a diﬀer-
entially private mechanism, but that agent utility functions are deﬁned
to be functions both of the outcome of the mechanism, and of the
reported type t′
i of the agent: formally, we view the outcome space as
O′ = O × T. When the agent reports type t′
i to the mechanism, and

10.2. Diﬀerential privacy as a tool in mechanism design
193
the mechanism selects outcome o ∈O, then the utility experienced by
the agent is controlled by the outcome o′ = (o, t′
i). Now consider the
underlying utility function u : T × O′ →[0, 1]. Suppose we have that
ﬁxing a selection o of the mechanism, truthful reporting is a dominant
strategy — that is, for all types ti, t′
i, and for all outcomes o ∈O:
u(ti, (o, ti)) ≥u(ti, (o, t′
i)).
Then it remains the fact that truthful reporting to an ϵ-diﬀerentially
private mechanism M : T n →O remains a 2ϵ approximate dominant
strategy, because for any misreport t′
i that player i might consider, we
have:
u(ti, (M(t), ti)) = Eo∼M(t)[u(ti, (o, ti))]
≥(1 + 2ϵ)Eo∼M(t′
i,t−i)[u(ti, (o, ti))]
≥Eo∼M(t′
i,t−i)[u(ti, (o, t′
i))]
= u(ti, (M(t′
i, t−i), t′
i)).
However, we no longer have that every report is an approximate
dominant strategy, because player i’s utility can depend arbitrarily on
o′ = (o, t′
i), and only o (and not player i’s report t′
i itself) is diﬀerentially
private. This will be the case in all examples we consider here.
10.2
Diﬀerential privacy as a tool in mechanism design
In this section, we show how the machinery of diﬀerential privacy can
be used as a tool in designing novel mechanisms.
10.2.1
Warmup: digital goods auctions
To warm up, let us consider a simple special case of the ﬁrst application
of diﬀerential privacy in mechanism design. Consider a digital goods
auction, i.e., one where the seller has an unlimited supply of a good
with zero marginal cost to produce, for example a piece of software
or other digital media. There are n unit demand buyers for this good,
each with unknown valuation vi ∈[0, 1]. Informally, the valuation vi
of a bidder i represents the maximum amount of money that buyer i

194
Diﬀerential Privacy and Mechanism Design
would be willing to pay for a good. There is no prior distribution on
the bidder valuations, so a natural revenue benchmark is the revenue
of the best ﬁxed price. At a price p ∈[0, 1], each bidder i with vi ≥p
will buy. Therefore the total revenue of the auctioneer is
Rev(p, v) = p · |{i : vi ≥p}|.
The optimal revenue is the revenue of the best ﬁxed price: OPT =
maxp Rev(p, v). This setting is well studied: the best known result for
exactly dominant strategy truthful mechanisms is a mechanism which
achieves revenue at least OPT −O(√n).
We show how a simple application of the exponential mechanism
achieves revenue at least OPT −O

log n
ϵ

. That is, the mechanism
trades exact for approximate truthfulness, but achieves an exponen-
tially better revenue guarantee. Of course, it also inherits the beneﬁts
of diﬀerential privacy discussed previously, such as resilience to collu-
sion, and composability.
The idea is to select a price from the exponential mechanism,
using as our “quality score” the revenue that this price would obtain.
Suppose we choose the range of the exponential mechanism to be
R = {α, 2α, . . . , 1}. The size of the range is |R| = 1/α. What have
we lost in potential revenue if we restrict ourselves to selecting a price
from R? It is not hard to see that
OPTR ≡max
p∈R Rev(p, v) ≥OPT −αn.
This is because if p∗is the price that achieves the optimal revenue, and
we use a price p such that p∗−α ≤p ≤p∗, every buyer who bought
at the optimal price continues to buy, and provides us with at most α
less revenue per buyer. Since there are at most n buyers, the total lost
revenue is at most αn.
So how do we parameterize the exponential mechanism? We have a
family of discrete ranges R, parameterized by α. For a vector of values
v and a price p ∈R, we deﬁne our quality function to be q(v, p) =
Rev(v, p). Observe that because each value vi ∈[0, 1], we can restrict
attention to prices p ≤1 and hence, the sensitivity of q is ∆= 1:
changing one bidder valuation can only change the revenue at a ﬁxed

10.2. Diﬀerential privacy as a tool in mechanism design
195
price by at most vi ≤1. Therefore, if we require ϵ-diﬀerential privacy,
by Theorem 3.11, we get that with high probability, the exponential
mechanism returns some price p such that
Rev(p, v) ≥(OPT −αn) −O
1
ϵ ln
 1
α

.
Choosing our discretization parameter α to minimize the two sources
of error, we ﬁnd that this mechanism with high probability ﬁnds us a
price that achieves revenue
Rev(p, v) ≥OPT −O
log n
ϵ

.
What is the right level to choose for the privacy parameter ϵ? Note
that here, we do not necessarily view privacy itself as a goal of our
computation. Rather, ϵ is a way of trading oﬀthe revenue guarantee
with an upper bound on agent’s incentives to deviate. In the literature
on large markets in economics, a common goal when exact truthfulness
is out of reach is “asymptotic truthfulness” – that is, the maximum
incentive that any agent has to deviate from his truthful report tend
to 0 as the size of the market n grows large. To achieve a result like
that here, all we need to do is set ϵ to be some diminishing function in
the number of agents n. For example, if we take ϵ = 1/ log(n), then we
obtain a mechanism that is asymptotically exactly truthful (i.e., as the
market grows large, the approximation to truthfulness becomes exact).
We can also ask what our approximation to the optimal revenue is as
n grows large. Note that our approximation to the optimal revenue is
only additive, and so even with this setting of ϵ, we can still guarantee
revenue at least (1 −o(1))OPT, so long as OPT grows more quickly
than log(n)2 with the size of the population n.
Finally, notice that we could make the reported value vi of each
agent i binding. In other words, we could allocate an item to agent i
and extract payment of the selected posted price p whenever vi ≥p. If
we do this, the mechanism is approximately truthful, because the price
is picked using a diﬀerentially private mechanism. Additionally, it is
not the case that every report is an approximate dominant strategy:
if an agent over-reports, she may be forced to buy the good at a price
higher than her true value.

196
Diﬀerential Privacy and Mechanism Design
10.2.2
Approximately truthful equilibrium selection mechanisms
We now consider the problem of approximately truthful equilibrium
selection. We recall the deﬁnition of a Nash Equilibrium: Suppose each
player has a set of actions A, and can choose to play any action ai ∈A.
Suppose, moreover, that outcomes are merely choices of actions that the
agents might choose to play, and so agent utility functions are deﬁned
as u : T × An →[0, 1]. Then:
Deﬁnition 10.3. A set of actions a ∈An is an ϵ-approximate Nash
equilibrium if for all players i and for all actions a′
i:
ui(a) ≥ui(a′
i, a−i) −ϵ
In other words, every agent is simultaneously playing an (approximate)
best response to what the other agents are doing, assuming they are
playing according to a.
Roughly speaking, the problem is as follows: suppose we are given
a game in which each player knows their own payoﬀs, but not others’
payoﬀs (i.e., the players do not know what the types are of the other
agents). The players therefore do not know the equilibrium structure
of this game. Even if they did, there might be multiple equilibria, with
diﬀerent agents preferring diﬀerent equilibria. Can a mechanism oﬀered
by an intermediary incentivize agents to truthfully report their utilities
and follow the equilibrium it selects?
For example, imagine a city in which (say) Google Navigation is
the dominant service. Every morning, each person enters their starting
point and destination, receives a set of directions, and chooses his/her
route according to those directions. Is it possible to design a naviga-
tion service such that: Each agent is incentivized to both (1) report
truthfully, and (2) then follow the driving directions provided? Both
misreporting start and end points, and truthfully reporting start and
end points, but then following a diﬀerent (shorter) path are to be dis-
incentivized.
Intuitively, our two desiderata are in conﬂict. In the commuting
example above, if we are to guarantee that every player is incentivized
to truthfully follow their suggested route, then we must compute an

10.2. Diﬀerential privacy as a tool in mechanism design
197
equilibrium of the game in question given players’ reports. On the other
hand, to do so, our suggested route to some player i must depend on
the reported location/destination pairs of other players. This tension
will pose a problem in terms of incentives: if we compute an equilibrium
of the game given the reports of the players, an agent can potentially
beneﬁt by misreporting, causing us to compute an equilibrium of the
wrong game.
This problem would be largely alleviated, however, if the report of
agent i only has a tiny eﬀect on the actions of agents j ̸= i. In this
case, agent i could hardly gain an advantage through his eﬀect on other
players. Then, assuming that everyone truthfully reported their type,
the mechanism would compute an equilibrium of the correct game, and
by deﬁnition, each agent i could do no better than follow the suggested
equilibrium action. In other words, if we could compute an approxi-
mate equilibrium of the game under the constraint of diﬀerential pri-
vacy, then truthful reporting, followed by taking the suggested action
of the coordination device would be a Nash equilibrium. A moment’s
reﬂection reveals that the goal of privately computing an equilibrium
is not possible in small games, in which an agent’s utility is a highly
sensitive function of the actions (and hence utility functions) of the
other agents. But what about in large games?
Formally, suppose we have an n player game with action set A, and
each agent with type ti has a utility function ui : An →[0, 1]. We say
that this game is ∆-large if for all players i ̸= j, vectors of actions
a ∈An, and pairs of actions aj, a′
j ∈A:
ui(aj, a−j) −ui(a′
j, a−j)
 ≤∆.
In other words, if some agent j unilaterally changes his action, then his
aﬀect on the payoﬀof any other agent i ̸= j is at most ∆. Note that if
agent j changes his own action, then his payoﬀcan change arbitrarily.
Many games are “large” in this sense. In the commuting example above,
if Alice changes her route to work she may substantially increase or
decrease her commute time, but will only have a minimal impact on
the commute time of any other agent Bob. The results in this section
are strongest for ∆= O(1/n), but hold more generally.

198
Diﬀerential Privacy and Mechanism Design
First we might ask whether we need privacy at all— could it be the
case that in a large game, any algorithm which computes an equilibrium
of a game deﬁned by reported types has the stability property that we
want? The answer is no. As a simple example, consider n people who
must each choose whether to go to the beach (B) or the mountains (M).
People privately know their types— each person’s utility depends on
his own type, his action, and the fraction of other people p who go to
the beach. A Beach type gets a payoﬀof 10p if he visits the beach, and
5(1 −p) if he visits the mountain. A mountain type gets a payoﬀ5p
from visiting the beach, and 10(1−p) from visiting the mountain. Note
that this is a large (i.e., low sensitivity) game — each player’s payoﬀs
are insensitive in the actions of others. Further, note that “everyone
visits beach” and “everyone visits mountain” are both equilibria of the
game, regardless of the realization of types. Consider the mechanism
that attempts to implement the following social choice rule— “if the
number of beach types is less than half the population, send everyone
to the beach, and vice versa.” It should be clear that if mountain types
are just in the majority, then each mountain type has an incentive to
misreport as a beach type; and vice versa. As a result, even though
the game is “large” and agents’ actions do not aﬀect others’ payoﬀs
signiﬁcantly, simply computing equilibria from reported type proﬁles
does not in general lead to even approximately truthful mechanisms.
Nevertheless, it turns out to be possible to give a mechanism with
the following property: it elicits the type ti of each agent, and then com-
putes an α-approximate correlated equilibrium of the game deﬁned by
the reported types.2 (In some cases, it is possible to strengthen this
result to compute an approximate Nash equilibrium of the underlying
game.) It draws an action proﬁle a ∈An from the correlated equi-
librium, and reports action ai to each agent i. The algorithm has the
guarantee that simultaneously for all players i, the joint distribution
a−i on reports to all players other than i is diﬀerentially private in
2A correlated equilibrium is deﬁned by a joint distribution on proﬁles of actions,
An. For an action proﬁle a drawn from the distribution, if agent i is told only ai,
then playing action ai is a best response given the induced conditional distribution
over a−i. An α-approximate correlated equilibrium is one where deviating improves
an agent’s utility by at most α.

10.2. Diﬀerential privacy as a tool in mechanism design
199
the reported type of agent i. When the algorithm computes a corre-
lated equilibrium of the underlying game, this guarantee is suﬃcient
for a restricted form of approximate truthfulness: agents who have the
option to opt-in or opt-out of the mechanism (but not to misreport
their type if they opt-in) have no disincentive to opt-out, because no
agent i can substantially change the distribution on actions induced on
the other players by opting out. Moreover, given that he opts in, no
agent has incentive not to follow his suggested action, as his suggestion
is part of a correlated equilibrium. When the mechanism computes a
Nash equilibrium of the underlying game, then the mechanism becomes
truthful even when agents have the ability to mis-report their type to
the mechanism when they opt in.
More
speciﬁcally,
when
these
mechanisms
compute
an
α-
approximate Nash equilibrium while satisfying ϵ-diﬀerential privacy,
every agent following the honest behavior (i.e., ﬁrst opting in and
reporting their true type, then following their suggested action) forms
an (2ϵ + α)-approximate Nash equilibrium. This is because, by pri-
vacy, reporting your true type is a 2ϵ-approximate dominant strat-
egy, and given that everybody reports their true type, the mechanism
computes an α-approximate equilibrium of the true game, and hence
by deﬁnition, following the suggested action is an α-approximate best
response. There exist mechanisms for computing and α-approximate
equilibrium in large games with α = O

1
√nϵ

. Therefore, by setting
ϵ = O

1
n1/4

, this gives an η-approximately truthful equilibrium selec-
tion mechanism for
η = 2ϵ + α = O
 1
n1/4

.
In other words, it gives a mechanism for coordinating equilibrium
behavior in large games that is asymptotically truthful in the size of
the game, all without the need for monetary transfers.
10.2.3
Obtaining exact truthfulness
So far we have discussed mechanisms that are asymptotically truthful
in large population games. However, what if we want to insist on mech-
anisms that are exactly dominant strategy truthful, while maintaining

200
Diﬀerential Privacy and Mechanism Design
some of the nice properties enjoyed by our mechanisms so far: for exam-
ple, that the mechanisms do not need to be able to extract monetary
payments? Can diﬀerential privacy help here? It can—in this section,
we discuss a framework which uses diﬀerentially private mechanisms as
a building block toward designing exactly truthful mechanisms without
money.
The basic idea is simple and elegant. As we have seen, the expo-
nential mechanism can often give excellent utility guarantees while
preserving diﬀerential privacy. This doesn’t yield an exactly truthful
mechanism, but it gives every agent very little incentive to deviate
from truthful behavior. What if we could pair this with a second mech-
anism which need not have good utility guarantees, but gives each agent
a strict positive incentive to report truthfully, i.e., a mechanism that
essentially only punishes non-truthful behavior? Then, we could ran-
domize between running the two mechanisms. If we put enough weight
on the punishing mechanism, then we inherit its strict-truthfulness
properties. The remaining weight that is put on the exponential mech-
anism contributes to the utility properties of the ﬁnal mechanism. The
hope is that since the exponential mechanism is approximately strategy
proof to begin with, the randomized mechanism can put small weight
on the strictly truthful punishing mechanism, and therefore will have
good utility properties.
To design punishing mechanisms, we will have to work in a slightly
non-standard environment. Rather than simply picking an outcome, we
can model a mechanism as picking an outcome, and then an agent as
choosing a reaction to that outcome, which together deﬁne his utility.
Mechanisms will then have the power to restrict the reactions allowed
by the agent based on his reported type. Formally, we will work in the
following framework:
Deﬁnition 10.4 (The Environment). An environment is a set N of n
players, a set of types ti ∈T , a ﬁnite set O of outcomes, a set of
reactions R and a utility function u : T × O × R →[0, 1].
We write ri(t, s, ˆRi) ∈arg maxr∈ˆRi ui(t, s, r) to denote is optimal
reaction among choices ˆRi ⊆R to alternative s if he is of type t.

10.2. Diﬀerential privacy as a tool in mechanism design
201
A direct revelation mechanism M deﬁnes a game which is played
as follows:
1. Each player i reports a type t′
i ∈T .
2. The mechanism chooses an alternative s ∈O and a subset ˆRi ⊆R
of reactions, for each player i.
3. Each player i chooses a reaction ri ∈ˆRi and experiences utility
u(ti, s, ri).
Agents play so as to maximize their own utility. Note that since there
is no further interaction after the 3rd step, rational agents will pick
ri = ri(ti, s, ˆRi), and so we can ignore this as a strategic step. Let
R = 2R. Then a mechanism is a randomized mapping M : T →O×Rn.
Let us consider the utilitarian welfare criterion: F(t, s, r)
=
1
n
Pn
i=1 u(ti, s, ri), Note that this has sensitivity ∆= 1/n, since each
agent’s utility lies in the range [0, 1]. Hence, if we simply choose an out-
come s and allow each agent to play their best response reaction, the
exponential mechanism is an ϵ-diﬀerentially private mechanism, which
by Theorem 3.11, achieves social welfare at least OPT −O
 log |O|
ϵn

with high probability. Let us denote this instantiation of the exponen-
tial mechanism, with quality score F, range O and privacy parameter
ϵ, as Mϵ.
The idea is to randomize between the exponential mechanism (with
good social welfare properties) and a strictly truthful mechanism which
punishes false reporting (but with poor social welfare properties). If we
mix appropriately, then we will get an exactly truthful mechanism with
reasonable social welfare guarantees.
Here is one such punishing mechanism which is simple, but not
necessarily the best for a given problem:
Deﬁnition 10.5. The commitment mechanism MP (t′) selects s ∈O
uniformly at random and sets ˆRi = {ri(t′
i, s, Ri)}, i.e., it picks a random
outcome and forces everyone to react as if their reported type was their
true type.
Deﬁne the gap of an environment as
γ =
min
i,ti̸=t′
i,t−i
max
s∈O
 u(ti, s, ri(ti, s, Ri)) −u(ti, s, ri(t′
i, s, Ri))
 ,

202
Diﬀerential Privacy and Mechanism Design
i.e., γ is a lower bound over players and types of the worst-case cost
(over s) of mis-reporting. Note that for each player, this worst-case is
realized with probability at least 1/|O|. Therefore we have the following
simple observation:
Lemma 10.2. For all i, ti, t′
i, t−i:
u(ti, MP (ti, t−i)) ≥u(ti, MP (t′
i, t−i)) + γ
|O|.
Note that the commitment mechanism is strictly truthful: every
individual has at least a
γ
|O| incentive not to lie.
This suggests an exactly truthful mechanism with good social wel-
fare guarantees:
Deﬁnition 10.6. The punishing exponential mechanism MP
ϵ (t) deﬁned
with parameter 0 ≤q ≤1 selects the exponential mechanism Mϵ(t)
with probability 1 −q and the punishing mechanism MP (t) with com-
plementary probability q.
Observe that by linearity of expectation, we have for all ti, t′
i, t−i:
u(ti, MP
ϵ (ti, t−i)) = (1 −q) · u(ti, Mϵ(ti, t−i)) + q · u(ti, MP (ti, t−i))
≥(1 −q)
 u(ti, Mϵ(t′
i, t−i)) −2ϵ

+q

u(ti, MP (t′
i, t−i)) + γ
|O|

= u(ti, MP
ϵ (t′
i, t−i)) −(1 −q)2ϵ + q γ
|O|
= u(ti, MP
ϵ (t′
i, t−i)) −2ϵ + q

2ϵ + γ
|O|

.
The following two theorems show incentive and social welfare prop-
erties of this mechanism.
Theorem 10.3. If 2ϵ ≤qγ
|O| then MP
ϵ is strictly truthful.

10.2. Diﬀerential privacy as a tool in mechanism design
203
Note that we also have utility guarantees for this mechanism. Set-
ting the parameter q so that we have a truthful mechanism:
Es, ˆR∼MPϵ [F(t, s, r(t, s, ˆR))]
≥(1 −q) · Es, ˆR∼Mϵ[F(t, s, r(t, s, ˆR))]
=

1 −2ϵ|O|
γ

· Es, ˆR∼Mϵ[F(t, s, r(t, s, ˆR))]
≥

1 −2ϵ|O|
γ

·

max
t,s,r F(t, s, r) −O
 1
ϵn log |O|

≥max
t,s,r F(t, s, r) −2ϵ|O|
γ
−O
 1
ϵn log |O|

.
Setting
ϵ ∈O
 s
log |O|γ
|O|n
!
we ﬁnd:
Es, ˆR∼MPϵ [F(t, s, r(t, s, ˆR))] ≥max
t,s,r F(t, s, r) −O
 s
|O| log |O|
γn
!
.
Note that in this calculation, we assume that ϵ ≤γ/(2|O|) so that
q =
2ϵ|O|
γ
≤1 and the mechanism is well deﬁned. This is true for
suﬃciently large n. That is, we have shown:
Theorem 10.4. For suﬃciently large n, MP
ϵ achieves social welfare at
least
OPT −O
 s
|O| log |O|
γn
!
.
Note that this mechanism is truthful without the need for payments!
Let us now consider an application of this framework: the facil-
ity location game. Suppose that a city wants to build k hospitals to
minimize the average distance between each citizen and their clos-
est hospital. To simplify matters, we make the mild assumption that
the city is built on a discretization of the unit line.3 Formally, let
3If this is not the case, we can easily raze and then re-build the city.

204
Diﬀerential Privacy and Mechanism Design
L(m) = {0, 1
m, 2
m, . . . , 1} denote the discrete unit line with step-size
1/m. |L(m)| = m+1. Let T = Ri = L(m) for all i and let |O| = L(m)k.
Deﬁne the utility of agent i to be:
u(ti, s, ri) =
(
−|ti −ri|,
If ri ∈s;
−1,
otherwise.
In other words, agents are associated with points on the line, and an
outcome is an assignment of a location on the line to each of the k
facilities. Agents can react to a set of facilities by deciding which one
to go to, and their cost for such a decision is the distance between their
own location (i.e., their type) and the facility that they have chosen.
Note that ri(ti, s) is here the closest facility ri ∈s.
We can instantiate Theorem 10.4. In this case, we have: |O| =
(m + 1)k and γ = 1/m, because any two positions ti ̸= t′
i diﬀer by at
least 1/m. Hence, we have:
Theorem 10.5. MP
ϵ
instantiated for the facility location game is
strictly truthful and achieves social welfare at least:
OPT −O


s
km(m + 1)k log m
n

.
This is already very good for small numbers of facilities k, since we
expect that OPT = Ω(1).
10.3
Mechanism design for privacy aware agents
In the previous section, we saw that diﬀerential privacy can be useful as
a tool to design mechanisms, for agents who care only about the outcome
chosen by the mechanism. We here primarily viewed privacy as a tool
to accomplish goals in traditional mechanism design. As a side aﬀect,
these mechanisms also preserved the privacy of the reported player
types. Is this itself a worthy goal? Why might we want our mechanisms
to preserve the privacy of agent types?
A bit of reﬂection reveals that agents might care about privacy.
Indeed, basic introspection suggests that in the real world, agents value
the ability to keep certain “sensitive” information private, for example,

10.3. Mechanism design for privacy aware agents
205
health information or sexual preferences. In this section, we consider the
question of how to model this value for privacy, and various approaches
taken in the literature.
Given that agents might have preferences for privacy, it is worth
considering the design of mechanisms that preserve privacy as an addi-
tional goal, even for tasks such as welfare maximization that we can
already solve non-privately. As we will see, it is indeed possible to
generalize the VCG mechanism to privately approximately optimize
social welfare in any social choice problem, with a smooth trade-oﬀ
between the privacy parameter and the approximation parameter, all
while guaranteeing exact dominant strategy truthfulness.
However, we might wish to go further. In the presence of agents with
preferences for privacy, if we wish to design truthful mechanisms, we
must somehow model their preferences for privacy in their utility func-
tion, and then design mechanisms which are truthful with respect to
these new “privacy aware” utility functions. As we have seen with dif-
ferential privacy, it is most natural to model privacy as a property of the
mechanism itself. Thus, our utility functions are not merely functions
of the outcome, but functions of the outcome and of the mechanism
itself. In almost all models, agent utilities for outcomes are treated as
linearly separable, that is, we will have for each agent i,
ui(o, M, t) ≡µi(o) −ci(o, M, t).
Here µi(o) represents agent is utility for outcome o and ci(o, M, t) the
(privacy) cost that agent i experiences when outcome o is chosen with
mechanism M.
We will ﬁrst consider perhaps the simplest (and most naïve) model
for the privacy cost function ci. Recall that for ϵ ≪1, diﬀerential
privacy promises that for each agent i, and for every possible utility
function fi, type vector t ∈T n, and deviation t′
i ∈T :
|Eo∼M(ti,t−i)[fi(o)] −Eo∼M(t′
i,t−i)[fi(o)]| ≤2ϵEo∼M(t)[fi(o)].
If we view fi as representing the “expected future utility” for agent i,
it is therefore natural to model agent i’s cost for having his data used
in an ϵ-diﬀerentially private computation as being linear in ϵ. That is,

206
Diﬀerential Privacy and Mechanism Design
we think of agent i as being parameterized by some value vi ∈R, and
take:
ci(o, M, t) = ϵvi,
where ϵ is the smallest value such that M is ϵ-diﬀerentially private.
Here we imagine vi to represent a quantity like Eo∼M(t)[fi(o)]. In this
setting, ci does not depend on the outcome o or the type proﬁle t.
Using this naïve privacy measure, we discuss a basic problem in
private data analysis: how to collect the data, when the owners of
the data value their privacy and insist on being compensated for it.
In this setting, there is no “outcome” that agents value, other than
payments, there is only dis-utility for privacy loss. We will then discuss
shortcomings of this (and other) measures of the dis-utility for privacy
loss, as well as privacy in more general mechanism design settings when
agents do have utility for the outcome of the mechanism.
10.3.1
A private generalization of the VCG mechanism
Suppose we have a general social choice problem, deﬁned by an outcome
space O, and a set of agents N with arbitrary preferences over the
outcomes given by ui : O →[0, 1]. We might want to choose an outcome
o ∈O to maximize the social welfare F(o) = 1
n
Pn
i=1 ui(o). It is well
known that in any such setting, the VCG mechanism can implement the
outcome o∗which exactly maximizes the social welfare, while charging
payments that make truth-telling a dominant strategy. What if we want
to achieve the same result, while also preserving privacy? How must the
privacy parameter ϵ trade oﬀwith our approximation to the optimal
social welfare?
Recall that we could use the exponential mechanism to choose
an outcome o ∈O, with quality score F. For privacy parameter
ϵ, this would give a distribution Mϵ deﬁned to be Pr[Mϵ = o] ∝
exp
 ϵF(o)
2n

. Moreover, this mechanism has good social welfare prop-
erties: with probability 1 −β, it selects some o such that: F(o) ≥
F(o∗) −
2
ϵn

ln |O|
β

. But as we saw, diﬀerential privacy only gives
ϵ-approximate truthfulness.

10.3. Mechanism design for privacy aware agents
207
However, it can be shown that Mϵ is the solution to the following
exact optimization problem:
Mϵ = arg max
D∈∆O

Eo∼D[F(o)] + 2
ϵnH(D)

,
where H represents the Shannon Entropy of the distribution D. In other
words, the exponential mechanism is the distribution which exactly
maximizes the expected social welfare, plus the entropy of the distri-
bution weighted by 2/(ϵn). This is signiﬁcant for the following reason:
it is known that any mechanism that exactly maximizes expected player
utilities in any ﬁnite range (known as maximal in distributional range
mechanisms) can be paired with payments to be made exactly domi-
nant strategy truthful. The exponential mechanism is the distribution
that exactly maximizes expected social welfare, plus entropy. In other
words, if we imagine that we have added a single additional player
whose utility is exactly the entropy of the distribution, then the expo-
nential mechanism is maximal in distributional range. Hence, it can be
paired with payments that make truthful reporting a dominant strategy
for all players — in particular, for the n real players. Moreover, it can
be shown how to charge payments in such a way as to preserve privacy.
The upshot is that for any social choice problem, the social welfare can
be approximated in a manner that both preserves diﬀerential privacy,
and is exactly truthful.
10.3.2
The sensitive surveyor’s problem
In this section, we consider the problem of a data analyst who wishes
to conduct a study using the private data of a collection of individuals.
However, he must convince these individuals to hand over their data!
Individuals experience costs for privacy loss. The data analyst can mit-
igate these costs by guaranteeing diﬀerential privacy and compensating
them for their loss, while trying to get a representative sample of data.
Consider the following stylized problem of the sensitive surveyor
Alice. She is tasked with conducting a survey of a set of n individuals
N, to determine what proportion of the individuals i ∈N satisfy some
property P(i). Her ultimate goal is to discover the true value of this
statistic, s = 1
n|{i ∈N : P(i)}|, but if that is not possible, she will be

208
Diﬀerential Privacy and Mechanism Design
satisﬁed with some estimate ˆs such that the error, |ˆs−s|, is minimized.
We will adopt a notion of accuracy based on large deviation bounds,
and say that a surveying mechanism is α-accurate if Pr[|ˆs−s| ≥α] ≤1
3.
The inevitable catch is that individuals value their privacy and will not
participate in the survey for free. Individuals experience some cost as a
function of their loss in privacy when they interact with Alice, and must
be compensated for this loss. To make matters worse, these individuals
are rational (i.e., selﬁsh) agents, and are apt to misreport their costs
to Alice if doing so will result in a ﬁnancial gain. This places Alice’s
problem squarely in the domain of mechanism design, and requires
Alice to develop a scheme for trading oﬀstatistical accuracy with cost,
all while managing the incentives of the individuals.
As an aside, this stylized problem is broadly relevant to any organi-
zation that makes use of collections of potentially sensitive data. This
includes, for example, the use of search logs to provide search query
completion and the use of browsing history to improve search engine
ranking, the use of social network data to select display ads and to
recommend new links, and the myriad other data-driven services now
available on the web. In all of these cases, value is being derived from
the statistical properties of a collection of sensitive data in exchange
for some payment.4
Collecting data in exchange for some ﬁxed price could lead to a
biased estimate of population statistics, because such a scheme will
result in collecting data only from those individuals who value their
privacy less than the price being oﬀered. However, without interacting
with the agents, we have no way of knowing what price we can oﬀer
so that we will have broad enough participation to guarantee that the
answer we collect has only small bias. To obtain an accurate estimate
of the statistic, it is therefore natural to consider buying private data
using an auction — as a means of discovering this price. There are two
obvious obstacles which one must confront when conducting an auction
for private data, and an additional obstacle which is less obvious but
more insidious. The ﬁrst obstacle is that one must have a quantitative
4The payment need not be explicit and/or dollar denominated — for example,
it may be the use of a “free” service.

10.3. Mechanism design for privacy aware agents
209
formalization of “privacy” which can be used to measure agents’ costs
under various operations on their data. Here, diﬀerential privacy pro-
vides an obvious tool. For small values of ϵ, because exp(ϵ) ≈(1 + ϵ),
and so as discussed earlier, a simple (but possibly naive) ﬁrst cut at a
model is to view each agent as having some linear cost for participating
in a private study. We here imagine that each agent i has an unknown
value for privacy vi, and experiences a cost ci(ϵ) = ϵvi when his private
data is used in an ϵ-diﬀerentially private manner.5 The second obstacle
is that our objective is to trade oﬀwith statistical accuracy, and the
latter is not well-studied objective in mechanism design.
The ﬁnal, more insidious obstacle, is that an individual’s cost for
privacy loss may be highly correlated with his private data itself! Sup-
pose we only know Bob has a high value for privacy of his AIDS status,
but do not explicitly know his AIDS status itself. This is already dis-
closive because Bob’s AIDS status is likely correlated with his value for
privacy, and knowing that he has a high cost for privacy lets us update
our belief about what his private data might be. More to the point,
suppose that in the ﬁrst step of a survey of AIDS prevalence, we ask
each individual to report their value for privacy, with the intention of
then running an auction to choose which individuals to buy data from.
If agents report truthfully, we may ﬁnd that the reported values natu-
rally form two clusters: low value agents, and high value agents. In this
case, we may have learned something about the population statistic
even before collecting any data or making any payments— and there-
fore, the agents will have already experienced a cost. As a result, the
agents may misreport their value, which could introduce a bias in the
survey results. This phenomenon makes direct revelation mechanisms
problematic, and distinguishes this problem from classical mechanism
design.
Armed with a means of quantifying an agent i’s loss for allowing his
data to be used by an ϵ-diﬀerentially-private algorithm (ci(ϵ) = ϵvi),
we are almost ready to describe results for the sensitive surveyor’s
problem. Recall that a diﬀerentially private algorithm is some mapping
M : T n →O, for a general type space T . It remains to deﬁne what
5As we will discuss later, this assumption can be problematic.

210
Diﬀerential Privacy and Mechanism Design
exactly the type space T is. We will consider two models. In both
models, we will associate with each individual a bit bi ∈{0, 1} which
represents whether they satisfy the sensitive predicate P(i), as well as
a value for privacy vi ∈R+.
1. In the insensitive value model, we calculate the ϵ parameter of the
private mechanism by letting the type space be T = {0, 1}: i.e.,
we measure privacy cost only with respect to how the mechanism
treats the sensitive bit bi, and ignore how it treats the reported
values for privacy, vi.6
2. In the sensitive value model, we calculate the ϵ parameter of the
private mechanism by letting the type space be T = ({0, 1}×R+):
i.e., we measure privacy with respect to how it treats the pair
(bi, vi) for each individual.
Intuitively, the insensitive value model treats individuals as ignoring
the potential privacy loss due to correlations between their values for
privacy and their private bits, whereas the sensitive value model treats
individuals as assuming these correlations are worst-case, i.e., their
values vi are just as disclosive as their private bits bi. It is known that
in the insensitive value model, one can derive approximately optimal
direct revelation mechanisms that achieve high accuracy and low cost.
By contrast, in the sensitive value model, no individually rational direct
revelation mechanism can achieve any non-trivial accuracy.
This leaves a somewhat unsatisfying state of aﬀairs. The sensitive
value model captures the delicate issues that we really want to deal
with, and yet there we have an impossibility result! Getting around
this result in a satisfying way (e.g., by changing the model, or the
powers of the mechanism) remains an intriguing open question.
10.3.3
Better measures for the cost of privacy
In the previous section, we took the naive modeling assumption that the
cost experienced by participation in an ϵ-diﬀerentially private mecha-
nism M was ci(o, M, t) = ϵvi for some numeric value vi. This measure
6That is, the part of the mapping dealing with reported values need not be
diﬀerentially private.

10.3. Mechanism design for privacy aware agents
211
is problematic for several reasons. First, although diﬀerential privacy
promises that any agent’s loss in utility is upper bounded by a quantity
that is (approximately) linear in ϵ, there is no reason to believe that
agents’ costs are lower bounded by such a quantity. That is, while tak-
ing ci(o, M, t) ≤ϵvi is well motivated, there is little support for making
the inequality an equality. Second, (it turns out) any privacy measure
which is a deterministic function only of ϵ (not just a linear function)
leads to problematic behavioral predictions.
So how else might we model ci? One natural measure is the mutual
information between the reported type of agent i, and the outcome
of the mechanism. For this to be well deﬁned, we must be in a world
where each agent’s type ti is drawn from a known prior, ti ∼T . Each
agent’s strategy is a mapping σi : T →T , determining what type he
reports, given his true type. We could then deﬁne
ci(o, M, σ) = I(T ; M(t−i, σ(T )),
where I is the mutual information between the random variable T
representing the prior on agent is type, and M(t−i, σ(T )), the random
variable representing the outcome of the mechanism, given agent is
strategy.
This measure has signiﬁcant appeal, because it represents how
“related” the output of the mechanism is to the true type of agent
i. However, in addition to requiring a prior over agent types, observe
an interesting paradox that results from this measure of privacy loss.
Consider a world in which there are two kinds of sandwich breads: Rye
(R), and Wheat (W). Moreover, in this world, sandwich preferences
are highly embarrassing and held private. The prior on types T is uni-
form over R and W, and the mechanism M simply gives agent i a
sandwich of the type that he purports to prefer. Now consider two pos-
sible strategies, σtruthful and σrandom. σtruthful corresponds to truthfully
reporting sandwich preferences (and subsequently leads to eating the
preferred sandwich type), while σrandom randomly reports independent
of true type (and results in the preferred sandwich only half the time).
The cost of using the random strategy is I(T ; M(t−i, σrandom(T )) = 0,
since the output is independent of agent i’s type. On the other hand,
the cost of truthfully reporting is I(T ; M(t−i, σtruthful(T )) = 1, since

212
Diﬀerential Privacy and Mechanism Design
the sandwich outcome is now the identity function on agent is type.
However, from the perspective of any outside observer, the two strate-
gies are indistinguishable! In both cases, agent i receives a uniformly
random sandwich. Why then should anyone choose the random strat-
egy? So long as an adversary believes they are choosing randomly, they
should choose the honest strategy.
Another approach, which does not need a prior on agent types, is as
follows. We may model agents as having a cost function ci that satisﬁes:
|ci(o, M, t)| = ln
 
max
ti,t′
i∈T
Pr[M(ti, t−i) = o]
Pr[M(t′
i, t−i) = o]
!
.
Note that if M is ϵ-diﬀerentially private, then
max
t∈T n max
o∈O max
ti,t′
i∈T ln
 
Pr[M(ti, t−i) = o]
Pr[M(t′
i, t−i) = o]
!
≤ϵ.
That is, we can view diﬀerential privacy as bounding the worst-case
privacy loss over all possible outcomes, whereas the measure proposed
here considers only the privacy loss for the outcome o (and type vec-
tor t) actually realized. Thus, for any diﬀerentially private mechanism
M, |ci(o, M, t)| ≤ϵ for all o, t, but it will be important that the cost
can vary by outcome.
We can then consider the following allocation rule for maximizing
social welfare F(o) = Pn
i=1 ui(o).7 We discuss the case when |O| = 2
(which does not require payments), but it is possible to analyze the
general case (with payments), which privately implements the VCG
mechanism for any social choice problem.
1. For each outcome o ∈O, choose a random number ro from the
distribution Pr[ro = x] ∝exp(−ϵ|x|).
2. Output o∗= arg maxo∈O(F(o) + ro).
The above mechanism is ϵ-diﬀerentially private, and that it is truthful
for privacy aware agents, so long as for each agent i, and for the two
outcomes o, o′ ∈O, |µi(o) −µi(o′)| > 2ϵ. Note that this will be true
7This allocation rule is extremely similar to, and indeed can be modiﬁed to be
identical to the exponential mechanism.

10.4. Bibliographical notes
213
for small enough ϵ so long as agent utilities for outcomes are distinct.
The analysis proceeds by considering an arbitrary ﬁxed realization of
the random variables ro, and an arbitrary deviation t′
i from truthful
reporting for the ith agent. There are two cases: In the ﬁrst case, the
deviation does not change the outcome o of the mechanism. In this
case, neither the agent’s utility for the outcome µi, nor his cost for
privacy loss ci change at all, and so the agent does not beneﬁt from
deviating. In the second case, if the outcome changes from o to o′ when
agent i deviates, it must be that µi(o′) < µi(o) −2ϵ. By diﬀerential
privacy, however, |ci(o, M, t) −ci(o′, M, t)| ≤2ϵ, and so the change in
privacy cost cannot be enough to make it beneﬁcial.
Finally, the most conservative approach to modeling costs for pri-
vacy generally considered is as follows. Given an ϵ-diﬀerentially private
mechanism M, assume only that
ci(o, M, t) ≤ϵvi,
for some number vi. This is similar to the linear cost functions that we
considered earlier, but crucially, here we assume only an upper bound.
This assumption is satisﬁed by all of the other models for privacy cost
that we have considered thus far. It can be shown that many mecha-
nisms that combine a diﬀerentially private algorithm with a punishing
mechanism that has the ability to restrict user choices, like those that
we considered in Section 10.2.3, maintain their truthfulness properties
in the presence of agents with preferences for privacy, so long as the
values vi are bounded.
10.4
Bibliographical notes
This section is based oﬀof a survey of Pai and Roth [70] and a survey
of Roth [73]. The connections between diﬀerential privacy and mech-
anism design were ﬁrst suggested by Jason Hartline and investigated
by McSherry and Talwar in their seminal work, “Mechanism Design
via Diﬀerential Privacy” [61], where they considered the application of
diﬀerential privacy to designing approximately truthful digital goods
auctions. The best result for exactly truthful mechanisms in the digital
goods setting is due to Balcan et al. [2].

214
Diﬀerential Privacy and Mechanism Design
The problem of designing exactly truthful mechanisms using diﬀer-
ential privacy as a tool was ﬁrst explored by Nissim, Smorodinsky, and
Tennenholtz in [69], who also ﬁrst posed a criticism as diﬀerential pri-
vacy (by itself) used as a solution concept. The example in this section
of using diﬀerential privacy to obtain exactly truthful mechanisms is
taken directly from [69]. The sensitive surveyors problem was ﬁrst con-
sidered by Ghosh and Roth [36], and expanded on by [56, 34, 75, 16].
Fleischer and Lyu [34] consider the Bayesian setting discussed in this
section, and Ligett and Roth [56] consider the worst-case setting with
take-it-or-leave-it oﬀers, both in an attempt to get around the impossi-
bility result of [36]. Ghosh and Ligett consider a related model in which
participation decisions (and privacy guarantees) are determined only
in equilibrium [35].
The question of conducting mechanism design in the presence of
agents who explicitly value privacy as part of their utility function was
ﬁrst raised by the inﬂuential work of Xiao [85], who considered (among
other measures for privacy cost) the mutual information cost function.
Following this, Chen et al. [15] and Nissim et al. [67] showed how in
two distinct models, truthful mechanisms can sometimes be designed
even for agents who value privacy. Chen Chong, Kash, Moran, and
Vadhan considered the outcome-based cost function that we discussed
in this section, and Nissim, Orlandi, and Smorodinsky considered the
conservative model of only upper bounding each agent’s cost by a linear
function in ϵ> The “sandwich paradox” of valuing privacy according
to mutual information is due to Nissim, Orlandi, and Smorodinsky.
Huang and Kannan proved that the exponential mechanism could
be made exactly truthful with the addition of payments [49]. Kearns
Pai, Roth, and Ullman showed how diﬀerential privacy could be used to
derive asymptotically truthful equilibrium selection mechanisms [54] by
privately computing correlated equilibria in large games. These results
were strengthened by Rogers and Roth [71], who showed how to pri-
vately compute approximate Nash equilibria in large congestion games,
which leads to stronger incentive properties of the mechanism. Both of
these papers use the solution concept of “Joint Diﬀerential Privacy,”

10.4. Bibliographical notes
215
which requires that for every player i, the joint distribution on messages
sent to other players j ̸= i be diﬀerentially private in is report. This
solution concept has also proven useful in other settings of private
mechanism design settings, including an algorithm for computing pri-
vate matchings by Hsu et al. [47].

11
Diﬀerential Privacy and Machine Learning
One of the most useful tasks in data analysis is machine learning: the
problem of automatically ﬁnding a simple rule to accurately predict cer-
tain unknown characteristics of never before seen data. Many machine
learning tasks can be performed under the constraint of diﬀerential pri-
vacy. In fact, the constraint of privacy is not necessarily at odds with
the goals of machine learning, both of which aim to extract informa-
tion from the distribution from which the data was drawn, rather than
from individual data points. In this section, we survey a few of the most
basic results on private machine learning, without attempting to cover
this large ﬁeld completely.
The goal in machine learning is very often similar to the goal in pri-
vate data analysis. The learner typically wishes to learn some simple
rule that explains a data set. However, she wishes this rule to general-
ize — that is, it should be that the rule she learns not only correctly
describes the data that she has on hand, but that it should also be
able to correctly describe new data that is drawn from the same dis-
tribution. Generally, this means that she wants to learn a rule that
captures distributional information about the data set on hand, in a
way that does not depend too speciﬁcally on any single data point. Of
216

217
course, this is exactly the goal of private data analysis — to reveal dis-
tributional information about the private data set, without revealing
too much about any single individual in the dataset. It should come
as no surprise then that machine learning and private data analysis
are closely linked. In fact, as we will see, we are often able to perform
private machine learning nearly as accurately, with nearly the same
number of examples as we can perform non-private machine learning.
Let us ﬁrst brieﬂy deﬁne the problem of machine learning. Here, we
will follow Valiant’s PAC (Or Probably Approximately Correct) model
of machine learning. Let X = {0, 1}d be the domain of “unlabeled
examples.” Think of each x ∈X as a vector containing d boolean
attributes. We will think of vectors x ∈X as being paired with labels
y ∈{0, 1}.
Deﬁnition 11.1. A labeled example is a pair (x, y) ∈X ×{0, 1}: a vector
paired with a label.
A learning problem is deﬁned as a distribution D over labeled exam-
ples. The goal will to be to ﬁnd a function f : X →{0, 1} that correctly
labels almost all of the examples drawn from the distribution.
Deﬁnition 11.2. Given a function f : X →{0, 1} and a distribution
D over labeled examples, the error rate of f on D is:
err(f, D) =
Pr
(x,y)∼D[f(x) ̸= y]
We can also deﬁne the error rate of f over a ﬁnite sample D:
err(f, D) =
1
|D||{(x, y) ∈D : f(x) ̸= y}|.
A learning algorithm gets to observe some number of labeled exam-
ples drawn from D, and has the goal of ﬁnding a function f with as
small an error rate as possible when measured on D. Two parameters
in measuring the quality of a learning algorithm are its running time,
and the number of examples it needs to see in order to ﬁnd a good
hypothesis.
Deﬁnition 11.3. An algorithm A is said to PAC-learn a class of func-
tions C over d dimensions if for every α, β > 0, there exists an

218
Diﬀerential Privacy and Machine Learning
m = poly(d, 1/α, log(1/β)) such that for every distribution D over
labeled examples, A takes as input m labeled examples drawn from D
and outputs a hypothesis f ∈C such that with probability 1 −β:
err(f, D) ≤min
f∗∈C err(f∗, D) + α
If minf∗∈C err(f∗, D) = 0, the learner is said to operate in the
realizable setting (i.e., there exists some function in the class which
perfectly labels the data). Otherwise, the learner is said to operate
in the agnostic setting. If A also has run time that is polynomial in
d, 1/α, and log(1/β), then the learner is said to be eﬃcient. If there is
an algorithm which PAC-learns C, then C is said to be PAC-learnable.
The above deﬁnition of learning allows the learner to have direct
access to labeled examples. It is sometimes also useful to consider mod-
els of learning in which the algorithm only has oracle access to some
noisy information about D.
Deﬁnition 11.4. A statistical query is some function φ : X × {0, 1} →
[0, 1]. A statistical query oracle for a distribution over labeled exam-
ples D with tolerance τ is an oracle Oτ
D such that for every statistical
query φ:
Oτ
D(φ) −E(x,y)∼D[φ(x, y)]
 ≤τ
In other words, an SQ oracle takes as input a statistical query φ, and
outputs some value that is guaranteed to be within ±τ of the expected
value of φ on examples drawn from D.
The statistical query model of learning was introduced to model
the problem of learning in the presence of noise.
Deﬁnition 11.5. An algorithm A is said to SQ-learn a class of func-
tions C over d dimensions if for every α, β > 0 there exists an
m = poly(d, 1/α, log(1/β)) such that A makes at most m queries of tol-
erance τ = 1/m to Oτ
D, and with probability 1−β, outputs a hypothesis
f ∈C such that:
err(f, D) ≤min
f∗∈C err(f∗, D) + α

11.1
Sample complexity of diﬀerentially private machine learning
219
Note that an SQ learning algorithm does not get any access to
D except through the SQ oracle. As with PAC learning, we can talk
about an SQ learning algorithm operating in either the realizable or
the agnostic setting, and talk about the computational eﬃciency of the
learning algorithm. We say that a class C is SQ learnable if there exists
an SQ learning algorithm for C.
11.1
The sample complexity of diﬀerentially private
machine learning
Perhaps the ﬁrst question that one might ask, with respect to the
relationship between privacy and learning, is “When is it possible to
privately perform machine learning”? In other words, you might ask
for a PAC learning algorithm that takes as input a dataset (implic-
itly assumed to be sampled from some distribution D), and then
privately output a hypothesis f that with high probability has low
error over the distribution. A more nuanced question might be, “How
many additional samples are required to privately learn, as compared
with the number of samples already required to learn without the con-
straint of diﬀerential privacy?” Similarly, “How much additional run-
time is necessary to privately learn, as compared with the run-time
required to learn non-privately?” We will here brieﬂy sketch known
results for (ε, 0)-diﬀerential privacy. In general, better results for (ε, δ)-
diﬀerential privacy will follow from using the advanced composition
theorem.
A foundational information theoretic result in private machine
learning is that private PAC learning is possible with a polynomial num-
ber of samples if and only if non-private PAC learning is possible with
a polynomial number of samples, even in the agnostic setting. In fact,
the increase in sample complexity necessary is relatively small — how-
ever, this result does not preserve computational eﬃciency. One way to
do this is directly via the exponential mechanism. We can instantiate
the exponential mechanism with a range R = C, equal to the class of
queries to be learned. Given a database D, we can use the quality score
q(f, D) = −1
|D||{(x, y) ∈D : f(x) ̸= y}|: i.e., we seek to minimize the
fraction of misclassiﬁed examples in the private dataset. This is clearly

220
Diﬀerential Privacy and Machine Learning
a 1/n sensitive function of the private data, and so we have via our
utility theorem for the exponential mechanism that with probability
1−β, this mechanism returns a function f ∈C that correctly labels an
OPT −
2(log |C|+log 1
β )
εn
fraction of the points in the database correctly.
Recall, however, that in the learning setting, we view the database
D as consisting of n i.i.d. draws from some distribution over labeled
examples D. Recall the discussion of sampling bounds in Lemma 4.3.
A Chernoﬀbound combined with a union bound tells us that with high
probability, if D consists of n i.i.d. samples drawn from D, then for all
f ∈C: |err(f, D) −err(f, D)| ≤O(
q
log |C|
n
). Hence, if we wish to ﬁnd
a hypothesis that has error within α of the optimal error on the distri-
bution D, it suﬃces to draw a database D consisting of n ≥log |C|/α2
samples, and learn the best classiﬁer f∗on D.
Now consider the problem of privately PAC learning, using
the exponential mechanism as described above. Recall that, by
Theorem 3.11, it is highly unlikely that the exponential mechanism
will return a function f with utility score that is inferior to that of the
optimal f∗by more than an additive factor of O((∆u/ε) log |C|), where
in this case ∆u, the sensitivity of the utility function, is 1/n. That is,
with high probability the exponential mechanism will return a function
f ∈C such that:
err(f, D) ≤min
f∗∈C err(f∗, D) + O
(log |C|)
εn

≤min
f∗∈C err(f∗, D) + O


s
log |C|
n

+ O
(log |C|)
εn

.
Hence, if we wish to ﬁnd a hypothesis that has error within α of the
optimal error on the distribution D, it suﬃces to draw a database D
consisting of:
n ≥O

max
log |C|
εα
, log |C|
α2

,
which is not asymptotically any more than the database size that is
required for non-private learning, whenever ε ≥α.

11.1. Sample complexity of diﬀerentially private machine learning
221
A corollary of this simple calculation1 is that (ignoring computa-
tional eﬃciency), a class of functions C is PAC learnable if and only if
it is privately PAC learnable.
Can we say something stronger about a concept class C that is SQ
learnable? Observe that if C is eﬃciently SQ learnable, then the learn-
ing algorithm for C need only access the data through an SQ oracle,
which is very amenable to diﬀerential privacy: note that an SQ oracle
answers an expectation query deﬁned over a predicate φ(x, y) ∈[0, 1],
E(x,y)∼D[φ(x, y)], which is only 1/n sensitive when estimated on a
database D which is a sample of size n from D. Moreover, the learning
algorithm does not need to receive the answer exactly, but can be run
with any answer a that has the property that: |E(x,y)∼D[φ(x, y)]−a| ≤τ:
that is, the algorithm can be run using noisy answers on low sensitivity
queries. The beneﬁt of this is that we can answer such queries computa-
tionally eﬃciently, using the Laplace mechanism — but at the expense
of requiring a potentially large sample size. Recall that the Laplace
mechanism can answer m 1/n sensitive queries with (ε, 0)-diﬀerential
privacy and with expected worst-case error α = O(m log m
εn
). Therefore,
an SQ learning algorithm which requires the answers to m queries with
accuracy α can be run with a sample size of n = O(max(m log m
εα
, log m
α2 )).
Let us compare this to the sample size required for a non-private SQ
learner. If the SQ learner needs to make m queries to tolerance α, then
by a Chernoﬀbound and a union bound, a sample size of O(log m/α2)
suﬃces. Note that for ε = O(1) and error α = O(1), the non-private
algorithm potentially requires exponentially fewer samples. However,
at the error tolerance α ≤1/m as allowed in the deﬁnition of SQ learn-
ing, the sample complexity for private SQ learning is no worse than the
sample complexity for non-private SQ learning, for ϵ = Θ(1).
The upshot is that information theoretically, privacy poses very
little hinderance to machine learning. Moreover, for any algorithm that
accesses the data only though an SQ oracle,2 then the reduction to
1Together with corresponding lower bounds that show that for general C, it is
not possible to non-privately PAC learn using a sample with o(log |C|/α2) points.
2And in fact, almost every class (with the lone exception of parity functions) of
functions known to be PAC learnable is also learnable using only an SQ oracle.

222
Diﬀerential Privacy and Machine Learning
private learning is immediate via the Laplace mechanism, and preserves
computational eﬃciency!
11.2
Diﬀerentially private online learning
In this section, we consider a slightly diﬀerent learning problem, known
as the problem of learning from expert advice. This problem will appear
somewhat diﬀerent from the classiﬁcation problems that we discussed
in the previous section, but in fact, the simple algorithm presented here
is extremely versatile, and can be used to perform classiﬁcation among
many other tasks which we will not discuss here.
Imagine that you are betting on horse races, but unfortunately know
nothing about horses! Nevertheless, you have access to the opinions of
some k experts, who every day make a prediction about which horse is
going to win. Each day you can choose one of the experts whose advice
you will follow, and each day, following your bet, you learn which horse
actually won. How should you decide which expert to follow each day,
and how should you evaluate your performance? The experts are not
perfect (in fact they might not even be any good!), and so it is not
reasonable to expect you to make the correct bet all of the time, or
even most of the time if none of the experts do so. However, you might
have a weaker goal: can you bet on horses in such a way so that you
do almost as well as the best expert, in hindsight?
Formally, an online learning algorithm A operates in the following
environment:
1. Each day t = 1, . . . , T:
(a) A chooses an expert at ∈{1, . . . , k}
(b) A observes a loss ℓt
i ∈[0, 1] for each expert i ∈{1, . . . , k}
and experiences loss ℓt
at.
For a sequence of losses ℓ≤T ≡{ℓt}T
t=1, we write:
Li(ℓ≤T ) = 1
T
T
X
t=1
ℓt
i

11.2. Diﬀerentially private online learning
223
to denote the total average loss of expert i over all T rounds, and write
LA(ℓ≤T ) = 1
T
T
X
t=1
ℓt
at
to denote the total average loss of the algorithm.
The regret of the algorithm is deﬁned to be the diﬀerence between
the loss that it actually incurred, and the loss of the best expert in
hindsight:
Regret(A, ℓ≤T ) = LA(ℓ≤T ) −min
i
Li(ℓ≤T ).
The goal in online learning is to design algorithms that have the guaran-
tee that for all possible loss sequences ℓ≤T , even adversarialy chosen, the
regret is guaranteed to tend to zero as T →∞. In fact, this is possible
using the multiplicative weights algorithm (known also by many names,
e.g., the Randomized Weighted Majority Algorithm, Hedge, Exponen-
tiated Gradient Descent, and multiplicative weights being among the
most popular).
Remark 11.1. We have already seen this algorithm before in Section
4 — this is just the multiplicative weights update rule in another guise!
In fact, it would have been possible to derive all of the results about
the private multiplicative weights mechanism directly from the regret
bound we state in Theorem 11.1.
Algorithm 15 The Multiplicative Weights (or Randomized Weighted
Majority (RWM)) algorithm, version 1. It takes as input a stream of
losses ℓ1, ℓ2, . . . and outputs a stream of actions a1, a2, . . .. It is param-
eterized by an update parameter η.
RWM(η):
For each i ∈{1, . . . , k}, let wi ←1.
for t = 1, . . . do
Choose action at = i with probability proportional to wi
Observe ℓt and set wi ←wi · exp(−ηℓt
i), for each i ∈[k]
end for
It turns out that this simple algorithm already has a remarkable
regret bound.

224
Diﬀerential Privacy and Machine Learning
Theorem 11.1. For any adversarially chosen sequence of losses of
length T, ℓ≤T = (ℓ1, . . . , ℓT ) the Randomized Weighted Majority algo-
rithm with update parameter η has the guarantee that:
E[Regret(RWM(η), ℓ≤T )] ≤η + ln(k)
ηT ,
(11.1)
where k is the number of experts. Choosing η =
q
ln k
T
gives:
E[Regret(RWM(η), ℓ≤T )] ≤2
s
ln k
T .
This remarkable theorem states that even faced with an adversar-
ial sequence of losses, the Randomized Weighted Majority algorithm
can do as well, on average, as the best expert among k in hindsight,
minus only an additional additive term that goes to zero at a rate of
O(
q
ln k
T ). In other words, after at most T ≤4ln k
α2 rounds, the regret
of the randomized weighted majority algorithm is guaranteed to be at
most α! Moreover, this bound is the best possible.
Can we achieve something similar, but under the constraint of dif-
ferential privacy? Before we can ask this question, we must decide
what is the input database, and at what granularity we would like
to protect privacy? Since the input is the collection of loss vectors
ℓ≤T = (ℓ1, . . . , ℓT ), it is natural to view ℓ≤T as the database, and to
view a neighboring database
ˆ
ℓ≤T as one that diﬀers in the entire loss
vector in any single timestep: i.e., one in which for some ﬁxed timestep
t, ˆℓi = ℓi for all i ̸= t, but in which ℓt and ˆℓt can diﬀer arbitrarily.
The output of the algorithm is the sequence of actions that it chooses,
a1, . . . , aT , and it is this that we wish to be output in a diﬀerentially
private manner.
Our ﬁrst observation is that the randomized weighted majority algo-
rithm chooses an action at each day t in a familiar manner! We here
rephrase the algorithm in an equivalent way:
It
chooses
an
action
at
with
probability
proportional
to:
exp(−η Pt−1
j=1 ℓj
i), which is simply the exponential mechanism with qual-
ity score q(i, ℓ<T ) = Pt−1
j=1 ℓj
i, and privacy parameter ε = 2η. Note that
because each ℓt
i ∈[0, 1], the quality function has sensitivity 1. Thus,

11.2. Diﬀerentially private online learning
225
Algorithm 16 The Multiplicative Weights (or Randomized Weighted
Majority (RWM)) algorithm, rephrased. It takes as input a stream of
losses ℓ1, ℓ2, . . . and outputs a stream of actions a1, a2, . . .. It is param-
eterized by an update parameter η.
RWM(η):
for t = 1, . . . do
Choose
action
at
=
i
with
probability
proportional
to
exp(−η Pt−1
j=1 ℓj
i)
Observe ℓt
end for
each round t, the randomized weighted majority algorithm chooses an
action at in a way that preserves 2η diﬀerential privacy, so to achieve
privacy ε it suﬃces to set η = ε/2.
Moreover, over the course of the run of the algorithm, it will choose
an action T times. If we want the entire run of the algorithm to be
(ε, δ)-diﬀerentially private for some ε and δ, we can thus simply apply
our composition theorems. Recall that by Theorem 3.20, since there
are T steps in total, if each step of the algorithm is (ε′, 0)-diﬀerentially
private for ε′ = ε/
p
8T ln(1/δ), then the entire algorithm will be (ε, δ)
diﬀerentially private. Thus, the following theorem is immediate by set-
ting η = ε′/2:
Theorem 11.2. For a sequence of losses of length T, the algorithm
RWM(η) with η =
ε
√
32T ln(1/δ) is (ε, δ)-diﬀerentially private.
Remarkably, we get this theorem without modifying the original
randomized weighted majority algorithm at all, but rather just by set-
ting η appropriately. In some sense, we are getting privacy for free!
We can therefore use Theorem 11.1, the utility theorem for the RWM
algorithm, without modiﬁcation as well:
Theorem 11.3. For any adversarially chosen sequence of losses of
length T, ℓ≤T
= (ℓ1, . . . , ℓT ) the Randomized Weighted Majority

226
Diﬀerential Privacy and Machine Learning
algorithm with update parameter η =
ε
√
32T ln(1/δ) has the guarantee
that:
E[Regret(RWM(η), ℓ≤T )] ≤
ε
p
32T ln(1/δ) +
p
32 ln(1/δ) ln k
ε
√
T
≤
p
128 ln(1/δ) ln k
ε
√
T
,
where k is the number of experts.
Since the per-round loss at each time step t is an independently
chosen random variable (over the choices of at) with values bounded in
[−1, 1], we can also apply a Chernoﬀbound to get a high probability
guarantee:
Theorem 11.4. For any adversarially chosen sequence of losses of
length T, ℓ≤T = (ℓ1, . . . , ℓT ) the Randomized Weighted Majority algo-
rithm with update parameter η =
ε
√
32T ln(1/δ) produces a sequence of
actions such that with probability at least 1 −β:
Regret(RWM(η), ℓ≤T ) ≤
p
128 ln(1/δ) ln k
ε
√
T
+
s
ln k/β
T
= O
 p
ln(1/δ) ln(k/β)
ε
√
T
!
.
This bound is nearly as good as the best possible bound achievable
even without privacy (i.e., the RWM bound) — the regret bound is
larger only by a factor of Ω(
√
ln(k) ln(1/δ)
ε
). (We note that by using a
diﬀerent algorithm with a more careful analysis, we can remove this
extra factor of
√
ln k). Since we are in fact using the same algorithm,
eﬃciency is of course preserved as well. Here we have a powerful exam-
ple in machine learning where privacy is nearly “free.” Notably, just as
with the non-private algorithm, our utility bound only gets better the
longer we run the algorithm, while keeping the privacy guarantee the
same.3
3Of course, we have to set the update parameter appropriately, just as we have
to do with the non-private algorithm. This is easy when the number of rounds T is
known ahead of time, but can also be done adaptively when the number of rounds
is not known ahead of time.

11.3. Empirical risk minimization
227
11.3
Empirical risk minimization
In this section, we apply the randomized weighted majority algorithm
discussed in the previous section to a special case of the problem of
empirical risk minimization to learn a linear function. Rather than
assuming an adversarial model, we will assume that examples are drawn
from some known distribution, and we wish to learn a classiﬁer from
some ﬁnite number of samples from this distribution so that our loss
will be low on new samples drawn from the same distribution.
Suppose that we have a distribution D over examples x ∈[−1, 1]d,
and for each such vector x ∈[−1, 1]d, and for each vector θ ∈[0, 1]d with
∥θ∥1 = 1, we deﬁne the loss of θ on example x to be Loss(θ, x) = ⟨θ, x⟩.
We wish to ﬁnd a vector θ∗to minimize the expected loss over examples
drawn from D:
θ∗= arg
min
θ∈[0,1]d:∥θ∥1=1 Ex∼D[⟨θ, x⟩].
This problem can be used to model the task of ﬁnding a low error linear
classiﬁer. Typically our only access to the distribution D is through
some collection of examples S ⊂[−1, 1]d drawn i.i.d. from D, which
serves as the input to our learning algorithm. We will here think of this
sample S as our private database, and will be interested in how well
we can privately approximate the error of θ∗as a function of |S| (the
sample complexity of the learning algorithm).
Our approach will be to reduce the problem to that of learning
with expert advice, and apply the private version of the randomized
weighted majority algorithm as discussed in the last section:
1. The experts will be the d standard basis vectors {e1, . . . , ed},
where ei = (0, . . . , 0, 1
|{z}
i
, 0, . . . , 0).
2. Given an example x ∈[−1, 1]d, we deﬁne a loss vector ℓ(x) ∈
[−1, 1]d by setting ℓ(x)i = ⟨ei, x⟩for each i ∈{1, . . . , d}. In other
words, we simply set ℓ(x)i = xi.
3. At time t, we choose a loss function ℓt by sampling x ∼D and
setting ℓt = ℓ(x).

228
Diﬀerential Privacy and Machine Learning
Note that if we have a sample S from D of size |S| = T, then we can run
the RWM algorithm on the sequence of losses as described above for a
total of T rounds. This will produce a sequence of outputs a1, . . . , aT ,
and we will deﬁne our ﬁnal classiﬁer to be θT ≡
1
T
PT
i=1 ai. (Recall
that each ai is a standard basis vector ai ∈{e1, . . . , ed}, and so we
have ∥θT ∥1 = 1).
We summarize the algorithm below:
Algorithm 17 An algorithm for learning linear functions. It takes as
input a private database of examples S ⊂[−1, 1]d, S = (x1, . . . , xT ),
and privacy parameters ε and δ.
LinearLearner(S, ε, δ):
Let η ←
ε
√
32T ln(1/δ)
for t = 1 to T = |S| do
Choose
vector at
=
ei
with
probability
proportional
to
exp(−η Pt−1
j=1 ℓj
i)
Let loss vector ℓt = (⟨e1, xt⟩, ⟨e2, xt⟩, . . . , ⟨ed, xt⟩).
end for
Output θT = 1
T
PT
t=1 at.
We have already seen that LinearLearner is private, since it is sim-
ply an instantiation of the randomized weighted majority algorithm
with the correct update parameter η:
Theorem 11.5. LinearLearner(S, ε, δ) is (ε, δ)-diﬀerentially private.
It remains to analyze the classiﬁcation accuracy of LinearLearner,
which amounts to considering the regret bound of the private RWM
algorithm.
Theorem 11.6. If S consists of T i.i.d. samples x ∼D, then with
probability at least 1−β, LinearLearner outputs a vector θT such that:
Ex∼D[⟨θT , x⟩] ≤min
θ∗Ex∼D[⟨θ∗, x⟩] + O
 p
ln(1/δ) ln(d/β)
ε
√
T
!
,
where d is the number of experts.

11.3. Empirical risk minimization
229
Proof. By Theorem 11.4, we have the following guarantee with proba-
bility at least 1 −β/2:
1
T
T
X
t=1
⟨at, xt⟩≤
min
i∈{1,...,d}
*
ei, 1
T
T
X
t=1
xt
+
+ O
 p
ln(1/δ) ln(d/β)
ε
√
T
!
=
min
θ∗∈[0,1]d:∥θ∗∥1=1
*
θ∗, 1
T
T
X
t=1
xt
+
+ O
 p
ln(1/δ) ln(d/β)
ε
√
T
!
.
In the ﬁrst equality, we use the fact that the minimum of a linear func-
tion over the simplex is achieved at a vertex of the simplex. Noting that
each xt ∼D independently and that each ⟨xt, ei⟩is bounded in [−1, 1],
we can apply Azuma’s inequality twice to bound the two quantities
with probability at least 1 −β/2:

1
T
T
X
t=1
⟨at, xt⟩−1
T
T
X
t=1
Ex∼D⟨at, x⟩

=

1
T
T
X
t=1
⟨at, xt⟩−Ex∼D⟨θT , x⟩
 ≤O


s
ln(1/β)
T


and
max
i∈{1,...,d}

*
ei, 1
T
T
X
t=1
xt
+
−Ex∼D⟨ei, x⟩
 ≤O


s
ln(d/β)
T

.
Hence we also have:
max
θ∗∈[0,1]d:∥θ∗∥1=1

*
θ∗, 1
T
T
X
t=1
xt
+
−Ex∼D⟨θ∗, x⟩
 ≤O


s
ln d/β
T

.
Combining these inequalities gives us our ﬁnal result about the output
of the algorithm θT :
Ex∼D⟨θT , x⟩≤
min
θ∗∈[0,1]d:∥θ∗∥1=1 Ex∼D⟨θ∗, x⟩+ O
 p
ln(1/δ) ln(d/β)
ε
√
T
!
.

230
Diﬀerential Privacy and Machine Learning
11.4
Bibliographical notes
The PAC model of machine learning was introduced by Valiant in 1984
[83], and the SQ model was introduced by Kearns [53]. The randomized
weighted majority algorithm is originally due to Littlestone and War-
muth [57], and has been studied in many forms. See Blum and Mansour
[9] or Arora et al. [1] for a survey. The regret bound that we use for
the randomized weighted majority algorithm is given in [1].
Machine learning was one of the ﬁrst topics studied in diﬀerential
privacy, beginning with the work of Blum et al. [7], who showed that
algorithms that operate in the SQ-learning framework could be con-
verted into privacy preserving algorithms. The sample complexity of
diﬀerentially private learning was ﬁrst considered by Kasiviswanathan,
Lee, Nissim, Raskhodnikova, and Smith, “What can we Learn Pri-
vately?” [52], which characterize the sample complexity of private learn-
ing up to polynomial factors. For more reﬁned analysis of the sample
complexity of private learning, see [3, 4, 12, 19].
There
is
also
extensive
work
on
eﬃcient
machine
learning
algorithms, including the well known frameworks of SVMs and empir-
ical risk minimizers [13, 55, 76]. Spectral learning techniques, includ-
ing PCA and low rank matrix approximation have also been studied
[7, 14, 33, 42, 43, 51].
Private learning from expert advice was ﬁrst considered by Dwork
et al. [26]. The fact that the randomized weighted majority algorithm is
privacy preserving without modiﬁcation (when the update parameter
is set appropriately) is folklore (following from advanced composition
[32]) and has been widely used; for example, in [48]. For a more general
study of private online learning, see [50], and for a more general study
of empirical risk minimization, see [50, 13].

12
Additional Models
So far, we have made some implicit assumptions about the model of
private data analysis. For example, we have assumed that there is some
trusted curator who has direct access to the private dataset, and we
have assumed that the adversary only has access to the output of the
algorithm, not to any of its internal state during its execution. But what
if this is not the case? What if we trust no one to look at our data,
even to perform the privacy preserving data analysis? What if some
hacker might gain access to the internal state of the private algorithm
while it is running? In this section, we relax some of our previously
held assumptions and consider these questions.
In this section we describe some additional computational models
that have received attention in the literature.
• The local model is a generalization of randomized response (see
Section 2), and is motivated by situations in which individuals do
not trust the curator with their data. While this lack of trust can
be addressed using secure multiparty computation to simulate the
role played by the trusted curator, there are also some techniques
that do not require cryptography.
231

232
Additional Models
The next two models consider streams of events, each of which may be
associated with an individual. For example, an event may be a search
by a particular person on an arbitrary term. In a given event stream,
the (potentially many) events associated with a given individual can be
arbitrarily interleaved with events associated with other individuals.
• In pan-privacy the curator is trusted, but may be subject to com-
pulsory non-private data release, for example, because of a sub-
poena, or because the entity holding the information is purchased
by another, possibly less trustworthy, entity. Thus, in pan-privacy
the internal state of the algorithm is also diﬀerentially private, as
is the joint distribution of the internal state and the outputs.
• The continual observation model addresses the question of main-
taining privacy when the goal is to continually monitor and report
statistics about events, such as purchases of over-the-counter
medications that might be indicative of an impending epidemic.
Some work addresses pan-privacy under continual observation.
12.1
The local model
So far, we have considered a centralized model of data privacy, in which
there exists a database administrator who has direct access to the pri-
vate data. What if there is instead no trusted database administrator?
Even if there is a suitable trusted party, there are many reasons not to
want private data aggregated by some third party. The very existence
of an aggregate database of private information raises the possibility
that at some future time, it will come into the hands of an untrusted
party, either maliciously (via data theft), or as a natural result of orga-
nizational succession. A superior model — from the perspective of the
owners of private data — would be a local model, in which agents could
(randomly) answer questions in a diﬀerentially private manner about
their own data, without ever sharing it with anyone else. In the con-
text of predicate queries, this seems to severely limit the expressivity of
a private mechanism’s interaction with the data: The mechanism can
ask each user whether or not her data satisﬁes a given predicate, and

12.1. The local model
233
the user may ﬂip a coin, answering truthfully only with slightly higher
probability than answering falsely. In this model what is possible?
The local privacy model was ﬁrst introduced in the context of learn-
ing. The local privacy model formalizes randomized response: there is
no central database of private data. Instead, each individual maintains
possession of their own data element (a database of size 1), and answers
questions about it only in a diﬀerentially private manner. Formally, the
database x ∈N|X| is a collection of n elements from some domain X,
and each xi ∈x is held by an individual.
Deﬁnition 12.1 (Local Randomizer). An ε-local randomizer R : X →
W is an ε-diﬀerentially private algorithm that takes as input a database
of size n = 1.
In the local privacy model, algorithms may interact with the
database only through a local randomizer oracle:
Deﬁnition 12.2 (LR Oracle). An LR oracle LRD(·, ·) takes as input an
index i ∈[n] and an ε-local randomizer R and outputs a random value
w ∈W chosen according to the distribution R(xi), where xi ∈D is the
element held by the ith individual in the database.
Deﬁnition 12.3 ((Local Algorithm)). An algorithm is ε-local if it
accesses the database D via the oracle LRD, with the following restric-
tion: If LRD(i, R1), . . . , LRD(i, Rk) are the algorithm’s invocations
of LRD on index i, where each RJ is an εj-local randomizer, then
ε1 + · · · + εk ≤ε.
Because diﬀerential privacy is composable, it is easy to see that
ε-local algorithms are ε-diﬀerentially private.
Observation 12.1. ε-local algorithms are ε-diﬀerentially private.
That is to say, an ε-local algorithm interacts with the data using
only a sequence of ε-diﬀerentially private algorithms, each of which
computes only on a database of size 1. Because nobody other than its
owner ever touches any piece of private data, the local setting is far
more secure: it does not require a trusted party, and there is no central
party who might be subject to hacking. Because even the algorithm

234
Additional Models
never sees private data, the internal state of the algorithm is always
diﬀerentially private as well (i.e., local privacy implies pan privacy,
described in the next section). A natural question is how restrictive
the local privacy model is. In this section, we merely informally discuss
results. The interested reader can follow the bibliographic references at
the end of this section for more information. We note that an alternative
name for the local privacy model is the fully distributed model.
We recall the deﬁnition of the statistical query (SQ) model, intro-
duced in Section 11. Roughly speaking, given a database x of size n,
the statistical query model allows an algorithm to access this database
by making a polynomial (in n) number of noisy linear queries to the
database, where the error in the query answers is some inverse polyno-
mial in n. Formally:
Deﬁnition 12.4. A statistical query is some function φ : X × {0, 1} →
[0, 1]. A statistical query oracle for a distribution over labeled exam-
ples D with tolerance τ is an oracle Oτ
D such that for every statistical
query φ:
Oτ
D(φ) −E(x,y)∼D[φ(x, y)]
 ≤τ
In other words, an SQ oracle takes as input a statistical query φ, and
outputs some value that is guaranteed to be within ±τ of the expected
value of φ on examples drawn from D.
Deﬁnition 12.5. An algorithm A is said to SQ-learn a class of functions
C if for every α, β > 0 there exists an m = poly(d, 1/α, log(1/β)) such
that A makes at most m queries of tolerance τ = 1/m to Oτ
D, and with
probability 1 −β, outputs a hypothesis f ∈C such that:
err(f, D) ≤min
f∗∈C err(f∗, D) + α
More generally, we can talk about an algorithm (for performing any
computation) as operating in the SQ model if it accesses the data only
through an SQ oracle:
Deﬁnition 12.6. An algorithm A is said to operate in the SQ model
if there exists an m such that A makes at most m queries of tolerance
τ = 1/m to Oτ
D, and does not have any other access to the database.
A is eﬃcient if m is polynomial in the size of the database, D.

12.1. The local model
235
It turns out that up to polynomial factors in the size of the database
and in the number of queries, any algorithm that can be implemented
in the SQ model can be implemented and analyzed for privacy in the
local privacy model, and vice versa. We note that there is a distinction
between an algorithm being implemented in the SQ model, and its
privacy analysis being carried out in the local model: almost all of
the algorithms that we have presented in the end access the data using
noisy linear queries, and so can be thought of as acting in the SQ model.
However, their privacy guarantees are analyzed in the centralized model
of data privacy (i.e., because of some “global” part of the analysis, as
in the sparse vector algorithm).
In the following summary, we will also recall the deﬁnition of PAC
learning, also introduced in Section 11:
Deﬁnition 12.7. An algorithm A is said to PAC-learn a class of func-
tions C if for every α, β > 0, there exists an m = poly(d, 1/α, log(1/β))
such that for every distribution D over labeled examples, A takes as
input m labeled examples drawn from D and outputs a hypothesis
f ∈C such that with probability 1 −β:
err(f, D) ≤min
f∗∈C err(f∗, D) + α
If minf∗∈C err(f∗, D) = 0, the learner is said to operate in the
realizable setting (i.e., there exists some function in the class which
perfectly labels the data). Otherwise, the learner is said to operate
in the agnostic setting. If A also has run time that is polynomial in
d, 1/α, and log(1/β), then the learner is said to be eﬃcient. If there is
an algorithm which PAC-learns C, then C is said to be PAC-learnable.
Note that the main distinction between an SQ learning algorithm and a
PAC learning algorithm, is that the PAC learning algorithm gets direct
access to the database of examples, whereas the SQ learning algorithm
only has access to the data through a noisy SQ oracle.
What follows is some of our understanding of the limitations of the
SQ model and problems which separate it from the centralized model
of data privacy.

236
Additional Models
1. A single sensitivity-1 query can be answered to error O(1) in the
centralized model of data privacy using the Laplace mechanism,
but requires error Θ(√n) in the local data privacy model.
2. The set of function classes that we can (properly) learn in the
local privacy model is exactly the set of function classes that we
can properly learn in the SQ model (up to polynomial factors
in the database size and query complexity of the algorithm). In
contrast, the set of things we can (properly or agnostically) learn
in the centralized model corresponds to the set of things we can
learn in the PAC model. SQ learning is strictly weaker, but this
is not a huge handicap, since parity functions are essentially the
only interesting class that is PAC learnable but not SQ learnable.
We remark that we refer explicitly to proper learning here (mean-
ing the setting in which there is some function in the class which
perfectly labels the data). In the PAC model there is no informa-
tion theoretic diﬀerence between proper and agnostic learning,
but in the SQ model the diﬀerence is large: see the next point.
3. The set of queries that we can release in the local privacy model
are exactly those queries that we can agnostically learn in the SQ
model. In contrast, the set of things we can release in the central-
ized model corresponds to the set of things we can agnostically
learn in the PAC model. This is a much bigger handicap — even
conjunctions (i.e., marginals) are not agnostically learnable in the
SQ model. This follows from the information theoretic reduction
from agnostic learning (i.e., distinguishing) to query release that
we saw in Section 5 using the iterative construction mechanism.
We note that if we are only concerned about computationally bounded
adversaries, then in principle distributed agents can use secure mul-
tiparty computation to simulate private algorithms in the centralized
setting. While this does not actually give a diﬀerential privacy guar-
antee, the result of such simulations will be indistinguishable from the
result of diﬀerentially private computations, from the point of view of
a computationally bounded adversary. However, general secure mul-
tiparty computation protocols typically require huge amounts of mes-
sage passing (and hence sometimes have unreasonably large run times),

12.2. Pan-private streaming model
237
whereas algorithms in the local privacy model tend to be extremely
simple.
12.2
Pan-private streaming model
The goal of a pan-private algorithm is to remain diﬀerentially private
even against an adversary that can, on rare occasions, observe the algo-
rithm’s internal state. Intrusions can occur for many reasons, including
hacking, subpoena, or mission creep, when data collected for one pur-
pose are used for a diﬀerent purpose (“Think of the children!”). Pan-
private streaming algorithms provide protection against all of these.
Note that ordinary streaming algorithms do not necessarily provide
privacy against intrusions, as even a low-memory streaming algorithm
can hold a small number of data items in memory, which would be
completely exposed in an intrusion. On the technical side, intrusions
can be known to the curator (subpoena) or unknown (hacking). These
can have very diﬀerent eﬀects, as a curator aware of an intrusion can
take protective measures, such as re-randomizing certain variables.
12.2.1
Deﬁnitions
We assume a data stream of unbounded length composed of elements
in a universe X. It may be helpful to keep in mind as motivation data
analysis on a query stream, in which queries are accompanied by the
IP address of the issuer. For now, we ignore the query text itself; the
universe X is the universe of potential IP addresses. Thus, intuitively,
user-level privacy protects the presence or absence of an IP address
in the stream, indpendent of the number of times it arises, should
it actually be present at all. In contrast, event-level privacy merely
protects the privacy of individual accesses. For now, we focus on user-
level privacy.
As usual in diﬀerentially private algorithms, the adversary can have
arbitrary control of the input stream, and may have arbitrary auxil-
iary knowledge obtained from other sources. It can also have arbitrary
computational power.

238
Additional Models
We assume the algorithm runs until it receives a special signal,
at which point it produces (observable) outputs. The algorithm may
optionally continue to run and produce additional outputs later, again
in response to a special signal. Since outputs are obvservable we do not
provide privacy for the special signals.
A streaming algorithm experiences a sequence of internal states.
and produces a (possibly unbounded) sequence of outputs. Let I denote
the set of possible internal states of the algorithm, and σ the set of
possible output sequences. We assume that the adversary can only
observe internal states and the output sequence; it cannot see the
data in the stream (although it may have auxiliary knowledge about
some of these data) and it has no access to the length of the input
sequence.
Deﬁnition 12.8 (X-Adjacent Data Streams). We think of data streams
as being of unbounded length; preﬁxes have ﬁnite length. Data streams
S and S′ are X-adjacent if they diﬀer only in the presence or absence
of all occurrences of a single element u ∈X. We deﬁne X-adjacency
for stream preﬁxes analogously.
User-Level Pan-Privacy.
An algorithm Alg mapping data stream
preﬁxes to the range I × σ, is pan-private against a single intrusion
if for all sets I′ ⊆I of internal states and σ′ ⊆σ of output sequences,
and for all pairs of adjacent data stream preﬁxes S, S′
Pr[Alg(S) ∈(I′, σ′)] ≤eε Pr[Alg(S′) ∈(I′, σ′)],
where the probability spaces are over the coin ﬂips of the algorithm
Alg.
This deﬁnition speaks only of a single intrusion. For multiple intru-
sions we must consider interleavings of observations of internal states
and outputs.
The relaxation to event-level privacy is obtained by modifying the
notion of adjacency so that, roughly speaking, two streams are event-
adjacent if they diﬀer in a single instance of a single element in X; that
is, one instance of one element is deleted/added. Clearly, event-level
privacy is a much weaker guarantee than user-level privacy.

12.2. Pan-private streaming model
239
Remark 12.1. If we assume the existence of a very small amount of
secret storage, not visible to the adversary, then many problems for
which we have been unable to obtain pan-private solutions have (non-
pan-) private streaming solutions. However, the amount of secret stor-
age is not so important as its existence, since secret storage is vulnerable
to the social pressures against which pan-privacy seeks to protect the
data (and the curator).
Pan-Private Density Estimation.
Quite surprisingly, pan-privacy can
be achieved even for user-level privacy of many common streaming com-
putations. As an example, consider the problem of density estimation:
given a universe X of data elements and a stream σ, the goal is to
estimate the fraction of X that acutally appears in the stream. For
example, the universe consists of all teenagers in a given community
(represented by IP addresses), and the goal is to understand what frac-
tion visit the Planned Parenthood website.
Standard low-memory streaming solutions for density estimation
involve recording the results of deterministic computations of at least
some input items, an approach that is inherently not pan-private.
Here is a simple, albeit high-memory, solution inspired by random-
ized response. The algorithm maintains a bit ba for each IP address a
(which may appear any number of times in the stream), initialized uni-
formly at random. The stream is processed one element at a time. On
input a the algorithm ﬂips a bit biased to 1; that is, the biased bit
will take value 0 with probability 1/2 −ε, and value 1 with probabil-
ity 1/2 + ε. The algorithm follows this procedure independent of the
number of times IP address a appears in the data stream. This algo-
rithm is (ε, 0)-diﬀerentially private. As with randomized response, we
can estimate the fraction of “real” 1’s by z = 2(y −|X|/2)/|X|, where y
is the actual number of 1’s in the table after the stream is processed.
To ensure pan-privacy, the algorithm publishes a noisy version of z. As
with randomized response, the error will be on the order of 1/
p
|X|,
yielding meaningful results when the density is high.
Other problems enjoying user-level pan-private algorithms include:
• Estimating, for any t, the fraction of elements appearing exactly
t times;

240
Additional Models
• Estimating the t-cropped mean: roughly, the average, over all ele-
ments, of the minimum of t and the number of occurrences of the
element in the data stream;
• Estimating the fraction of k-heavy hitters (elements of X that
appear at least k times in the data stream).
Variants of these problems can also be deﬁned for fully dynamic data, in
which counts can be decremented as well as incremented. For example,
density estimation (what fraction appeared in the stream?) becomes
“How many (or what fraction) of elements have a (net) count equal to
zero?” These, too, can be solved with user-level pan-privacy, using dif-
ferentially private variations of sketching techniques from the streaming
literature.
12.3
Continual observation
Many applications of data analysis involve repeated computations,
either because the entire goal is one of monitoring of, for example, traf-
ﬁc conditions, search trends, or incidence of inﬂuenza. In such applica-
tions the system is required to continually produce outputs. We there-
fore need techniques for achieving diﬀerential privacy under continual
observation.
As usual, diﬀerential privacy will require having essentially the same
distribution on outputs for each pair of adjacent databases, but how
should we deﬁne adjacency in this setting? Let us consider two example
scenarios.
Suppose the goal is to monitor public health by analyzing statis-
tics from an H1N1 self-assessment Web site.1 Individuals can inter-
act with the site to learn whether symptoms they are experiencing
may be indicative of the H1N1 ﬂu. The user ﬁlls in some demographic
data (age, zipcode, sex), and responds to queries about his symptoms
(fever over 100.4◦F?, sore throat?, duration of symptoms?). We would
expect a given individual to interact very few times with the H1N1
self-assessment site (say, if we restrict our attention to a six-month
1https://h1n1.cloudapp.net provided such a service during the winter of 2010;
user-supplied data were stored for analysis with the user’s consent.

12.3. Continual observation
241
period). For simplicity, let us say this is just once. In such a setting,
it is suﬃcient to ensure event-level privacy, in which the privacy goal
is to hide the presence or absence of a single event (interaction of one
user with the self-assessment site).
Suppose again that the goal is to monitor public health, this time
by analyzing search terms submitted to a medical search engine. Here
it may no longer be safe to assume an individual has few interactions
with the Web site, even if we restrict attention to a relatively short
period of time. In this case we would want user-level privacy, ensuring
that the entire set of a user’s search terms is protected simultaneously.
We think of continual observation algorithms as taking steps at
discrete time intervals; at each step the algorithm receives an input,
computes, and produces output. We model the data as arriving in a
stream, at most one data element in each time interval. To capture the
fact that, in real life, there are periods of time in which nothing hap-
pens, null events are modeled by a special symbol in the data stream.
Thus, the intuitive notion of “t time periods” corresponds to processing
a sequence of t elements in the stream.
For example, the motivation behind the counter primitive below is
to count the number of times that something has occurred since the
algorithm was started (the counter is very general; we don’t specify
a priori what it is counting). This is modeled by an input stream over
{0, 1}. Here, “0” means “nothing happened,” “1” means the event of
interest occurred, and for t = 1, 2, . . . , T the algorithm outputs an
approximation to the number of 1s seen in the length t preﬁx of the
stream.
There are three natural options:
1. Use randomized response for each time period and add this ran-
domized value to the counter;
2. Add noise distributed according to Lap(1/ε) to the true value for
each time step and add this perturbed value to the counter;
3. Compute the true count at each time step, add noise distributed
according to Lap(T/ε) to the count, and release this noisy count.
All of these options result in noise on the order of at least Ω(
√
T/ε).
The hope is to do much better by exploiting structure of the query set.

242
Additional Models
Let X be the universe of possible input symbols. Let S and S′ be
stream preﬁxes (i.e., ﬁnite streams) of symbols drawn from X. Then
Adj(S, S′) (“S is adjacent to S′”) if and only if there exist a, b ∈X
so that if we change some of the instances of a in S to instances of b,
then we get S′. More formally, Adj(S, S′) iﬀ∃a, b ∈X and ∃R ⊆[|S|],
such that S|R:a→b = S′. Here, R is a set of indices in the stream preﬁx
S, and S|R:a→b is the result of replacing all the occurrences of a at
these indices with b. Note that adjacent preﬁxes are always of the same
length.
To capture event-level privacy, we restrict the deﬁnition of adja-
cency to the case |R| ≤1. To capture user-level privacy we do not
constrain the size of R in the deﬁnition of adjacency.
As noted above, one option is to publish a noisy count at each time
step; the count published at time t reﬂects the approximate number
of 1s in the length t preﬁx of the stream. The privacy challenge is
that early items in the stream are subject to nearly T statistics, so
for (ε, 0)-diﬀerential privacy we would be adding noise scaled to T/ε,
which is unacceptable. In addition, since the 1s are the “interesting”
elements of the stream, we would like that the distortion be scaled to
the number of 1s seen in the stream, rather than to the length of the
stream. This rules out applying randomized response to each item in
the stream independently.
The algorithm below follows a classical approach for converting
static algorithms to dynamic algorithms.
Assume T is a power of 2. The intervals are the natural ones cor-
responding to the labels on a complete binary tree with T leaves,
where the leaves are labeled, from left to right, with the intervals
[0, 0], [1, 1], . . . , [T −1, T −1] and each parent is labeled with the inter-
val that is the union of the intervals labeling its children. The idea is
to compute and release a noisy count for each label [s, t]; that is, the
released value corresponding to the label [s, t] is a noisy count of the
number of 1s in positions s, s + 1, . . . , t of the input stream. To learn
the approximate cumulative count at time t ∈[0, T −1] the analyst
uses the binary representation of t to determine a set of at most log2 T

12.3. Continual observation
243
Figure 12.1: Event-level private counter algorithm (not pan-private).
disjoint intervals whose union is [0, t], and computes the sum of the
corresponding released noisy counts.2 See Figure 12.1.
Each stream position t ∈[0, T −1] appears in at most 1 + log2 T
intervals (because the height of the tree is log2 T), and so each ele-
ment in the stream aﬀects at most 1 + log2 T released noisy counts.
Thus, adding noise to each interval count distributed according to
Lap((1 + log2 T)/ε) ensures (ε, 0)-diﬀerential privacy. As for accuracy,
since the binary representation of any index t ∈[0, T −1] yields a dis-
joint set of at most log2 T intervals whose union is [0, t] we can apply
Lemma 12.2 below to conclude that the expected error is tightly con-
centrated around (log2 T)3/2. The maximum expected error, over all
times t, is on the order of (log2 T)5/3.
Lemma 12.2. Let Let Y1, . . . , Yk be independent variables with distri-
bution Lap(bi). Let Y = P
i Yi and bmax = maxi bi. Let ν ≥
pP
i(bi)2,
and 0 < λ < 2
√
2ν2
bmax . Then
Pr[Y > λ] ≤exp
 
−λ2
8ν2
!
.
2This algorithm can be optimized slightly (for example, we never use the count
corresponding to the root, eliminating one level from the tree), and it can be modiﬁed
to handle the case in which T is not a power of 2 and, more interestingly, when T
is not known a priori.

244
Additional Models
Proof. The moment generating function of Yi is E[exp(hYi)] = 1/(1 −
h2b2
i ), where |h| < 1/bi. Using the inequality (1 −x)−1 ≤1 + 2x ≤
exp(2x) for 0 ≤x < 1/2, we have E[exp(hYi)] ≤exp(2h2b2
i ), if
|h| < 1/2bi. We now calculate, for 0 < h < 1/
√
2bmax:
Pr[Y > λ] = Pr[exp(hY ) > exp(hλ)]
≤exp(−hλ)E[exp(hY )]
= exp(−hλ)
Y
i
E[exp(hYi)]
≤exp(−hλ + 2h2ν2).
By assumption, 0 < λ <
2
√
2ν2
bmax . We complete the proof by setting
h = λ/4ν2 < 1/
√
2bmax.
Corollary 12.3. Let Y, ν, {bi}i, bmax be as in Lemma 12.2. For δ ∈
(0, 1) and ν > max{
qP
i b2
i , bmax
p
ln(2/δ)}, we have that Pr[|Y | >
ν
p
8 ln(2/δ)] ≤δ.
In our case, all the bi’s are the same (e.g., b = (log2 T)/ε). Taking
ν =
√
kb we have the following corollary:
Corollary 12.4. For all λ < α(
√
kb) < 2
√
2kb = 2
√
2kν,
Pr[Y > λ] ≤e−α2/8 .
Note that we have taken the unusual step of adding noise to the
count before counting, rather than after. In terms of the outputs it
makes no diﬀerence (addition is commutative). However, it has an inter-
esting eﬀect on the algorithm’s internal states: they are diﬀerentially
private! That is, suppose the intrusion occurs at time t, and consider
any i ∈[0, t]. Since there are at most log2 T intervals containing step i
(in the algorithm we abolished the interval corresponding to the root),
xi aﬀects at most log2 T of the noisy counts, and so xi is protected
against the intrusion for exactly the same reason that it is protected in
the algorithm’s outputs. Nevertheless, the algorithm in Figure 12.1 is
not pan-private even against a single intrusion. This is because, while
its internal state and its outputs are each independently diﬀerentially
private, the joint distribution does not ensure ε-diﬀerential privacy. To

12.3. Continual observation
245
see why this is so, consider an intruder that sees the internal state at
time t and knows the entire data stream except xt+1, and let I = [a, b]
be an interval containing both t and t + 1. Since the adversary knows
x[0,t], it can subtract from cI the contribution from the stream occur-
ring up through time t (that is, it subtracts oﬀfrom the observed cI at
time t the values xa, xa+1, . . . , xt, all of which it knows). From this the
intruder learns the value of the Laplace draw to which cI was initial-
ized. When cI is published at the end of step b, the adversary subtracts
from the published value this initial draw, together with the contribu-
tions of all elements in x[a,b] except xt+1, which it does not know. What
remains is the unknown xt+1.
12.3.1
Pan-private counting
Although the algorithm in Figure 12.1 is easily modiﬁed to ensure
event-level pan-privacy against a single intrusion, we give a diﬀerent
algorithm here in order to introduce a powerful bijection technique
which has proved useful in other applications. This algorithm main-
tains in its internal state a single noisy counter, or accumulator, as
well as noise values for each interval. The output at any given time
period t is the sum of the accumulator and the noise values for the
intervals containing t. When an interval I ends, its associated noise
value, ηI, is erased from memory.
Theorem 12.5. The counter algorithm of Figure 12.2, when run with
parameters T, ε, and suﬀering at most one intrusion, yields an (ε, 0)-
pan-private counter that, with probability at least 1 −β has maximum
error, over its T outputs, of O(log(1/β) · log2.5 T/ε). We note also that
in every round individually (rather than in all rounds simultaneously),
with all but β probability, the error has magnitude at most O(log(1/β)·
log1.5 T/ε).
Proof. The proof of accuracy is the same as that for the algorithm in
Figure 12.1, relying on Corollary 12.4. We focus here on the proof of
pan-privacy.
During an intrusion between atomic steps t∗and t∗+ 1, that is,
immediately following the processing of element t∗in the input stream

246
Additional Models
Figure 12.2: Event-level pan-private counter algorithm.
(recall that we begin numbering the elements with 0), the view of the
adversary consists of (1) the noisy cumulative count (in the variable
“count”), (2) the interval noise values ηS in memory when the intrusion
occurs, and (3) the complete sequence of all of the algorithm’s outputs
in rounds 0, 1, . . . , t. Consider adjacent databases x and x′, which diﬀer
in time t, say, without loss of generality, xt = 1 and x′
t = 0, and an
intrusion immediately following time period t∗≥t (we will discuss the
case t∗< t below). We will describe a bijection between the vector of
noise values used in executions on x and executions on x′, such that
corresponding noise values induce identical adversary views on x and
x′, and the probabilities of adjacent noise values diﬀer only by an eε
multiplicative factor. This implies ε-diﬀerential pan-privacy.
By assumption, the true count just after the time period t∗≥t
is larger when the input is x than it is when the input is x′. Fix an
arbitrary execution Ex when the input stream is x. This amounts to
ﬁxing the randomness of the algorithm, which in turn ﬁxes the noise
values generated. We will describe the corresponding execution Ex′ by
describing how its noise values diﬀer from those in Ex.
The program variable Counter was initialized with Laplace noise.
By increasing this noise by 1 in Ex′ the value of Counter just after step
t∗is identical in Ex′ and Ex. The noise variables in memory immedi-
ately following period t∗are independent of the input; these will be

12.3. Continual observation
247
unchanged in Ex′. We will make the sequence of outputs in Ex′ identi-
cal to those in Ex by changing a collection of log T interval noise values
ηS that are not in memory when the adversary intrudes, so that the
sum of all noise values in all rounds up through t −1 is unchanged,
but the sum from round t on is larger by 1 for database x′ than for x.
Since we increased the initialization noise for Counter, we now need to
decrease the sum of interval noise values for periods 0, . . . , t −1 by 1,
and leave unchanged the sum of interval noise values from period t.
To do this, we ﬁnd a collection of disjoint intervals whose union is
{0, . . . , t −1}. There is always such a collection, and it is always of size
at most log T. We can construct it iteratively by, for i decreasing from
⌊log(t −1)⌋to 0, choosing the interval of size 2i that is contained in
{0, . . . , t −1} and is not contained in a previously chosen interval (if
such an interval exists). Given this set of disjoint intervals, we notice
also that they all end by time t −1 < t ≤t∗, and so their noises are
not in memory when the adversary intrudes (just following period t∗).
In total (taking into account also changing the initial noise value for
Counter), the complete view seen by the adversary is identical and the
probabilities of the (collection of) noise values used for x and x′ diﬀer
by at most an eε multiplicative factor.
Note that we assumed t∗≥t. If t∗< t then the initial noise added
to Counter in Ex′ will be the same as in Ex, and we need to add 1 to
the sum of interval noises in every time period from t through T (the
sum of interval noises before time t remains unchanged). This is done
as above, by ﬁnding a disjoint collection of at most log T intervals that
exactly covers {t, . . . , T −1}. The noise values for these intervals are
not yet in memory when the intrusion occurs in time t∗< t, and the
proof follows similarly.
12.3.2
A logarithmic (in T ) lower bound
Given the upper bound of Theorem 12.5, where the error depends only
poly-logarithmically on T, it is natural to ask whether any dependence
is inherent. In this section we show that a logarithmic dependence on
T is indeed inherent.

248
Additional Models
Theorem 12.6. Any diﬀerentially private event-level algorithm for
counting over T rounds must have error Ω(log T) (even with ε = 1).
Proof. Let ε = 1. Suppose for the sake of contradiction that there exists
a diﬀerentially private event-level counter for streams of length T that
guarantees that with probability at least 2/3, its count at all time
periods is accurate up to a maximum error of (log2 T)/4. Let k =
(log2 T)/4. We construct a set S of T/k inputs as follows. Divide the
T time periods into T/k consecutive phases, each of length k (except,
possibly, the last one). For i = 1, . . . , T/k, the i-th input xi ∈S has
0 input bits everywhere except during the ith phase. That is, xi =
0k·i ◦1k ◦0k·((T/k)−(i+1)).
Fo 1 ≤i ≤T/k, we say an output matches i if just before the
ith phase the output is less than k/2 and at the end of the ith phase
the output is at least k/2. By accuracy, on input xi the output should
match i with probability at least 2/3. By ε diﬀerential privacy, this
means that for every i, j ∈[T/k] such that i ̸= j, the output on input
xi should match j with probability at least
e−2ε·k = e−ε log(T 1/2)
= e−log(T 1/2) = 1/
√
T.
This is a contradiction, because the events that the output matches j
are disjoint for diﬀerent j, and yet the sum of their probabilities on
input xi exceeds 1.
12.4
Average case error for query release
In Sections 4 and 5, we considered various mechanisms for solving the
private query release problem, where we were interested in worst case
error. That is, given a class of queries Q, of size |Q| = k, we wished
to recover a vector of answers ˆa ∈Rk such that for each query fi ∈Q,
|fi(x) −ˆai| ≤α for some worst-case error rate α. In other words, if we
let a ∈Rk denote the vector of true answers, with ai ≡fi(x), then we
require a bound of the form: ∥a−ˆa∥∞≤α. In this section, we consider
a weakened utility guarantee, on the ℓ2 (rather than ℓ∞) error: a bound
of the form ∥a −ˆa∥2 ≤α. A bound of this form does not guarantee

12.4. Average case error for query release
249
that we have low error for every query, but it does guarantee that on
average, we have small error.
Although this sort of bound is weaker than worst-case error, the
mechanism is particularly simple, and it makes use of an elegant
geometric view of the query release problem that we have not seen
until now.
Recall that we can view the database x as a vector x ∈N|X| with
∥x∥1 = n. We can similarly also view the queries fi ∈Q as vectors
fi ∈N|X|, such that fi(x) = ⟨fi, x⟩. It will therefore be helpful to view
our class of queries Q as a matrix A ∈Rk×|X|, with the ith row of A
being the vector fi. We can then see that our answer vector a ∈Rk is,
in matrix notation:
A · x = a.
Let’s consider the domain and range of A when viewed as a linear
map. Write B1 = {x ∈R|X| : ∥x∥1 = 1} denote the unit ℓ1 ball in
|X| dimensional space. Observe that x ∈nB1, since ∥x∥1 = n. We will
refer to nB1 as “Database Space.” Write K = AB1. Note similarly that
for all x ∈nB1, a = A · x ∈nK. We will refer to nK as “answer
space.” We make a couple of observations about K: Note that because
B1 is centrally symmetric, so is K — that is, K = −K. Note also that
K ⊂Rk is a convex polytope with vertices ±A1, . . . , ±A|X| equal to
the columns of A, together with their negations.
The following algorithm is extremely simple: it simply answers every
query independently with the Laplace mechanism, and then projects
back into answer space. In other words, it adds independent Laplace
noise to every query, which as we have seen, by itself leads to distortion
that is linear in k (or at least
√
k, if we relax to (ε, δ)-diﬀerential pri-
vacy). However, the resulting vector ˜a of answers is likely not consistent
with any database y ∈nB1 in database space. Therefore, rather than
returning ˜a, it instead returns some consistent answer vector ˆa ∈nK
that is as close to ˜a as possible. As we will see, this projection step
improves the accuracy of the mechanism, while having no eﬀect on
privacy (since it is just post-processing!)
We ﬁrst observe that Project is diﬀerentially private.
Theorem 12.7. For any A ∈[0, 1]k×|X|, Project(x, A, ε) preserves
(ε, δ)-diﬀerential privacy.

250
Additional Models
Algorithm 18 The K-Projected Laplace Mechanism. It takes as input
a matrix A ∈[0, 1]k×|X|, a database x ∈nB1, and a privacy parameters
ε and δ.
Project(x, A, ε, δ):
Let a = A · x
For each i ∈[k], sample νi ∼Lap(
p
8k ln(1/δ)/ε), and let ˜a = a+ν.
Output ˆa = arg minˆa∈nK ∥ˆa −˜a∥2
2.
Proof. We simply note that ˜a is the output of the Laplace mechanism
on k sensitivity 1 queries, which is (ε, δ)-diﬀerentially private by Theo-
rems 3.6 and 3.20 . Finally, since ˆa is derived from ˜a without any further
access to the private data, the release of ˆa is diﬀerentially private by the
post-processing guarantee of diﬀerential privacy, Proposition 2.1.
Theorem 12.8. For any class of linear queries A and database x, let
a = A·x denote the true answer vector. Let ˆa denote the output of the
mechanism Project: ˆa = Project(x, A, ε). With probability at least
1 −β:
∥a −ˆa∥2
2 ≤kn
p
192 ln(1/δ) ln(2|X|/β)
ε
.
To prove this theorem, we will introduce a couple of simple concepts
from convex geometry. For a convex body K ⊂Rk, its polar body is K◦
deﬁned to be K◦= {y ∈Rk : ⟨y, x⟩≤1 for all x ∈K}. The Minkowski
Norm deﬁned by a convex body K is
∥x∥K ≡min{r ∈R such that x ∈rK}.
The dual norm of ∥x∥K is the Minkowski norm induced by the polar
body of K, i.e., ∥x∥K◦. This norm also has the following form:
∥x∥K◦= max
y∈K ⟨x, y⟩.
The key fact we will use is Holder’s Inequality, which is satisﬁed by all
centrally symmetric convex bodies K:
|⟨x, y⟩| ≤∥x∥K∥y∥K◦.

12.4. Average case error for query release
251
Proof of Theorem 12.8. The proof will proceed in two steps. First we
will show that: ∥a−ˆa∥2
2 ≤2⟨ˆa−a, ˜a−a⟩, and then we will use Holder’s
inequality to bound this second quantity.
Lemma 12.9.
∥a −ˆa∥2
2 ≤2⟨ˆa −a, ˜a −a⟩
Proof. We calculate:
∥ˆa −a∥2
2 = ⟨ˆa −a, ˆa −a⟩
= ⟨ˆa −a, ˜a −a⟩+ ⟨ˆa −a, ˆa −˜a⟩
≤2⟨ˆa −a, ˜a −a⟩.
The inequality follows from calculating:
⟨ˆa −a, ˜a −a⟩= ∥˜a −a∥2
2 + ⟨ˆa −˜a, ˜a −a⟩
≥∥ˆa −˜a∥2
2 + ⟨ˆa −˜a, ˜a −a⟩
= ⟨ˆa −˜a, ˆa −a⟩,
Where the ﬁnal inequality follows because by choice of ˆa, for all a′ ∈
nK: ∥˜a −ˆa∥2
2 ≤∥˜a −a′∥2
2.
We can now complete the proof. Recall that by deﬁnition, ˜a−a = ν,
the vector of i.i.d. Laplace noise added by the Laplace mechanism. By
Lemma 12.9 and Holder’s inequality, we have:
∥a −ˆa∥2
2 ≤2⟨ˆa −a, ν⟩
≤2∥ˆa −a∥K∥ν∥K◦.
We bound these two terms separately. Since by deﬁnition ˆa, a ∈nK,
we have max(∥ˆa∥K, ∥a∥K) ≤n, and so by the triangle inequality, ∥ˆa −
a∥K ≤2n.
Next, observe that since ∥ν∥K◦= maxy∈K⟨y, ν⟩, and since the max-
imum of a linear function taken over a polytope is attained at a vertex,
we have: ∥ν∥K◦= maxi∈[|X|] |⟨Ai, ν⟩|.
Because each Ai ∈Rk is such that ∥Ai∥∞≤1, and recalling that for
any scalar q, if Z ∼Lap(b), then qZ ∼Lap(qb), we can apply Lemma by

252
Additional Models
Lemma 12.2 to bound the weighted sums of Laplace random variables
⟨Ai, ν⟩. Doing so, we have that with probability at least 1 −β:
max
i∈[|X|] |⟨Ai, ν⟩| ≤8k
p
ln(1/δ) ln(|X|/β)
ϵ
.
Combining all of the above bounds, we get that with probability
1 −β:
∥a −ˆa∥2
2 ≤16nk
p
ln(1/δ) ln(|X|/β)
ϵ
.
Let’s interpret this bound. Observe that ∥a−ˆa∥2
2 = Pk
i=1(ai −ˆai)2,
and so this is a bound on the sum of squared errors over all queries.
Hence, the average per-query squared error of this mechanism is only:
1
k
k
X
i=1
(ai −ˆai)2 ≤16n
p
ln(1/δ) ln(|X|/β)
ϵ
.
In contrast, the private multiplicative weights mechanism guaran-
tees that maxi∈[k] |ai −ˆai| ≤˜O(√n log |X|1/4/ε1/2), and so matches
the average squared error guarantee of the projected Laplace mecha-
nism, with a bound of: ˜O(n
p
log |X|/ε). However, the multiplicative
weights mechanism (and especially its privacy analysis) its much more
complex than the Projected Laplace mechanism! In particular, the pri-
vate part of the K-Projected Laplace mechanism is simply the Laplace
mechanism itself, and requires no coordination between queries. Inter-
estingly — and, it turns out, necessarily — coordination occurs in the
projection phase. Since projection is in post-precessing, it incurs no
further privacy loss; indeed, it can be carrie out (online, if necessary)
by the data analyst himself.
12.5
Bibliographical notes
The local model of data privacy has its roots in randomized response, as
ﬁrst proposed by Warner in 1965 [84]. The local model was formalized
by Kasiviswanathan et al. [52] in the context of learning, who proved
that private learning in the local modal is equivalent to non-private

12.5. Bibliographical notes
253
learning in the statistical query (SQ) model. The set of queries which
can be released in the local model was shown to be exactly equal to
the set of queries that can be agnostically learned in the SQ model by
Gupta et al. [38].
Pan-Privacy was introduced by Dwork et al. [27], and further
explored by Mir et al. [62]. The pan-private density estimation, as well
as a low-memory variant using hashing, appear in [27].
Privacy under continual observation was introduced by Dwork
et al. [26]; our algorithm for counting under continual observation is
from that paper, as is the lower bound on error. Similar algorithms were
given by Chan et al. [11]. The proof of concentration of measure inequal-
ity for the sums of Laplace random variables given in Lemma 12.2 is
from [11].
The Projected Laplace mechanism for achieving low average error
was given by Nikolov et al.[66], who also give instance optimal algo-
rithms for the (average error) query release problem for any class of
queries. This work extends a line of work on the connections between
diﬀerential privacy and geometry started by Hardt and Talwar [45],
and extended by Bhaskara et al. [5] and Dwork et al. [30].
Dwork, Naor, and Vadhan proved an exponential gap between the
number of queries that can be answered (with non-trivial error) by
stateless and stateful diﬀerentially private mechanisms [29]. The lesson
learned — that coordination is essential for accurately and privately
answering very large numbers of queries — seems to rule out the inde-
pendent noise addition in the Projected Laplace mechanism. The state-
fulness of that algorithm appears in the projection step, resolving the
paradox.

13
Reﬂections
13.1
Toward practicing privacy
Diﬀerential Privacy was designed with internet-scale data sets in mind.
Reconstruction attacks along the lines of those in Section 8 can be
carried out by a polynomial time bounded adversary asking only O(n)
queries on databases of size n. When n is on the order of hundreds of
millions, and each query requires a linear amount of computation, such
an attack is unrealistic, even thought the queries can be parallelized.
This observation led to the earliest steps toward diﬀerential privacy: If
the adversary is restricted to a sublinear number of counting queries,
then o(√n) noise per query — less than the sampling error! — is suf-
ﬁcient for preserving privacy (Corollary 3.21).
To what extent can diﬀerential privacy be brought to bear on
smaller data sets, or even targeted attacks that isolate a small sub-
set of a much larger database, without destroying statistical utility?
First, an analysis may require a number of queries that begins to look
something like the size of this smaller set. Second, letting n now denote
the size of the smaller set or small database, and letting k be the num-
ber of queries, fractional errors on the order of
√
k/n are harder to
ignore when n is small. Third, the
p
ln(1/δ)/ε factor in the advanced
254

13.1. Toward practicing privacy
255
composition theorem becomes signiﬁcant. Keeping in mind the recon-
struction attacks when noise is o(√n), there appears to be little room
to maneuver for arbitrary sets of k ≈n low-sensitivity queries.
There are several promising lines of research for addressing these
concerns.
The Query Errors Don’t Tell the Whole Story.
As an example of this
phenomenon, consider the problem of linear regression. The input is
a collection of labeled data points of the form (x, y), where x ∈Rd
and y ∈R, for arbitrary dimension d. The goal is to ﬁnd θ ∈Rd that
“predicts” y “as well as possible,” given x, under the assumption that
the relationship is linear. If the goal is simply to “explain” the given
data set, diﬀerential privacy may well introduce unacceptable error.
Certainly the speciﬁc algorithm that simply computes
argminθ|
n
X
i=1
θ · xi −yi|2
and adds appropriately scaled Laplace noise independently to each
coordinate of θ may produce a ˜θ that diﬀers substantially from θ. But if
the goal is to learn a predictor that will do well for future, unseen inputs
(x, y) then a slightly diﬀerent computation is used to avoid overﬁtting,
and the (possibly large) diﬀerence between the private and non-private
coeﬃcient vectors does not translate into a gap in classiﬁcation error!
A similar phenomenon has been observed in model ﬁtting.
Less Can Be More.
Many analyses ask for more than they actually
use. Exploitation of this principle is at the heart of Report Noisy Max,
where for the accuracy “price” of one measurement we learn one of
the largest of many measurements. By asking for “less” (that is, not
requiring that all noisy measurements be released, but rather only ask-
ing for the largest one), we obtain “more” (better accuracy). A familiar
principle in privacy is to minimize collection and reporting. Here we
see this play out in the realm of what must be revealed, rather than
what must be used in the computation.
Quit When You are NOT Ahead.
This is the philosophy behind
Propose-Test-Release, in which we test in a privacy-preserving way

256
Reﬂections
that small noise is suﬃcient for a particular intended computation on
the given data set.
Algorithms with Data-Dependent Accuracy Bounds.
This can be
viewed as a generalization of Quit When You are Not Ahead. Algo-
rithms with data-dependent accuracy bounds can deliver excellent
results on “good” data sets, as in Propose-Test-Release, and the accu-
racy can degrade gradually as the “goodness” decreases, an improve-
ment over Propose-Test-Release.
Exploit “Nice” Query Sets.
When (potentially large) sets of linear
queries are presented as a batch it is possible, by analyzing the geometry
of the query matrix to obtain higher quality answers than would be
obtained were the queries answered independently1.
Further Relaxation of Diﬀerential Privacy
We have seen that (ϵ, δ)-
diﬀerential privacy is a meaningful relaxation of diﬀerential privacy
that can provide substantially improved accuracy bounds. Moreover,
such a relaxation can be essential to these improvements. For example,
Propose-Test-Release algorithms can only oﬀer (ε, δ)-diﬀerential pri-
vacy for δ > 0. What about other, but still meaningful, relaxations of
diﬀerential privacy? Concentrated Diﬀerential Privacy is such a relax-
ation that is incomparable to (ε, δ)-diﬀerential privacy and that per-
mits better accuracy. Roughly speaking, it ensures that large privacy
loss happens with very small probability; for example, for all k the
probability of privacy loss kε falls exponentially in k2. In contrast,
(ε, δ)-diﬀerential privacy is consistent with having inﬁnite privacy loss
with probability δ; on the other hand, privacy lost 2ε can happen in
concentrated diﬀerential privacy with constant probability, while in
(ε, δ)-diﬀerential privacy it will only occur with probability bounded
by δ, which we typically take to be cryptographically small.
Why might we feel comfortable with this relaxation? The answer
lies in behavior under composition. As an individual’s data participate
1More accurately, the analysis is of the object K = ABk
1 , where A is the query
matrix and Bk
1 is the k-dimensional L1 ball; note that K is the feasible region in
answer space when the database has one element.

13.1. Toward practicing privacy
257
in many databases and many diﬀerent computations, perhaps the real
worry is the combined threat of multiple exposures. This is captured by
privacy under composition. Concentrated diﬀerential privacy permits
better accuracy while yielding the same behavior under composition as
(ε, δ) (and (ε, 0)) diﬀerential privacy.
Diﬀerential privacy also faces a number of cultural challenges. One
of the most signiﬁcant is non-algorithmic thinking. Diﬀerential privacy
is a property of an algorithm. However, many people who work with
data describe their interactions with the data in fundamentally non-
algorithmic terms, such as, “First, I look at the data.” Similarly, data
cleaning is often described in non-algorithmic terms. If data are rea-
sonably plentiful, and the analysts are energetic, then the “Raw Data”
application of the Subsample and Aggregate methodology described in
Example 7.3 suggests a path toward enabling non-algorithmic, inter-
actions by trusted analysts who will follow directions. In general, it
seems plausible that on high-dimensional and on internet-scale data
sets non-algorithmic interactions will be the exception.
What about ε? In Example 3.7 we applied Theorem 3.20 to con-
clude that to bound the cumulative lifetime privacy loss at ε = 1 with
probability 1 −e−32, over participation in 10, 000 databases, it is suf-
ﬁcient that each database be (1/801, 0)-diﬀerentially private. While
k = 10, 000 may be an overestimate, the dependence on k is fairly
weak (
√
k), and in the worst case these bounds are tight, ruling out a
more relaxed bound than ε0 = 1/801 for each database over the lifetime
of the database. This is simply too strict a requirement in practice.
Perhaps we can ask a diﬀerent question: Fix ε, say, ε = 1 or
ε = 1/10; now ask: How can multiple ε’s be apportioned? Permitting
ε privacy loss per query is too weak, and ε loss over the lifetime of the
database is too strong. Something in between, say, ε per study or ε
per researcher, may make sense, although this raises the questions of
who is a “researcher” and what constitutes a “study.” This aﬀords sub-
stantially more protection against accidental and intentional privacy
compromise than do current practices, from enclaves to conﬁdentiality
contracts.
A diﬀerent proposal is less prescriptive. This proposal draws from
second-generation regulatory approaches to reducing environmental

258
Reﬂections
degradation, in particular pollution release registries such as the Toxic
Release Inventory that have been found to encourage better practices
through transparency. Perhaps a similar eﬀect could arise with private
data analysis: an Epsilon Registry describing data uses, granularity of
privacy protection, a “burn rate” of privacy loss per unit time, and a
cap on total privacy loss permitted before data are retired, when accom-
panied with a ﬁnancial penalty for inﬁnite (or very large) loss, can lead
to innovation and competition, deploying the talents and resources of
a larger set of researchers and privacy professionals in the search for
diﬀerentially private algorithms.
13.2
The diﬀerential privacy lens
An online etymological dictionary describes the original 18th century
meaning of the term of the word “statistics” as “science dealing with
data about the condition of a state or community.” This resonates with
diﬀerential privacy in the breach: if the presence or absence of the data
of a small number of individuals changes the outcome of an analysis
then in some sense the outcome is “about” these few individuals, and
is not describing the condition of the community as a whole. Put diﬀer-
ently, stability to small perturbations in the data is both the hallmark
of diﬀerential privacy and the essence of a common conception of the
term “statistical.” Diﬀerential privacy is enabled by stability (Section 7)
and ensures stability (by deﬁnition). In some sense it forces all queries
to be statistical in nature. As stability is also increasingly understood to
be a key necessary and suﬃcient condition for learnability, we observe a
tantalizing moral equivalence between learnability, diﬀerential privacy,
and stability.
With this in mind, it is not surprising that diﬀerential privacy is also
a means to ends other than privacy, and indeed we saw this with game
theory in Section 10. The power of diﬀerential privacy comes from its
amenability to composition. Just as composition allows us to build com-
plex diﬀerentially private algorithms from smaller diﬀerentially private
building blocks, it provides a programming language for constructing
stable algorithms for complex analytical tasks. Consider, for example,
the problem of eliciting a set of bidder values, and using them to price

13.2. The diﬀerential privacy lens
259
a collection of goods that are for sale. Informally, Walrasian equilib-
rium prices are prices such that every individual can simultaneously
purchase their favorite bundle of goods given the prices, while ensuring
that demand exactly equals the supply of each good. It would seem
at ﬁrst blush, then, that simply computing these prices, and assigning
each person their favorite bundle of goods given the prices would yield
a mechanism in which agents were incentivized to tell the truth about
their valuation function — since how could any agent do better than
receiving their favorite bundle of goods? However, this argument fails —
because in a Walrasian equilibrium, agents receive their favorite bundle
of goods given the prices, but the prices are computed as a function
of the reported valuations, so an industrious but dishonest agent could
potentially gain by manipulating the computed prices. However, this
problem is solved (and an approximately truthful mechanism results)
if the equilibrium prices are computed using a diﬀerentially private
algorithm — precisely because individual agents have almost no eﬀect
on the distribution of prices computed. Note that this application is
made possible by the use of the tools of diﬀerential privacy, but is com-
pletely orthogonal to privacy concerns. More generally, this connection
is more fundamental: computing equilibria of various sorts using algo-
rithms that have the stability property guaranteed by diﬀerential pri-
vacy leads to approximately truthful mechanisms implementing these
equilibrium outcomes.
Diﬀerential privacy also helps in ensuring generalizability in adap-
tive data analysis. Adaptivity means that the questions asked and
hypotheses tested depend on outcomes of earlier questions. General-
izability means that the outcome of a computation or a test on the
data set is close to the ground truth of the distribution from which the
data are sampled. It is known that the naive paradigm of answering
queries with the exact empirical values on a ﬁxed data set fails to gener-
alize even under a limited amount of adaptive questioning. Remarkably,
answering with diﬀerential privacy not only ensures privacy, but with
high probability it ensures generalizability even for exponentially many
adaptively chosen queries. Thus, the deliberate introduction of noise
using the techniques of diﬀerential privacy has profound and promising
implications for the validity of traditional scientiﬁc inquiry.

Appendices

A
The Gaussian Mechanism
Let f : N|X| →Rd be an arbitrary d-dimensional function, and deﬁne its
ℓ2 sensitivity to be ∆2f = maxadjacentx,y ∥f(x) −f(y)∥2. The Gaussian
Mechanism with parameter σ adds noise scaled to N(0, σ2) to each of
the d components of the output.
Theorem A.1. Let ε ∈(0, 1) be arbitrary. For c2 > 2 ln(1.25/δ), the
Gaussian Mechanism with parameter σ ≥c∆2f/ε is (ε, δ)-diﬀerentially
private.
Proof. There is a database D and a query f, and the mechanism will
return f(D)+η, where the noise is normally distributed. We are adding
noise N(0, σ2). For now, assume we are talking about real-valued func-
tions, so
∆f = ∆1f = ∆2f.
We are looking at
ln
e(−1/2σ2)x2
e(−1/2σ2)(x+∆f)2
 .
(A.1)
We are investigating the probability, given that the database is D,
of observing an output that occurs with a very diﬀerent probability
261

262
The Gaussian Mechanism
under D than under an adjacent database D′, where the probability
space is the noise generation algorithm. The numerator in the ratio
above describes the probability of seeing f(D) + x when the database
is D, the denominator corresponds the probability of seeing this same
value when the database is D′. This is a ratio of probabilities, so it is
always positive, but the logarithm of the ratio may be negative. Our
random variable of interest — the privacy loss — is
ln
e(−1/2σ2)x2
e(−1/2σ2)(x+∆f)2
and we are looking at its absolute value.
ln
e(−1/2σ2)x2
e(−1/2σ2)(x+∆f)2
 = | ln e(−1/2σ2)[x2−(x+∆f)2]|
= | −
1
2σ2 [x2 −(x2 + 2x∆f + ∆f2)]|
= | 1
2σ2 (2x∆f + (∆f)2)|.
(A.2)
This quantity is bounded by ε whenever x < σ2ε/∆f −∆f/2. To
ensure privacy loss bounded by ε with probability at least 1 −δ, we
require
Pr[|x| ≥σ2ε/∆f −∆f/2] < δ,
and because we are concerned with |x| we will ﬁnd σ such that
Pr[x ≥σ2ε/∆f −∆f/2] < δ/2.
We will assume throughout that ε ≤1 ≤∆f.
We will use the tail bound
Pr[x > t] ≤
σ
√
2πe−t2/2σ2.
We require:
σ
√
2π
1
t e−t2/2σ2 < δ/2
⇔σ1
t e−t2/2σ2 <
√
2πδ/2
⇔t
σet2/2σ2 > 2/
√
2πδ
⇔ln(t/σ) + t2/2σ2 > ln(2/
√
2πδ).

263
Taking t = σ2ε/∆f −∆f/2, we get
ln((σ2ε/∆f −∆f/2)/σ) + (σ2ε/∆f −∆f/2)2/2σ2 > ln(2/
√
2πδ)
= ln
 r 2
π
1
δ
!
.
Let us write σ = c∆f/ε; we wish to bound c. We begin by ﬁnding the
conditions under which the ﬁrst term is non-negative.
1
σ

σ2 ε
∆f −∆f
2

= 1
σ
" 
c2 (∆f)2
ε2
!
ε
∆f −∆f
2
#
= 1
σ

c2
∆f
ε

−∆f
2

=
ε
c∆f

c2
∆f
ε

−∆f
2

= c −ε
2c.
Since ε ≤1 and c ≥1, we have c −ε/(2c) ≥c −1/2. So ln( 1
σ(σ2 ε
∆f −
∆f
2 )) > 0 provided c ≥3/2. We can therefore focus on the t2/σ2 term.
 
1
2σ2
σ2ε
∆f −∆f
2
!2
=
1
2σ2
"
∆f
 
c2
ε −1
2
!#2
=
"
(∆f)2
 
c2
ε −1
2
!#2 "
ε2
c2(∆f)2
#
1
2
= 1
2
 
c2
ε −1
2
!2 ε2
c2
= 1
2(c2 −ε + ε2/4c2).
Since ε ≤1 the derivative of (c2−ε+ε2/4c2) with respect to c is positive
in the range we are considering (c ≥3/2), so c2 −ε + ε2/4c2 ≥c2 −8/9
and it suﬃces to ensure
c2 −8/9 > 2 ln
 r 2
π
1
δ
!
.

264
The Gaussian Mechanism
In other words, we need that
c2 > 2 ln(
q
2/π) + 2 ln(1/δ) + ln(e8/9) = ln(2/π) + ln(e8/9) + 2 ln(1/δ),
which, since (2/π)e8/9 < 1.55, is satisﬁed whenever c2 > 2 ln(1.25/δ).
Let us parition R as R = R1 ∪R2, where R1 = {x ∈R : |x| ≤
c∆f/ε} and R2 = {x ∈R : |x| > c∆f/ε}. Fix any subset S ⊆R, and
deﬁne
S1 = {f(x) + x | x ∈R1}
S2 = {f(x) + x | x ∈R2}.
We have
Pr
x∼N(0,σ2)[f(x) + x ∈S] =
Pr
x∼N(0,σ2)[f(x) + x ∈S1]
+
Pr
x∼N(0,σ2)[f(x) + x ∈S2]
≤
Pr
x∼N(0,σ2)[f(x) + x ∈S1] + δ
≤eε
 
Pr
x∼N(0,σ2)[f(y) + x ∈S1]
!
+ δ,
yielding (ε, δ)-diﬀerential privacy for the Gaussian mechanism in one
dimension.
High Dimension.
To extend this to functions in Rm, deﬁne ∆f =
∆2f. We can now repeat the argument, using Euclidean norms. Let v
be any vector satisfying ∥v∥≤∆f. For a ﬁxed pair of databases x, y
we are interested in v = f(x) −f(y), since this is what our noise must
obscure. As in the one dimensional, case we seek conditions on σ under
which the privacy loss
ln e(−1/2σ2)∥x−µ∥2
e(−1/2σ2)∥x+v−µ∥2


265
is bounded by ε; here x is chosen from N(0, Σ), where (Σ) is a diagonal
matrix with entries σ2, whence µ = (0, . . . , 0).
ln e(−1/2σ2)∥x−µ∥2
e(−1/2σ2)∥x+v−µ∥2
 = | ln e(−1/2σ2)[∥x−µ∥2−∥x+v−µ∥2]|
=

1
2σ2 (∥x∥2 −∥x + v∥2))
 .
We will use the fact that the distribution of a spherically symmetric nor-
mal is independent of the orthogonal basis from which its constituent
normals are drawn, so we may work in a basis that is alligned with v.
Fix such a basis b1, . . . , bm, and draw x by ﬁrst drawing signed lengths
λi ∼N(0, σ2), for i ∈[m], then deﬁning x[i] = λibi, and ﬁnally letting
x = Pm
i=1 x[i]. Assume without loss of generality that b1 is parallel to v.
We are interested in | ∥x∥2 −∥x + v∥2 |.
Consider the right triangle with base v + x[1] and edge Pm
i=2 x[i]
orthogonal to v. The hypotenuse of this triangle is x + v.
∥x + v∥2 = ∥v + x[1]∥2 +
m
X
i=2
∥x[i]∥2
∥x∥2 =
m
X
i=1
∥x[i]∥2.
Since v is parallel to x[1] we have ∥v + x[1]∥2 = (∥v∥+ λ1)2. Thus,
∥x + v∥2 −∥x∥2 = ∥v∥2 + 2λ1 · ∥v∥. Recall that ∥v∥≤∆f, and λ ∼
N(0, σ), so we are now exactly back in the one-dimensional case, writing
λ1 instead of x in Equation (A.2):

1
2σ2 (∥x∥2 −∥x + v∥2))
 ≤

1
2σ2 (2λ1∆f −(∆f)2)

and the rest of the argument proceeds as above.
The argument for the high dimensional case highlights a weakness
of (ε, δ)-diﬀerential privacy that does not exist for (ε, 0)-diﬀerential
privacy. Fix a database x. In the (ε, 0)-case, the guarantee of indis-
tinguishability holds for all adjacent databases simultaneously. In the

266
The Gaussian Mechanism
(ε, δ) case indistinguishability only holds “prospectively,” i.e., for any
ﬁxed y adjacent to x, the probability that the mechanism will allow the
adversary to distinguish x from y is small. In the proof above, this is
manifested by the fact that we ﬁxed v = f(x) −f(y); we did not have
to argue about all possible directions of v simultaneously, and indeed
we cannot, as once we have ﬁxed our noise vector x ∼N(0, Σ), so that
the output on x is o = f(x) + x, there may exist an adjacent y such
that output o = f(x) + x is much more likely when the database is y
than it is on x.
A.1
Bibliographic notes
Theorem A.1 is folklore initially observed by the authors of [23]. A gen-
eralization to non-spherical gaussian noise appears in [66].

B
Composition Theorems for (ε, δ)-DP
B.1
Extension of Theorem 3.16
Theorem B.1. Let T1(D) : D 7→T1(D) ∈C1 be an (ϵ, δ)-d.p. function,
and for any s1 ∈C1, T2(D, s1) : (D, s1) 7→T2(D, s1) ∈C2 be an (ϵ, δ)-
d.p. function given the second input s1. Then we show that for any
neighboring D, D′, for any S ⊆C2 × C1, we have, using the notation in
our paper
P((T2, T1) ∈S) ≤e2ϵP ′((T2, T1) ∈S) + 2δ.
(B.1)
Proof. For any C1 ⊆C1, deﬁne
µ(C1) =
 P(T1 ∈C1) −eϵP ′(T1 ∈C1)

+ ,
then µ is a measure on C1 and µ(C1) ≤δ since T1 is (ϵ, δ)-d.p. As a
result, we have for all s1 ∈C1,
P(T1 ∈ds1) ≤eϵP ′(T1 ∈ds1) + µ(ds1).
(B.2)
Also note that by the deﬁnition of (ϵ, δ)-d.p., for any s1 ∈C1,
P((T2, s1) ∈S) ≤
 eϵP ′((T2, s1) ∈S) + δ
 ∧1
≤
 eϵP ′((T2, s1) ∈S)
 ∧1 + δ.
(B.3)
267

268
Composition Theorems for (ε, δ)-DP
Then (B.2) and (B.3) give (B.1):
P((T2, T1) ∈S) ≤
Z
S1
P((T2, s1) ∈S)P(T1 ∈ds1)
≤
Z
S1
  eϵP ′((T2, s1) ∈S)
 ∧1 + δ
 P(T1 ∈ds1)
≤
Z
S1
  eϵP ′((T2, s1) ∈S)
 ∧1
 P(T1 ∈ds1) + δ
≤
Z
S1
  eϵP ′((T2, s1) ∈S)
 ∧1

× (eϵP ′(T1 ∈ds1) + µ(ds1)) + δ
≤e2ϵ
Z
S1
P ′((T2, s1) ∈S)P ′(T1 ∈ds1) + µ(S1) + δ
≤e2ϵP ′((T2, T1) ∈S) + 2δ.
(B.4)
In the equations above, S1 denotes the projection of S onto C1.
The event {(T2, s1)
∈
S} refers to {(T2(D, s1), s1)
∈
S} (or
{(T2(D′, s1), s1) ∈S}).
Using induction, we have:
Corollary B.2 (general composition theorem for (ϵ, δ)-d.p. algorithms).
Let T1
:
D
7→
T1(D) be (ϵ, δ)-d.p., and for k
≥
2, Tk
:
(D, s1, . . . , sk−1) 7→Tk(D, s1, . . . , sk−1) ∈Ck be (ϵ, δ)-d.p., for all
given (sk−1, . . . , s1) ∈Nk−1
j=1 Cj. Then for all neighboring D, D′ and
all S ⊆Nk
j=1 Cj
P((T1, . . . , Tk) ∈S) ≤ekϵP ′((T1, . . . , Tk) ∈S) + kδ.
