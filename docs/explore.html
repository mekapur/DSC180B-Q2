<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Explore | Training-based vs Training-free Differential Privacy</title>
  <link rel="stylesheet" href="style.css" />
  <script src="https://cdn.plot.ly/plotly-2.30.0.min.js"></script>
</head>
<body data-page="explore">
  <div id="siteHeader"></div>

  <main>
    <section class="section">
      <h1>Explore the Results</h1>
      <p class="lead">
        Scroll through interactive visualizations: see our workflow, browse queries, compare distributions,
        and view performance metrics side-by-side.
      </p>
    </section>

    <!-- SCROLLYTELLING CONTAINER -->
    <div class="scrolly-container">
      <!-- LEFT: NARRATIVE STEPS -->
      <div class="scrolly-text">
        <!-- STEP 1: HERO -->
        <section class="step" data-step="step-hero">
          <h2>Step 1: Project Overview</h2>
          <p>
            This project compares four approaches to differentially private synthetic data generation:
          </p>
          <ul>
            <li><strong>Wide-Table DP-VAE:</strong> Train a VAE on all columns flattened into one table.</li>
            <li><strong>Per-Table DP-SGD:</strong> Train independent VAE models for each table separately.</li>
            <li><strong>MST (Marginal):</strong> Reconstruct synthetic data from low-cardinality marginals using a Chow-Liu tree.</li>
            <li><strong>Private Evolution:</strong> Use an LLM to generate candidates, then select via differential privacy.</li>
          </ul>
          <p>
            All methods add differential privacy noise to protect individual privacy while maintaining data utility.
            We benchmark each approach on 21 production-style SQL queries.
          </p>
        </section>

        <!-- STEP 2: QUERY EXPLORER -->
        <section class="step" data-step="step-queries">
          <h2>Step 2: Benchmark Queries</h2>
          <p>
            We executed 21 SQL queries grouped into four categories: aggregate (GROUP BY with metrics),
            distribution (histograms and percentages), ranking (top-K items), and row-level (summary statistics).
          </p>
          <p>
            Use the slider on the right to explore each query. Each card shows:
          </p>
          <ul>
            <li>Query name and type (e.g., "aggregate", "distribution")</li>
            <li>Description of what the query measures</li>
            <li>Performance scores (relative error, total variation, or Jaccard) for each method</li>
            <li>An example visualization (placeholder)</li>
          </ul>
          <p>
            <strong>Green means passed</strong> (metric below threshold); <strong>red means failed</strong>.
          </p>
        </section>

        <!-- STEP 3: INTERACTIVE HISTOGRAM -->
        <section class="step" data-step="step-histogram">
          <h2>Step 3: Distribution Visualization</h2>
          <p>
            This interactive histogram compares real (measured) distributions against synthetic data from each method.
          </p>
          <p>
            On the right, you can:
          </p>
          <ul>
            <li><strong>Select a method:</strong> Choose which synthetic approach to compare against real data.</li>
            <li><strong>Toggle normalization:</strong> View as probability (normalized) or raw counts.</li>
            <li><strong>View difference:</strong> See synthetic minus real, highlighting overestimates (positive) and underestimates (negative).</li>
            <li><strong>Check metrics:</strong> Total Variation distance indicates how different the distributions are.</li>
          </ul>
          <p>
            Lower Total Variation means better preservation of the distribution. A value below 0.15 typically passes.
          </p>
        </section>

        <!-- STEP 4: IMAGE COMPARE SLIDER -->
        <section class="step" data-step="step-compare">
          <h2>Step 4: Real vs. Synthetic Plots</h2>
          <p>
            Drag the handle on the right to compare actual query results side-by-side:
            the left side shows real data, the right shows synthetic.
          </p>
          <p>
            Look for:
          </p>
          <ul>
            <li><strong>Shape match:</strong> Does the synthetic distribution follow the same shape as real?</li>
            <li><strong>Peak alignment:</strong> Are modes (peaks) at similar locations?</li>
            <li><strong>Tail behavior:</strong> Do tails match, or is one method missing rare values?</li>
            <li><strong>Zero-inflation:</strong> Excessive zeros or gaps in synthetic suggest mode collapse or sparsity.</li>
          </ul>
          <p>
            This is a qualitative check alongside the quantitative metrics shown earlier.
          </p>
        </section>

        <!-- STEP 5: SCOREBOARD -->
        <section class="step" data-step="step-scoreboard">
          <h2>Step 5: Summary Scorecard</h2>
          <p>
            Here is the final tally: how many queries each method passed out of the total evaluated.
          </p>
          <div class="results-summary">
            <div class="score-card">
              <h3>Per-Table DP-SGD</h3>
              <p class="score">6/21</p>
              <p class="small">Queries passed</p>
            </div>
            <div class="score-card">
              <h3>MST (Marginal)</h3>
              <p class="score">6/21</p>
              <p class="small">Queries passed</p>
            </div>
            <div class="score-card">
              <h3>Wide-Table DP-VAE</h3>
              <p class="score">1/8</p>
              <p class="small">Evaluated subset</p>
            </div>
            <div class="score-card">
              <h3>Private Evolution</h3>
              <p class="score">2/8</p>
              <p class="small">Evaluated subset</p>
            </div>
          </div>
          <p>
            Key insight: simple distributions pass, but complex join-heavy queries challenge all methods.
            See <a href="results.html">Results</a> for a deep dive into failure modes and recommendations.
          </p>
        </section>
      </div>

      <!-- RIGHT: STICKY PANEL -->
      <div class="scrolly-panel">
        <!-- Progress indicator -->
        <div class="scrolly-progress">
          <div class="progress-bar"></div>
          <span class="progress-text">Step 1 of 5</span>
        </div>

        <!-- Panel content placeholder -->
        <div class="panel-content" id="panelContent">
          <div class="figure-placeholder">
            <p>Workflow diagram placeholder</p>
            <small>Project overview and evaluation workflow.</small>
          </div>
        </div>
      </div>
    </div>

    <!-- FOOTER SECTION -->
    <section class="section">
      <h2>Summary</h2>
      <p>
        This exploration shows that differential privacy introduces a fundamental trade-off: stronger privacy
        (lower epsilon) means less utility, but weaker privacy risks individual leakage. Our evaluation found that:
      </p>
      <ul class="bullets">
        <li>Simple distributions and independent features are well-preserved by most methods.</li>
        <li>Join-heavy and cross-table queries remain the biggest challenge.</li>
        <li>Wide-table training sacrifices individual feature fidelity for multi-table cohesion.</li>
        <li>Hybrid routing (different methods for different query types) is the most promising path forward.</li>
      </ul>
      <p>
        For full technical details, see the <a href="results.html">Results</a> page and the <a href="https://github.com/mekapur/DSC180B-Q2/raw/main/report/q2-report.pdf" target="_blank">full report (PDF)</a>.
      </p>
    </section>
  </main>

  <div id="siteFooter"></div>

  <script src="js/scrolly.js"></script>
  <script src="js/charts.js"></script>
  <script src="app.js"></script>
</body>
</html>
