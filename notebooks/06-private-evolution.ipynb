{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06: Private Evolution for Tabular Data\n",
    "\n",
    "This notebook implements Private Evolution (PE) adapted for the DCA telemetry wide table, following Lin et al. (2024) and Swanberg et al. (2025). Instead of training a generative model with DP-SGD, PE uses black-box API access to a foundation model (GPT-5 nano) and a DP nearest-neighbor histogram to iteratively select synthetic candidates that best approximate the real data distribution.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Load the wide training table (from notebook 05)\n",
    "2. Configure PE parameters and privacy budget\n",
    "3. Run PE (RANDOM_API -> DP histogram -> selection -> VARIATION_API)\n",
    "4. Decompose synthetic wide table into reporting tables\n",
    "5. Run benchmark queries and compare with ground truth and DP-SGD results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(Path(\"../.env\"))\n",
    "\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "REPORTING = Path(\"../data/reporting\")\n",
    "QUERIES_DIR = Path(\"../docs/queries\")\n",
    "REAL_RESULTS = Path(\"../data/results/real\")\n",
    "PE_REPORTING = Path(\"../data/reporting/pe\")\n",
    "PE_RESULTS = Path(\"../data/results/pe\")\n",
    "MODEL = \"gpt-5-nano\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Load the wide training table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = pd.read_parquet(REPORTING / \"wide_training_table.parquet\")\n",
    "\n",
    "cat_cols = [\"chassistype\", \"countryname_normalized\", \"modelvendor_normalized\",\n",
    "            \"os\", \"cpuname\", \"cpucode\", \"cpu_family\", \"persona\", \"processornumber\"]\n",
    "numeric_cols = [c for c in wide.columns if c != \"guid\" and c not in cat_cols]\n",
    "\n",
    "display(Markdown(\n",
    "    f\"Wide table: {len(wide):,} rows x {len(wide.columns)} columns\\n\\n\"\n",
    "    f\"Categorical: {len(cat_cols)} columns, Numeric: {len(numeric_cols)} columns\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Configure PE and privacy budget\n",
    "\n",
    "Following Swanberg et al. (2025), we use T=1 iteration as the primary setting (their finding that T=1 is optimal for tabular PE). We match the DP-SGD privacy budget: epsilon=4.0, delta=1e-5.\n",
    "\n",
    "The noise multiplier sigma is calibrated via the analytic Gaussian mechanism (Balle and Wang, 2018) with adaptive composition (Dong et al., 2019): T iterations with noise sigma each compose to a single Gaussian mechanism with effective sensitivity sqrt(T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pe.privacy import calibrate_sigma, compute_epsilon\n",
    "\n",
    "N_SYNTH = 50000\n",
    "T = 1\n",
    "L = 3\n",
    "EPSILON = 4.0\n",
    "DELTA = 1e-5\n",
    "MODEL = \"gpt-5-nano\"\n",
    "\n",
    "sigma = calibrate_sigma(EPSILON, DELTA, T)\n",
    "\n",
    "display(Markdown(\n",
    "    f\"PE configuration:\\n\\n\"\n",
    "    f\"- Model: `{MODEL}`\\n\"\n",
    "    f\"- N_synth: {N_SYNTH:,}\\n\"\n",
    "    f\"- T (iterations): {T}\\n\"\n",
    "    f\"- L (variations per candidate + 1): {L}\\n\"\n",
    "    f\"- Target epsilon: {EPSILON}, delta: {DELTA}\\n\"\n",
    "    f\"- Calibrated sigma: {sigma:.4f}\\n\"\n",
    "    f\"- Initial population: {N_SYNTH * L:,} (N_synth x L)\\n\"\n",
    "    f\"- Privacy guarantee: (epsilon={EPSILON}, delta={DELTA})-DP via analytic Gaussian mechanism\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Run Private Evolution\n",
    "\n",
    "The PE loop:\n",
    "1. RANDOM_API generates 150,000 initial candidates (N_synth x L = 50K x 3)\n",
    "2. Each of the 1M real records votes for its nearest synthetic candidate under the workload-aware distance\n",
    "3. Gaussian noise (sigma) is added to the histogram to ensure DP\n",
    "4. Top 50,000 candidates are selected by rank\n",
    "\n",
    "With T=1, there is no VARIATION_API call (selection is the final step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import src.pe.api, src.pe.distance, src.pe.privacy, src.pe.histogram\n",
    "importlib.reload(src.pe.api)\n",
    "importlib.reload(src.pe.distance)\n",
    "importlib.reload(src.pe.privacy)\n",
    "importlib.reload(src.pe.histogram)\n",
    "from src.pe.histogram import private_evolution\n",
    "from src.pe.api import PEApi\n",
    "\n",
    "api = PEApi(wide, model=MODEL, max_concurrent=50)\n",
    "\n",
    "USE_BATCH = True\n",
    "WORK_DIR = Path(\"../data/batch_jobs\")\n",
    "CHECKPOINT_DIR = Path(\"../data/pe_checkpoints\")\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "synth_wide, pe_history = await private_evolution(\n",
    "    real_df=wide,\n",
    "    api=api,\n",
    "    n_synth=N_SYNTH,\n",
    "    T=T,\n",
    "    L=L,\n",
    "    epsilon=EPSILON,\n",
    "    delta=DELTA,\n",
    "    real_chunk=5000,\n",
    "    synth_chunk=10000,\n",
    "    batch_size=10,\n",
    "    variation_batch_size=5,\n",
    "    use_batch=USE_BATCH,\n",
    "    work_dir=WORK_DIR,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    ")\n",
    "\n",
    "display(Markdown(\n",
    "    f\"PE complete:\\n\\n\"\n",
    "    f\"- Synthetic records: {len(synth_wide):,}\\n\"\n",
    "    f\"- Total time: {pe_history['total_time']:.1f}s\\n\"\n",
    "    f\"- Actual epsilon: {pe_history['actual_epsilon']:.4f}\\n\"\n",
    "    f\"- Sigma: {pe_history['sigma']:.4f}\\n\"\n",
    "    f\"- Mode: {'Batch API (50% cheaper)' if USE_BATCH else 'Realtime API'}\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_wide.to_parquet(REPORTING / \"pe_wide_table.parquet\", index=False)\n",
    "\n",
    "display(Markdown(f\"Saved PE synthetic wide table: {len(synth_wide):,} rows x {len(synth_wide.columns)} columns\"))\n",
    "display(synth_wide.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect sparsity patterns\n",
    "\n",
    "A key question: does the LLM generate realistic sparsity patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_rows = []\n",
    "for c in numeric_cols:\n",
    "    real_nz = (wide[c] > 0).mean() * 100\n",
    "    synth_nz = (synth_wide[c] > 0).mean() * 100 if c in synth_wide.columns else 0\n",
    "    sparsity_rows.append({\"column\": c, \"real_nonzero_pct\": round(real_nz, 1), \"synth_nonzero_pct\": round(synth_nz, 1)})\n",
    "\n",
    "sparsity_df = pd.DataFrame(sparsity_rows)\n",
    "display(Markdown(\"Nonzero percentage comparison (real vs PE synthetic):\"))\n",
    "display(sparsity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Decompose into reporting tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.decompose import decompose_wide_table\n",
    "\n",
    "counts = decompose_wide_table(synth_wide, PE_REPORTING)\n",
    "\n",
    "rows = \"\\n\".join(f\"| {t} | {c:,} |\" for t, c in counts.items())\n",
    "display(Markdown(f\"Decomposed into {len(counts)} synthetic reporting tables:\\n\\n| Table | Rows |\\n|---|---|\\n{rows}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Benchmark evaluation\n",
    "\n",
    "Run the same 8 benchmark queries evaluated for DP-SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval.benchmark import run_benchmark\n",
    "\n",
    "eval_queries = [\n",
    "    \"avg_platform_power_c0_freq_temp_by_chassis\",\n",
    "    \"Xeon_network_consumption\",\n",
    "    \"pkg_power_by_country\",\n",
    "    \"ram_utilization_histogram\",\n",
    "    \"battery_power_on_geographic_summary\",\n",
    "    \"persona_web_cat_usage_analysis\",\n",
    "    \"popular_browsers_by_count_usage_percentage\",\n",
    "    \"most_popular_browser_in_each_country_by_system_count\",\n",
    "]\n",
    "\n",
    "pe_results = run_benchmark(eval_queries, QUERIES_DIR, PE_REPORTING, PE_RESULTS)\n",
    "\n",
    "display(Markdown(f\"{len(pe_results)}/{len(eval_queries)} queries executed on PE synthetic data.\"))\n",
    "for name, df in pe_results.items():\n",
    "    display(Markdown(f\"### `{name}` ({len(df)} rows)\"))\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Comparison with ground truth and DP-SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DPSGD_RESULTS = Path(\"../data/results/synthetic\")\n",
    "\n",
    "comparison_rows = []\n",
    "for name in eval_queries:\n",
    "    real_path = REAL_RESULTS / f\"{name}.csv\"\n",
    "    dpsgd_path = DPSGD_RESULTS / f\"{name}.csv\"\n",
    "    pe_path = PE_RESULTS / f\"{name}.csv\"\n",
    "\n",
    "    if not real_path.exists():\n",
    "        continue\n",
    "    real_df = pd.read_csv(real_path)\n",
    "\n",
    "    for col in real_df.select_dtypes(include=[np.number]).columns:\n",
    "        real_mean = real_df[col].mean()\n",
    "        if abs(real_mean) < 1e-10:\n",
    "            continue\n",
    "\n",
    "        row = {\"query\": name.replace(\"_\", \" \"), \"column\": col, \"real_mean\": real_mean}\n",
    "\n",
    "        if dpsgd_path.exists():\n",
    "            dpsgd_df = pd.read_csv(dpsgd_path)\n",
    "            if col in dpsgd_df.columns:\n",
    "                dpsgd_mean = dpsgd_df[col].mean()\n",
    "                row[\"dpsgd_mean\"] = dpsgd_mean\n",
    "                row[\"dpsgd_rel_error\"] = abs(real_mean - dpsgd_mean) / abs(real_mean)\n",
    "\n",
    "        if pe_path.exists():\n",
    "            pe_df = pd.read_csv(pe_path)\n",
    "            if col in pe_df.columns:\n",
    "                pe_mean = pe_df[col].mean()\n",
    "                row[\"pe_mean\"] = pe_mean\n",
    "                row[\"pe_rel_error\"] = abs(real_mean - pe_mean) / abs(real_mean)\n",
    "\n",
    "        comparison_rows.append(row)\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "display(Markdown(\"Column-level mean comparison (real vs DP-SGD vs PE):\"))\n",
    "display(comp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_query = \"most_popular_browser_in_each_country_by_system_count\"\n",
    "real_browsers = pd.read_csv(REAL_RESULTS / f\"{browser_query}.csv\")\n",
    "\n",
    "pe_browsers_path = PE_RESULTS / f\"{browser_query}.csv\"\n",
    "if pe_browsers_path.exists():\n",
    "    pe_browsers = pd.read_csv(pe_browsers_path)\n",
    "    merged = real_browsers.merge(pe_browsers, on=\"country\", suffixes=(\"_real\", \"_pe\"), how=\"inner\")\n",
    "    matches = (merged[\"browser_real\"] == merged[\"browser_pe\"]).sum()\n",
    "    total = len(merged)\n",
    "    display(Markdown(\n",
    "        f\"Browser ranking accuracy (PE): {matches}/{total} countries correct \"\n",
    "        f\"({100*matches/total:.0f}%)\"\n",
    "    ))\n",
    "\n",
    "    dpsgd_browsers_path = DPSGD_RESULTS / f\"{browser_query}.csv\"\n",
    "    if dpsgd_browsers_path.exists():\n",
    "        dpsgd_browsers = pd.read_csv(dpsgd_browsers_path)\n",
    "        merged_dpsgd = real_browsers.merge(dpsgd_browsers, on=\"country\", suffixes=(\"_real\", \"_dpsgd\"), how=\"inner\")\n",
    "        dpsgd_matches = (merged_dpsgd[\"browser_real\"] == merged_dpsgd[\"browser_dpsgd\"]).sum()\n",
    "        dpsgd_total = len(merged_dpsgd)\n",
    "        display(Markdown(\n",
    "            f\"Browser ranking accuracy (DP-SGD): {dpsgd_matches}/{dpsgd_total} countries correct \"\n",
    "            f\"({100*dpsgd_matches/dpsgd_total:.0f}%)\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_lines = [\n",
    "    \"| | DP-SGD (VAE) | Private Evolution |\",\n",
    "    \"|---|---|---|\",\n",
    "    f\"| Model | DP-VAE (505K params) | GPT-5 nano (API) |\",\n",
    "    f\"| Privacy | (3.996, 1e-5)-DP | ({pe_history['actual_epsilon']:.3f}, 1e-5)-DP |\",\n",
    "    f\"| Synthetic records | 1,000,000 | {len(synth_wide):,} |\",\n",
    "    f\"| Training/generation time | 360 min (CPU) | {pe_history['total_time']:.0f}s |\",\n",
    "    f\"| Iterations | 20 epochs | {T} PE iteration(s) |\",\n",
    "]\n",
    "\n",
    "display(Markdown(\"\\n\".join(summary_lines)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
