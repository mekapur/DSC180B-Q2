{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Consolidated Evaluation\n",
    "\n",
    "Compares all synthesis methods against the 21-query SQL benchmark:\n",
    "1. Wide-table DP-SGD (VAE, epsilon=4.0)\n",
    "2. Per-table DP histogram synthesis (epsilon=4.0 per table)\n",
    "3. MST marginal-based baseline (epsilon=4.0 per table)\n",
    "4. Private Evolution (pending completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.eval.compare import QUERY_METADATA, evaluate_all, results_to_dataframe, detailed_results_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REAL_DIR = Path(\"../data/results/real\")\n",
    "SYNTH_DIRS = {\n",
    "    \"Wide-table DP-SGD\": Path(\"../data/results/synthetic\"),\n",
    "    \"Per-table DP-SGD\": Path(\"../data/results/synth_pertable\"),\n",
    "    \"MST baseline\": Path(\"../data/results/synth_mst\"),\n",
    "}\n",
    "\n",
    "pe_dir = Path(\"../data/results/synth_pe\")\n",
    "if pe_dir.exists() and any(pe_dir.glob(\"*.csv\")):\n",
    "    SYNTH_DIRS[\"Private Evolution\"] = pe_dir\n",
    "\n",
    "display(Markdown(f\"Methods to compare: {', '.join(SYNTH_DIRS.keys())}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Run evaluation for all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summaries = {}\n",
    "all_details = {}\n",
    "\n",
    "for method, synth_dir in SYNTH_DIRS.items():\n",
    "    results = evaluate_all(REAL_DIR, synth_dir)\n",
    "    summary = results_to_dataframe(results)\n",
    "    detail = detailed_results_to_dataframe(results)\n",
    "    all_summaries[method] = summary\n",
    "    all_details[method] = detail\n",
    "\n",
    "    evaluated = summary[summary[\"error\"].isna()]\n",
    "    n_pass = int(evaluated[\"passed\"].sum())\n",
    "    n_eval = len(evaluated)\n",
    "    avg_score = evaluated[\"score\"].mean()\n",
    "    display(Markdown(\n",
    "        f\"{method}: {n_pass}/{n_eval} queries passed, \"\n",
    "        f\"average score = {avg_score:.3f}\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Query pass rates by method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_queries = sorted(QUERY_METADATA.keys())\n",
    "infeasible = {\n",
    "    \"ranked_process_classifications\",\n",
    "    \"top_10_processes_per_user_id_ranked_by_total_power_consumption\",\n",
    "    \"top_20_most_power_consuming_processes_by_avg_power_consumed\",\n",
    "}\n",
    "feasible_queries = [q for q in all_queries if q not in infeasible]\n",
    "\n",
    "rows = []\n",
    "for q in feasible_queries:\n",
    "    row = {\"query\": q, \"type\": QUERY_METADATA[q][\"type\"]}\n",
    "    for method, summary in all_summaries.items():\n",
    "        match = summary[summary[\"query\"] == q]\n",
    "        if len(match) == 0 or match.iloc[0][\"error\"] == match.iloc[0][\"error\"]:  # NaN check\n",
    "            if len(match) > 0 and pd.isna(match.iloc[0][\"error\"]):\n",
    "                row[f\"{method}_score\"] = match.iloc[0][\"score\"]\n",
    "                row[f\"{method}_passed\"] = bool(match.iloc[0][\"passed\"])\n",
    "            else:\n",
    "                row[f\"{method}_score\"] = None\n",
    "                row[f\"{method}_passed\"] = None\n",
    "        else:\n",
    "            row[f\"{method}_score\"] = None\n",
    "            row[f\"{method}_passed\"] = None\n",
    "    rows.append(row)\n",
    "\n",
    "comparison = pd.DataFrame(rows)\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Overall pass rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = list(SYNTH_DIRS.keys())\n",
    "pass_rates = []\n",
    "avg_scores = []\n",
    "n_evaluated = []\n",
    "\n",
    "for m in methods:\n",
    "    s = all_summaries[m]\n",
    "    ev = s[s[\"error\"].isna()]\n",
    "    pass_rates.append(int(ev[\"passed\"].sum()))\n",
    "    avg_scores.append(ev[\"score\"].mean())\n",
    "    n_evaluated.append(len(ev))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#9b59b6\"]\n",
    "x = np.arange(len(methods))\n",
    "\n",
    "bars = axes[0].bar(x, pass_rates, color=colors[:len(methods)])\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(methods, rotation=15, ha=\"right\", fontsize=9)\n",
    "axes[0].set_ylabel(\"Queries passed\")\n",
    "axes[0].set_title(\"Queries passing (score >= 0.5)\")\n",
    "for i, (bar, pr, ne) in enumerate(zip(bars, pass_rates, n_evaluated)):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
    "                 f\"{pr}/{ne}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "bars2 = axes[1].bar(x, avg_scores, color=colors[:len(methods)])\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(methods, rotation=15, ha=\"right\", fontsize=9)\n",
    "axes[1].set_ylabel(\"Average score\")\n",
    "axes[1].set_title(\"Average query score (fraction of metrics passing)\")\n",
    "axes[1].set_ylim(0, 1)\n",
    "for bar, sc in zip(bars2, avg_scores):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                 f\"{sc:.3f}\", ha=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../report/figures/method_comparison_overall.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Per-query-type breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_types = sorted(set(QUERY_METADATA[q][\"type\"] for q in feasible_queries))\n",
    "\n",
    "type_scores = {m: [] for m in methods}\n",
    "for qt in query_types:\n",
    "    for m in methods:\n",
    "        col = f\"{m}_score\"\n",
    "        subset = comparison[comparison[\"type\"] == qt]\n",
    "        valid = subset[col].dropna()\n",
    "        type_scores[m].append(valid.mean() if len(valid) > 0 else 0)\n",
    "\n",
    "x = np.arange(len(query_types))\n",
    "width = 0.8 / len(methods)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "for i, m in enumerate(methods):\n",
    "    offset = (i - len(methods)/2 + 0.5) * width\n",
    "    ax.bar(x + offset, type_scores[m], width, label=m, color=colors[i])\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([t.replace(\"_\", \" \") for t in query_types], rotation=20, ha=\"right\", fontsize=9)\n",
    "ax.set_ylabel(\"Average score\")\n",
    "ax.set_title(\"Average query score by query type\")\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc=\"upper right\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../report/figures/method_comparison_by_type.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Per-query score heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [f\"{m}_score\" for m in methods]\n",
    "heatmap_data = comparison.set_index(\"query\")[score_cols].copy()\n",
    "heatmap_data.columns = methods\n",
    "heatmap_data = heatmap_data.apply(pd.to_numeric, errors=\"coerce\").fillna(-0.1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "im = ax.imshow(heatmap_data.values, aspect=\"auto\", cmap=\"RdYlGn\", vmin=0, vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(methods)))\n",
    "ax.set_xticklabels(methods, fontsize=9)\n",
    "ax.set_yticks(np.arange(len(heatmap_data)))\n",
    "\n",
    "short_names = []\n",
    "for q in heatmap_data.index:\n",
    "    name = q.replace(\"_\", \" \")\n",
    "    if len(name) > 45:\n",
    "        name = name[:42] + \"...\"\n",
    "    short_names.append(name)\n",
    "ax.set_yticklabels(short_names, fontsize=7)\n",
    "\n",
    "for i in range(len(heatmap_data)):\n",
    "    for j in range(len(methods)):\n",
    "        val = heatmap_data.values[i, j]\n",
    "        if val < 0:\n",
    "            text = \"N/A\"\n",
    "            color = \"gray\"\n",
    "        else:\n",
    "            text = f\"{val:.2f}\"\n",
    "            color = \"white\" if val < 0.3 else \"black\"\n",
    "        ax.text(j, i, text, ha=\"center\", va=\"center\", fontsize=7, color=color)\n",
    "\n",
    "fig.colorbar(im, ax=ax, label=\"Query score\", shrink=0.6)\n",
    "ax.set_title(\"Per-query scores across methods\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../report/figures/method_comparison_heatmap.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Detailed metric comparison for key queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_queries = [\n",
    "    \"avg_platform_power_c0_freq_temp_by_chassis\",\n",
    "    \"most_popular_browser_in_each_country_by_system_count\",\n",
    "    \"ram_utilization_histogram\",\n",
    "    \"battery_power_on_geographic_summary\",\n",
    "    \"popular_browsers_by_count_usage_percentage\",\n",
    "]\n",
    "\n",
    "for q in key_queries:\n",
    "    display(Markdown(f\"### {q.replace('_', ' ').title()}\"))\n",
    "    rows = []\n",
    "    for m in methods:\n",
    "        detail = all_details[m]\n",
    "        q_detail = detail[detail[\"query\"] == q]\n",
    "        for _, r in q_detail.iterrows():\n",
    "            rows.append({\n",
    "                \"method\": m,\n",
    "                \"column\": r[\"column\"],\n",
    "                \"metric\": r[\"metric_type\"],\n",
    "                \"value\": f\"{r['value']:.4f}\" if pd.notna(r['value']) and r['value'] != float('inf') else \"inf\",\n",
    "                \"passed\": r[\"passed\"],\n",
    "                \"detail\": r[\"detail\"],\n",
    "            })\n",
    "    if rows:\n",
    "        display(pd.DataFrame(rows))\n",
    "    else:\n",
    "        display(Markdown(\"No data available for this query.\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Summary table for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_rows = []\n",
    "for m in methods:\n",
    "    s = all_summaries[m]\n",
    "    ev = s[s[\"error\"].isna()]\n",
    "    report_rows.append({\n",
    "        \"Method\": m,\n",
    "        \"Queries evaluated\": len(ev),\n",
    "        \"Queries passed\": int(ev[\"passed\"].sum()),\n",
    "        \"Pass rate\": f\"{ev['passed'].mean():.1%}\",\n",
    "        \"Avg score\": f\"{ev['score'].mean():.3f}\",\n",
    "        \"Median score\": f\"{ev['score'].median():.3f}\",\n",
    "    })\n",
    "\n",
    "report_df = pd.DataFrame(report_rows)\n",
    "display(Markdown(\"### Method comparison summary\"))\n",
    "display(report_df)\n",
    "\n",
    "display(Markdown(\"### LaTeX table\"))\n",
    "latex = report_df.to_latex(index=False, escape=True)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Strengths and weaknesses analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Queries where methods diverge\"))\n",
    "\n",
    "for q in feasible_queries:\n",
    "    scores = {}\n",
    "    for m in methods:\n",
    "        col = f\"{m}_score\"\n",
    "        val = comparison.loc[comparison[\"query\"] == q, col].values\n",
    "        if len(val) > 0 and pd.notna(val[0]):\n",
    "            scores[m] = val[0]\n",
    "    \n",
    "    if len(scores) >= 2:\n",
    "        vals = list(scores.values())\n",
    "        spread = max(vals) - min(vals)\n",
    "        if spread >= 0.3:\n",
    "            best = max(scores, key=scores.get)\n",
    "            worst = min(scores, key=scores.get)\n",
    "            display(Markdown(\n",
    "                f\"  {q}: spread={spread:.2f}, \"\n",
    "                f\"best={best} ({scores[best]:.2f}), \"\n",
    "                f\"worst={worst} ({scores[worst]:.2f})\"\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Queries all methods fail\"))\n",
    "\n",
    "all_fail = []\n",
    "for q in feasible_queries:\n",
    "    all_scores = []\n",
    "    for m in methods:\n",
    "        col = f\"{m}_score\"\n",
    "        val = comparison.loc[comparison[\"query\"] == q, col].values\n",
    "        if len(val) > 0 and pd.notna(val[0]):\n",
    "            all_scores.append(val[0])\n",
    "    if all_scores and max(all_scores) < 0.5:\n",
    "        all_fail.append(q)\n",
    "        \n",
    "for q in all_fail:\n",
    "    scores_str = \", \".join(\n",
    "        f\"{m}: {comparison.loc[comparison['query']==q, f'{m}_score'].values[0]:.2f}\"\n",
    "        for m in methods\n",
    "        if pd.notna(comparison.loc[comparison['query']==q, f'{m}_score'].values[0])\n",
    "    )\n",
    "    display(Markdown(f\"  {q} ({scores_str})\"))\n",
    "\n",
    "display(Markdown(f\"\\nTotal: {len(all_fail)}/{len(feasible_queries)} queries fail all methods\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}