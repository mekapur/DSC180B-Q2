{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 — Data Exploration\n",
        "\n",
        "Initial exploration of downloaded DCA data to determine:\n",
        "1. What we actually have (schemas, row counts, column names)\n",
        "2. Which of the 24 benchmark queries our data can support\n",
        "3. How the raw tables map to the `reporting.system_*` tables the queries expect\n",
        "4. What aggregation work is needed to bridge the gap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import duckdb\n",
        "import pandas as pd\n",
        "\n",
        "DATA = Path(\"../data\")\n",
        "QUERIES = Path(\"../docs/queries\")\n",
        "\n",
        "con = duckdb.connect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Inventory of downloaded data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parquet tables\n",
        "parquet_tables = {}\n",
        "for d in sorted(DATA.iterdir()):\n",
        "    if d.is_dir():\n",
        "        pfiles = sorted(d.glob(\"*.parquet\"))\n",
        "        if pfiles:\n",
        "            parquet_tables[d.name] = pfiles\n",
        "\n",
        "# Gzipped text tables (from dca_update_dec_2024)\n",
        "gz_files = sorted(DATA.glob(\"*.gz\"))\n",
        "\n",
        "print(\"=== Parquet Tables ===\")\n",
        "for name, files in parquet_tables.items():\n",
        "    total_mb = sum(f.stat().st_size for f in files) / 1e6\n",
        "    print(f\"  {name:45s} {len(files)} file(s)  {total_mb:>8.1f} MB\")\n",
        "\n",
        "print(f\"\\n=== Gzipped Text Files (dca_update_dec_2024) ===\")\n",
        "for f in gz_files:\n",
        "    mb = f.stat().st_size / 1e6\n",
        "    print(f\"  {f.name:45s} {mb:>8.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Schema inspection — Parquet tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, files in parquet_tables.items():\n",
        "    glob = str(files[0].parent / \"*.parquet\")\n",
        "    schema = con.execute(f\"DESCRIBE SELECT * FROM read_parquet('{glob}')\").df()\n",
        "    nrows = con.execute(f\"SELECT COUNT(*) FROM read_parquet('{glob}')\").fetchone()[0]\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{name} — {nrows:,} rows, {len(schema)} columns\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(schema[['column_name', 'column_type']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Schema inspection — Gzipped text files\n",
        "\n",
        "DuckDB can read gzipped CSVs/TSVs directly. Let's detect the format and inspect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for f in gz_files:\n",
        "    try:\n",
        "        schema = con.execute(\n",
        "            f\"DESCRIBE SELECT * FROM read_csv('{f}', auto_detect=true, sample_size=1000)\"\n",
        "        ).df()\n",
        "        nrows = con.execute(\n",
        "            f\"SELECT COUNT(*) FROM read_csv('{f}', auto_detect=true, sample_size=1000)\"\n",
        "        ).fetchone()[0]\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"{f.name} — {nrows:,} rows, {len(schema)} columns\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(schema[['column_name', 'column_type']].to_string(index=False))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n{f.name}: ERROR — {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3b. Verify new downloads — battery events and blocker history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check __tmp_batt_dc_events — unlocks 2 battery queries\n",
        "batt_file = DATA / \"__tmp_batt_dc_events.txt000.gz\"\n",
        "if batt_file.exists():\n",
        "    batt_schema = con.execute(\n",
        "        f\"DESCRIBE SELECT * FROM read_csv('{batt_file}', auto_detect=true, sample_size=1000)\"\n",
        "    ).df()\n",
        "    batt_rows = con.execute(\n",
        "        f\"SELECT COUNT(*) FROM read_csv('{batt_file}', auto_detect=true, sample_size=1000)\"\n",
        "    ).fetchone()[0]\n",
        "    print(f\"__tmp_batt_dc_events: {batt_rows:,} rows, {len(batt_schema)} columns\")\n",
        "    print(batt_schema[['column_name', 'column_type']].to_string(index=False))\n",
        "    \n",
        "    # The battery queries need: guid, num_power_ons, duration_mins\n",
        "    print(f\"\\nSample rows:\")\n",
        "    print(con.execute(\n",
        "        f\"SELECT * FROM read_csv('{batt_file}', auto_detect=true, sample_size=1000) LIMIT 5\"\n",
        "    ).df().to_string())\n",
        "else:\n",
        "    print(\"__tmp_batt_dc_events.txt000.gz not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Check mods_sleepstudy_top_blocker_hist — unlocks 2 blocker queries\n",
        "blocker_file = DATA / \"mods_sleepstudy_top_blocker_hist.txt000.gz\"\n",
        "if blocker_file.exists():\n",
        "    blocker_schema = con.execute(\n",
        "        f\"DESCRIBE SELECT * FROM read_csv('{blocker_file}', auto_detect=true, sample_size=1000)\"\n",
        "    ).df()\n",
        "    blocker_rows = con.execute(\n",
        "        f\"SELECT COUNT(*) FROM read_csv('{blocker_file}', auto_detect=true, sample_size=1000)\"\n",
        "    ).fetchone()[0]\n",
        "    print(f\"\\nmods_sleepstudy_top_blocker_hist: {blocker_rows:,} rows, {len(blocker_schema)} columns\")\n",
        "    print(blocker_schema[['column_name', 'column_type']].to_string(index=False))\n",
        "else:\n",
        "    print(\"mods_sleepstudy_top_blocker_hist.txt000.gz not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Key column checks\n",
        "\n",
        "### 4a. hw_metric_stats — what metric names are available?\n",
        "\n",
        "The `name` column should contain values like `HW::PACKAGE:RAP:WATTS`, `HW::CORE:C0:PERCENT`, etc.\n",
        "This determines whether we can build the 4 reporting tables needed for the 5-way chassis query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hw_glob = str(DATA / \"hw_metric_stats\" / \"*.parquet\")\n",
        "\n",
        "metric_names = con.execute(f\"\"\"\n",
        "    SELECT name, COUNT(*) AS n, COUNT(DISTINCT guid) AS n_guids\n",
        "    FROM read_parquet('{hw_glob}')\n",
        "    GROUP BY name\n",
        "    ORDER BY n DESC\n",
        "\"\"\").df()\n",
        "\n",
        "print(f\"Total distinct metric names: {len(metric_names)}\")\n",
        "print(f\"\\nAll metric names:\")\n",
        "print(metric_names.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check specifically for the 4 metrics we need for the reporting tables\n",
        "targets = ['RAP', 'PSYS', 'C0', 'FREQ', 'TEMP', 'POWER', 'PKG']\n",
        "\n",
        "for t in targets:\n",
        "    matches = metric_names[metric_names['name'].str.contains(t, case=False, na=False)]\n",
        "    if len(matches) > 0:\n",
        "        print(f\"\\n✓ Matches for '{t}':\")\n",
        "        print(matches.to_string(index=False))\n",
        "    else:\n",
        "        print(f\"\\n✗ No matches for '{t}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. sysinfo — verify key columns for query joins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sysinfo_glob = str(DATA / \"system_sysinfo_unique_normalized\" / \"*.parquet\")\n",
        "\n",
        "# Check a few key columns\n",
        "for col in ['chassistype', 'countryname_normalized', 'persona', 'cpu_family', 'processornumber', 'ram', 'os']:\n",
        "    try:\n",
        "        vals = con.execute(f\"\"\"\n",
        "            SELECT \"{col}\", COUNT(*) AS n\n",
        "            FROM read_parquet('{sysinfo_glob}')\n",
        "            GROUP BY \"{col}\"\n",
        "            ORDER BY n DESC\n",
        "            LIMIT 10\n",
        "        \"\"\").df()\n",
        "        print(f\"\\n{col} — top 10 values:\")\n",
        "        print(vals.to_string(index=False))\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ {col}: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4c. os_network_consumption_v2 — verify input_description values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net_glob = str(DATA / \"os_network_consumption_v2\" / \"*.parquet\")\n",
        "\n",
        "net_descs = con.execute(f\"\"\"\n",
        "    SELECT input_description, COUNT(*) AS n\n",
        "    FROM read_parquet('{net_glob}')\n",
        "    GROUP BY input_description\n",
        "    ORDER BY n DESC\n",
        "\"\"\").df()\n",
        "\n",
        "print(\"input_description values:\")\n",
        "print(net_descs.to_string(index=False))\n",
        "\n",
        "# The queries need these exact strings:\n",
        "needed = [\n",
        "    'OS:NETWORK INTERFACE::BYTES RECEIVED/SEC::',\n",
        "    'OS:NETWORK INTERFACE::BYTES SENT/SEC::',\n",
        "]\n",
        "for n in needed:\n",
        "    found = n in net_descs['input_description'].values\n",
        "    print(f\"\\n{'✓' if found else '✗'} '{n}' — {'FOUND' if found else 'MISSING'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4d. web_cat_pivot — check column names match persona query expectations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pivot_glob = str(DATA / \"web_cat_pivot\" / \"*.parquet\")\n",
        "\n",
        "pivot_schema = con.execute(f\"DESCRIBE SELECT * FROM read_parquet('{pivot_glob}')\").df()\n",
        "pivot_cols = set(pivot_schema['column_name'].values)\n",
        "\n",
        "# The persona query references these column names (from the SQL)\n",
        "expected_cols = {\n",
        "    'guid', 'content_creation_photo_edit_creation', 'content_creation_video_audio_edit_creation',\n",
        "    'content_creation_web_design_development', 'education_education', 'entertainment_music_audio_streaming',\n",
        "    'entertainment_other', 'entertainment_video_streaming', 'finance_banking_and_accounting',\n",
        "    'games_other', 'games_video_games', 'mail_mail', 'news_news', 'other_unclassified',\n",
        "    'private_private', 'productivity_crm', 'productivity_other', 'productivity_presentations',\n",
        "    'productivity_programming', 'productivity_project_management', 'productivity_spreadsheets',\n",
        "    'productivity_word_processing', 'recreation_travel', 'reference_reference', 'search_search',\n",
        "    'shopping_shopping', 'social_social_network', 'social_communication', 'social_communication_live',\n",
        "}\n",
        "\n",
        "print(f\"Pivot table columns ({len(pivot_cols)}): {sorted(pivot_cols)}\")\n",
        "print(f\"\\nExpected columns ({len(expected_cols)}):\")\n",
        "\n",
        "# The persona query SQL uses shortened names (e.g., 'education' not 'education_education')\n",
        "# Let's check what matches and what doesn't\n",
        "for ec in sorted(expected_cols):\n",
        "    if ec in pivot_cols:\n",
        "        print(f\"  ✓ {ec}\")\n",
        "    else:\n",
        "        # Try partial match\n",
        "        partial = [c for c in pivot_cols if ec in c or c in ec]\n",
        "        if partial:\n",
        "            print(f\"  ~ {ec} → partial match: {partial}\")\n",
        "        else:\n",
        "            print(f\"  ✗ {ec}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4e. os_memsam_avail_percent — check column names for RAM histogram query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mem_glob = str(DATA / \"os_memsam_avail_percent\" / \"*.parquet\")\n",
        "\n",
        "# The RAM query expects: sysinfo_ram, guid, nrs, avg_percentage_used\n",
        "# Our raw table has: sample_count, average, below_0..below_100, equal_to_greater_than_100\n",
        "# We need to derive avg_percentage_used from (100 - average) and map sample_count -> nrs\n",
        "\n",
        "sample = con.execute(f\"\"\"\n",
        "    SELECT *\n",
        "    FROM read_parquet('{mem_glob}')\n",
        "    LIMIT 5\n",
        "\"\"\").df()\n",
        "\n",
        "print(\"Sample rows (first 5):\")\n",
        "print(sample.to_string())\n",
        "print(f\"\\nNote: 'average' column = % available memory (not % used)\")\n",
        "print(f\"avg_percentage_used = 100 - average\")\n",
        "print(f\"nrs = sample_count\")\n",
        "print(f\"sysinfo_ram must come from JOIN with sysinfo table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Query coverage analysis\n",
        "\n",
        "Parse all 24 benchmark queries, extract the `reporting.system_*` tables they reference,\n",
        "and determine which ones we can actually serve with our downloaded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse all queries and extract table references\n",
        "queries = []\n",
        "for qf in sorted(QUERIES.glob(\"*.json\")):\n",
        "    with open(qf) as f:\n",
        "        data = json.load(f)\n",
        "        if isinstance(data, list):\n",
        "            data = data[0]\n",
        "        sql = data.get('sql', '')\n",
        "        # Extract reporting.system_* table names\n",
        "        tables = set(re.findall(r'reporting\\.(\\w+)', sql))\n",
        "        queries.append({\n",
        "            'file': qf.stem,\n",
        "            'question': data.get('question', '')[:80] + '...',\n",
        "            'tables': tables,\n",
        "            'sql': sql,\n",
        "        })\n",
        "\n",
        "print(f\"Total benchmark queries: {len(queries)}\\n\")\n",
        "\n",
        "# Map reporting tables to our available data\n",
        "AVAILABLE_DATA = {\n",
        "    # Direct matches (Parquet)\n",
        "    'system_sysinfo_unique_normalized': 'system_sysinfo_unique_normalized/*.parquet',\n",
        "    # Raw tables that need aggregation into reporting schema\n",
        "    'system_network_consumption': 'os_network_consumption_v2/*.parquet (needs column rename: input_description->input_desc, nr_samples->nrs)',\n",
        "    'system_web_cat_usage': 'web_cat_usage_v2/*.parquet (direct, has browser/duration_ms/guid)',\n",
        "    'system_web_cat_pivot_duration': 'web_cat_pivot/*.parquet (has 28 category columns, needs days/total_duration derivation)',\n",
        "    'system_memory_utilization': 'os_memsam_avail_percent/*.parquet (needs: avg_percentage_used=100-average, nrs=sample_count, sysinfo_ram from JOIN)',\n",
        "    'system_hw_pkg_power': 'hw_metric_stats/*.parquet (filter by PKG_POWER metric name)',\n",
        "    # Reporting tables derived from hw_metric_stats (need to verify metric names exist)\n",
        "    'system_psys_rap_watts': 'hw_metric_stats/*.parquet (filter by PSYS_RAP metric — NEEDS VERIFICATION)',\n",
        "    'system_pkg_C0': 'hw_metric_stats/*.parquet (filter by C0 metric — NEEDS VERIFICATION)',\n",
        "    'system_pkg_avg_freq_mhz': 'hw_metric_stats/*.parquet (filter by FREQ metric — NEEDS VERIFICATION)',\n",
        "    'system_pkg_temp_centigrade': 'hw_metric_stats/*.parquet (filter by TEMP metric — NEEDS VERIFICATION)',\n",
        "    # From dca_update_dec_2024 gzipped text\n",
        "    'system_cpu_metadata': 'system_cpu_metadata.txt000.gz',\n",
        "    'system_os_codename_history': 'system_os_codename_history.txt000.gz',\n",
        "    'system_on_off_suspend_time_day': 'guids_on_off_suspend_time_day.txt000.gz',\n",
        "    'system_mods_top_blocker_hist': 'mods_sleepstudy_top_blocker_hist.txt000.gz (92M rows)',\n",
        "    'system_batt_dc_events': '__tmp_batt_dc_events.txt000.gz (12 MiB, pre-aggregated)',\n",
        "}\n",
        "\n",
        "NOT_AVAILABLE = {\n",
        "    'system_mods_power_consumption': 'NOT DOWNLOADED — need mods_sleepstudy_power_estimation_data_13wks (unknown size)',\n",
        "    'system_display_devices': 'NOT DOWNLOADED — display_devices.txt000.gz (6.15 GiB)',\n",
        "    'system_frgnd_apps_types': 'NOT DOWNLOADED — __tmp_fgnd_apps_date (21.6 GiB) or frgnd_system_usage_by_app (338 GiB)',\n",
        "    'system_userwait': 'NOT DOWNLOADED — userwait_v2 (59 GiB)',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classify each query\n",
        "feasible = []\n",
        "infeasible = []\n",
        "partial = []\n",
        "\n",
        "for q in queries:\n",
        "    available = [t for t in q['tables'] if t in AVAILABLE_DATA]\n",
        "    missing = [t for t in q['tables'] if t not in AVAILABLE_DATA]\n",
        "    \n",
        "    if not missing:\n",
        "        feasible.append(q)\n",
        "    elif not available:\n",
        "        infeasible.append(q)\n",
        "    else:\n",
        "        partial.append(q)\n",
        "\n",
        "print(f\"=== FEASIBLE ({len(feasible)}/{len(queries)}) ===\")\n",
        "for q in feasible:\n",
        "    print(f\"  ✓ {q['file']}\")\n",
        "    print(f\"    Tables: {', '.join(q['tables'])}\")\n",
        "\n",
        "print(f\"\\n=== INFEASIBLE — missing data ({len(infeasible)}/{len(queries)}) ===\")\n",
        "for q in infeasible:\n",
        "    missing = [t for t in q['tables'] if t not in AVAILABLE_DATA]\n",
        "    print(f\"  ✗ {q['file']}\")\n",
        "    for t in missing:\n",
        "        reason = NOT_AVAILABLE.get(t, 'Unknown source')\n",
        "        print(f\"    Missing: {t} — {reason}\")\n",
        "\n",
        "print(f\"\\n=== PARTIAL — some tables available ({len(partial)}/{len(queries)}) ===\")\n",
        "for q in partial:\n",
        "    available = [t for t in q['tables'] if t in AVAILABLE_DATA]\n",
        "    missing = [t for t in q['tables'] if t not in AVAILABLE_DATA]\n",
        "    print(f\"  ~ {q['file']}\")\n",
        "    print(f\"    Have: {', '.join(available)}\")\n",
        "    for t in missing:\n",
        "        reason = NOT_AVAILABLE.get(t, 'Unknown source')\n",
        "        print(f\"    Missing: {t} — {reason}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Guid overlap check\n",
        "\n",
        "Since we only downloaded partial parquet files for event tables,\n",
        "check how many guids overlap between sysinfo (anchor) and each event table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sysinfo_guids = con.execute(f\"\"\"\n",
        "    SELECT COUNT(DISTINCT guid) FROM read_parquet('{sysinfo_glob}')\n",
        "\"\"\").fetchone()[0]\n",
        "print(f\"Sysinfo unique guids: {sysinfo_guids:,}\")\n",
        "\n",
        "for name, files in parquet_tables.items():\n",
        "    if name == 'system_sysinfo_unique_normalized' or name == 'data_dictionary':\n",
        "        continue\n",
        "    glob = str(files[0].parent / \"*.parquet\")\n",
        "    try:\n",
        "        result = con.execute(f\"\"\"\n",
        "            SELECT \n",
        "                COUNT(DISTINCT e.guid) AS event_guids,\n",
        "                COUNT(DISTINCT CASE WHEN s.guid IS NOT NULL THEN e.guid END) AS overlapping_guids\n",
        "            FROM read_parquet('{glob}') e\n",
        "            LEFT JOIN read_parquet('{sysinfo_glob}') s ON e.guid = s.guid\n",
        "        \"\"\").fetchone()\n",
        "        event_guids, overlap = result\n",
        "        pct = (overlap / event_guids * 100) if event_guids > 0 else 0\n",
        "        print(f\"  {name:40s} {event_guids:>8,} guids, {overlap:>8,} overlap ({pct:.1f}%)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {name:40s} ERROR: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "Quick printout of the final status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Data downloaded: {sum(f.stat().st_size for f in DATA.rglob('*') if f.is_file()) / 1e9:.1f} GiB\")\n",
        "print(f\"Parquet tables: {len(parquet_tables)}\")\n",
        "print(f\"Gzipped text files: {len(gz_files)}\")\n",
        "print(f\"\")\n",
        "print(f\"Benchmark queries: {len(queries)} total\")\n",
        "print(f\"  Feasible:   {len(feasible)}\")\n",
        "print(f\"  Partial:    {len(partial)}\")\n",
        "print(f\"  Infeasible: {len(infeasible)}\")\n",
        "print(f\"\")\n",
        "print(\"Next steps:\")\n",
        "print(\"  1. Verify hw_metric_stats metric names (cell 4a above)\")\n",
        "print(\"  2. Build reporting schema aggregation SQL for feasible queries\")\n",
        "print(\"  3. Run feasible benchmark queries to get ground truth results\")\n",
        "print(\"  4. Apply DP-SGD / Private Evolution to reporting tables\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
